The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
pynvml not found. GPU stats will not be printed.
Loading config
pynvml not found. GPU stats will not be printed.
Loading config
pynvml not found. GPU stats will not be printed.
Loading config
pynvml not found. GPU stats will not be printed.
Loading config
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.06s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.31it/s]
Some weights of ColQwen2 were not initialized from the model checkpoint at /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.31it/s]
Some weights of ColQwen2 were not initialized from the model checkpoint at /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.30it/s]
Some weights of ColQwen2 were not initialized from the model checkpoint at /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.39it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.30it/s]
Some weights of ColQwen2 were not initialized from the model checkpoint at /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Configurating PEFT model
Configurating PEFT model
Configurating PEFT model
Configurating PEFT model
trainable params: 36,982,784 || all params: 2,246,165,120 || trainable%: 1.6465
Creating Setup
trainable params: 36,982,784 || all params: 2,246,165,120 || trainable%: 1.6465
Creating Setup
trainable params: 36,982,784 || all params: 2,246,165,120 || trainable%: 1.6465
Creating Setup
trainable params: 36,982,784 || all params: 2,246,165,120 || trainable%: 1.6465
Creating Setup
Training modelTraining model

Training with in-batch negativesTraining with in-batch negatives

Training model
Training with in-batch negatives
Training model
Training with in-batch negatives
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jingfenqiao. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /ivi/ilps/personal/jqiao/colpali/wandb/run-20241223_000738-dwl0ad4a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-snow-13
wandb: â­ï¸ View project at https://wandb.ai/jingfenqiao/huggingface
wandb: ðŸš€ View run at https://wandb.ai/jingfenqiao/huggingface/runs/dwl0ad4a
  0%|          | 0/4620 [00:00<?, ?it/s]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[rank0]:[W1223 00:10:20.228039007 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W1223 00:10:20.544405319 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W1223 00:10:20.628835200 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1223 00:10:22.542656973 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/4620 [02:57<228:20:32, 177.97s/it]  0%|          | 2/4620 [03:08<101:37:31, 79.22s/it]   0%|          | 3/4620 [03:19<61:48:35, 48.19s/it]   0%|          | 4/4620 [03:31<43:24:48, 33.86s/it]  0%|          | 5/4620 [03:43<33:24:38, 26.06s/it]  0%|          | 6/4620 [03:56<27:37:43, 21.56s/it]  0%|          | 7/4620 [04:09<23:58:43, 18.71s/it]  0%|          | 8/4620 [04:21<21:25:58, 16.73s/it]  0%|          | 9/4620 [04:35<20:24:53, 15.94s/it]  0%|          | 10/4620 [04:48<19:14:18, 15.02s/it]                                                    {'loss': 0.7338, 'grad_norm': 2.796875, 'learning_rate': 5e-05, 'epoch': 0.01}
  0%|          | 10/4620 [04:48<19:14:18, 15.02s/it]  0%|          | 11/4620 [05:00<17:46:43, 13.89s/it]  0%|          | 12/4620 [05:12<17:01:25, 13.30s/it]  0%|          | 13/4620 [05:24<16:40:55, 13.04s/it]  0%|          | 14/4620 [05:36<16:28:01, 12.87s/it]  0%|          | 15/4620 [05:49<16:16:19, 12.72s/it]  0%|          | 16/4620 [06:01<16:00:45, 12.52s/it]  0%|          | 17/4620 [06:13<16:00:06, 12.52s/it]  0%|          | 18/4620 [06:26<15:53:22, 12.43s/it]  0%|          | 19/4620 [06:40<16:44:30, 13.10s/it]  0%|          | 20/4620 [06:55<17:17:35, 13.53s/it]                                                    {'loss': 0.5559, 'grad_norm': 1.2109375, 'learning_rate': 0.0001, 'epoch': 0.02}
  0%|          | 20/4620 [06:55<17:17:35, 13.53s/it]  0%|          | 21/4620 [07:06<16:27:31, 12.88s/it]  0%|          | 22/4620 [07:18<16:07:02, 12.62s/it]  0%|          | 23/4620 [07:31<16:02:20, 12.56s/it]  1%|          | 24/4620 [07:43<15:58:16, 12.51s/it]  1%|          | 25/4620 [07:55<15:54:17, 12.46s/it]  1%|          | 26/4620 [08:07<15:44:15, 12.33s/it]  1%|          | 27/4620 [08:20<15:44:25, 12.34s/it]  1%|          | 28/4620 [08:32<15:38:29, 12.26s/it]  1%|          | 29/4620 [08:45<16:08:49, 12.66s/it]  1%|          | 30/4620 [08:58<16:16:11, 12.76s/it]                                                    {'loss': 0.3981, 'grad_norm': 0.9453125, 'learning_rate': 0.00015, 'epoch': 0.03}
  1%|          | 30/4620 [08:58<16:16:11, 12.76s/it]  1%|          | 31/4620 [09:11<16:06:53, 12.64s/it]  1%|          | 32/4620 [09:22<15:41:16, 12.31s/it]  1%|          | 33/4620 [09:34<15:37:48, 12.27s/it]  1%|          | 34/4620 [09:47<15:34:24, 12.23s/it]  1%|          | 35/4620 [09:59<15:38:51, 12.29s/it]  1%|          | 36/4620 [10:11<15:39:30, 12.30s/it]  1%|          | 37/4620 [10:24<15:51:00, 12.45s/it]  1%|          | 38/4620 [10:37<15:49:42, 12.44s/it]  1%|          | 39/4620 [10:51<16:44:21, 13.15s/it]  1%|          | 40/4620 [11:05<16:42:39, 13.14s/it]                                                    {'loss': 0.3047, 'grad_norm': 0.70703125, 'learning_rate': 0.0002, 'epoch': 0.04}
  1%|          | 40/4620 [11:05<16:42:39, 13.14s/it]  1%|          | 41/4620 [11:16<16:01:11, 12.59s/it]  1%|          | 42/4620 [11:28<15:52:37, 12.49s/it]  1%|          | 43/4620 [11:40<15:42:02, 12.35s/it]  1%|          | 44/4620 [11:52<15:36:59, 12.29s/it]  1%|          | 45/4620 [12:05<15:40:51, 12.34s/it]  1%|          | 46/4620 [12:17<15:39:50, 12.33s/it]  1%|          | 47/4620 [12:29<15:42:24, 12.36s/it]  1%|          | 48/4620 [12:42<15:40:14, 12.34s/it]  1%|          | 49/4620 [12:56<16:12:55, 12.77s/it]  1%|          | 50/4620 [13:10<16:48:47, 13.24s/it]                                                    {'loss': 0.2354, 'grad_norm': 0.82421875, 'learning_rate': 0.00025, 'epoch': 0.05}
  1%|          | 50/4620 [13:10<16:48:47, 13.24s/it]  1%|          | 51/4620 [13:21<16:01:36, 12.63s/it]  1%|          | 52/4620 [13:33<15:45:27, 12.42s/it]  1%|          | 53/4620 [13:46<15:49:14, 12.47s/it]  1%|          | 54/4620 [13:58<15:50:33, 12.49s/it]  1%|          | 55/4620 [14:10<15:32:32, 12.26s/it]  1%|          | 56/4620 [14:22<15:28:18, 12.20s/it]  1%|          | 57/4620 [14:35<15:37:31, 12.33s/it]  1%|â–         | 58/4620 [14:48<16:09:47, 12.75s/it]  1%|â–         | 59/4620 [15:01<16:12:09, 12.79s/it]  1%|â–         | 60/4620 [15:15<16:31:53, 13.05s/it]                                                    {'loss': 0.2157, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 0.06}
  1%|â–         | 60/4620 [15:15<16:31:53, 13.05s/it]  1%|â–         | 61/4620 [15:27<16:02:47, 12.67s/it]  1%|â–         | 62/4620 [15:39<15:45:41, 12.45s/it]  1%|â–         | 63/4620 [15:51<15:39:33, 12.37s/it]  1%|â–         | 64/4620 [16:03<15:33:54, 12.30s/it]  1%|â–         | 65/4620 [16:15<15:38:47, 12.37s/it]  1%|â–         | 66/4620 [16:27<15:33:29, 12.30s/it]  1%|â–         | 67/4620 [16:41<15:59:27, 12.64s/it]  1%|â–         | 68/4620 [16:54<16:10:14, 12.79s/it]  1%|â–         | 69/4620 [17:07<16:21:22, 12.94s/it]  2%|â–         | 70/4620 [17:20<16:24:24, 12.98s/it]                                                    {'loss': 0.1566, 'grad_norm': 0.90234375, 'learning_rate': 0.00035, 'epoch': 0.08}
  2%|â–         | 70/4620 [17:20<16:24:24, 12.98s/it]  2%|â–         | 71/4620 [17:32<15:41:06, 12.41s/it]  2%|â–         | 72/4620 [17:44<15:34:19, 12.33s/it]  2%|â–         | 73/4620 [17:56<15:37:34, 12.37s/it]  2%|â–         | 74/4620 [18:08<15:35:34, 12.35s/it]  2%|â–         | 75/4620 [18:20<15:27:16, 12.24s/it]  2%|â–         | 76/4620 [18:32<15:22:07, 12.18s/it]  2%|â–         | 77/4620 [18:46<15:58:44, 12.66s/it]  2%|â–         | 78/4620 [18:58<15:41:47, 12.44s/it]  2%|â–         | 79/4620 [19:11<15:57:40, 12.65s/it]  2%|â–         | 80/4620 [19:25<16:10:19, 12.82s/it]                                                    {'loss': 0.1366, 'grad_norm': 0.80859375, 'learning_rate': 0.0004, 'epoch': 0.09}
  2%|â–         | 80/4620 [19:25<16:10:19, 12.82s/it]  2%|â–         | 81/4620 [19:36<15:41:53, 12.45s/it]  2%|â–         | 82/4620 [19:48<15:35:23, 12.37s/it]  2%|â–         | 83/4620 [20:00<15:31:04, 12.31s/it]  2%|â–         | 84/4620 [20:13<15:26:10, 12.25s/it]  2%|â–         | 85/4620 [20:25<15:29:47, 12.30s/it]  2%|â–         | 86/4620 [20:37<15:25:49, 12.25s/it]  2%|â–         | 87/4620 [20:52<16:27:22, 13.07s/it]  2%|â–         | 88/4620 [21:05<16:21:43, 13.00s/it]  2%|â–         | 89/4620 [21:16<15:40:40, 12.46s/it]  2%|â–         | 90/4620 [21:28<15:31:37, 12.34s/it]                                                    {'loss': 0.1643, 'grad_norm': 0.703125, 'learning_rate': 0.00045000000000000004, 'epoch': 0.1}
  2%|â–         | 90/4620 [21:28<15:31:37, 12.34s/it]  2%|â–         | 91/4620 [21:41<15:46:02, 12.53s/it]  2%|â–         | 92/4620 [21:53<15:25:36, 12.27s/it]  2%|â–         | 93/4620 [22:05<15:21:12, 12.21s/it]  2%|â–         | 94/4620 [22:17<15:18:27, 12.18s/it]  2%|â–         | 95/4620 [22:29<15:22:14, 12.23s/it]  2%|â–         | 96/4620 [22:43<15:49:28, 12.59s/it]  2%|â–         | 97/4620 [22:57<16:31:40, 13.16s/it]  2%|â–         | 98/4620 [23:09<15:53:23, 12.65s/it]  2%|â–         | 99/4620 [23:20<15:27:16, 12.31s/it]  2%|â–         | 100/4620 [23:34<15:52:08, 12.64s/it]                                                     {'loss': 0.1625, 'grad_norm': 0.609375, 'learning_rate': 0.0005, 'epoch': 0.11}
  2%|â–         | 100/4620 [23:34<15:52:08, 12.64s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:01<00:12,  1.11it/s][A
 19%|â–ˆâ–‰        | 3/16 [00:03<00:15,  1.22s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:05<00:17,  1.44s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:06<00:16,  1.49s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:08<00:15,  1.51s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:09<00:13,  1.51s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:11<00:12,  1.53s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:13<00:11,  1.59s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:15<00:09,  1.65s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:16<00:08,  1.61s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:18<00:06,  1.59s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:19<00:04,  1.57s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:21<00:03,  1.57s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:22<00:01,  1.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:25<00:00,  1.84s/it][A                                                     
                                               [A{'eval_loss': 0.11853552609682083, 'eval_runtime': 71.4132, 'eval_samples_per_second': 7.002, 'eval_steps_per_second': 0.224, 'epoch': 0.11}
  2%|â–         | 100/4620 [24:45<15:52:08, 12.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:25<00:00,  1.84s/it][A
                                               [A  2%|â–         | 101/4620 [24:56<41:58:00, 33.43s/it]  2%|â–         | 102/4620 [25:07<33:48:48, 26.94s/it]  2%|â–         | 103/4620 [25:19<28:12:48, 22.49s/it]  2%|â–         | 104/4620 [25:31<24:13:30, 19.31s/it]  2%|â–         | 105/4620 [25:45<22:07:34, 17.64s/it]  2%|â–         | 106/4620 [25:57<19:51:20, 15.84s/it]  2%|â–         | 107/4620 [26:10<19:02:52, 15.19s/it]  2%|â–         | 108/4620 [26:22<17:38:45, 14.08s/it]  2%|â–         | 109/4620 [26:34<16:53:32, 13.48s/it]  2%|â–         | 110/4620 [26:48<16:56:53, 13.53s/it]                                                     {'loss': 0.137, 'grad_norm': 2.375, 'learning_rate': 0.0004988938053097345, 'epoch': 0.12}
  2%|â–         | 110/4620 [26:48<16:56:53, 13.53s/it]  2%|â–         | 111/4620 [26:59<16:11:12, 12.92s/it]  2%|â–         | 112/4620 [27:11<15:46:39, 12.60s/it]  2%|â–         | 113/4620 [27:23<15:35:59, 12.46s/it]  2%|â–         | 114/4620 [27:35<15:26:17, 12.33s/it]  2%|â–         | 115/4620 [27:49<15:56:33, 12.74s/it]  3%|â–Ž         | 116/4620 [28:02<16:15:39, 13.00s/it]  3%|â–Ž         | 117/4620 [28:16<16:23:57, 13.11s/it]  3%|â–Ž         | 118/4620 [28:28<15:53:08, 12.70s/it]  3%|â–Ž         | 119/4620 [28:40<15:42:20, 12.56s/it]  3%|â–Ž         | 120/4620 [28:53<15:48:49, 12.65s/it]                                                     {'loss': 0.1417, 'grad_norm': 0.66796875, 'learning_rate': 0.000497787610619469, 'epoch': 0.13}
  3%|â–Ž         | 120/4620 [28:53<15:48:49, 12.65s/it]  3%|â–Ž         | 121/4620 [29:05<15:38:48, 12.52s/it]  3%|â–Ž         | 122/4620 [29:17<15:32:54, 12.44s/it]  3%|â–Ž         | 123/4620 [29:29<15:25:03, 12.34s/it]  3%|â–Ž         | 124/4620 [29:43<15:52:50, 12.72s/it]  3%|â–Ž         | 125/4620 [29:54<15:23:17, 12.32s/it]  3%|â–Ž         | 126/4620 [30:08<15:46:38, 12.64s/it]  3%|â–Ž         | 127/4620 [30:21<15:58:24, 12.80s/it]  3%|â–Ž         | 128/4620 [30:32<15:25:21, 12.36s/it]  3%|â–Ž         | 129/4620 [30:44<15:22:31, 12.33s/it]  3%|â–Ž         | 130/4620 [30:58<15:41:54, 12.59s/it]                                                     {'loss': 0.1545, 'grad_norm': 0.9453125, 'learning_rate': 0.0004966814159292035, 'epoch': 0.14}
  3%|â–Ž         | 130/4620 [30:58<15:41:54, 12.59s/it]  3%|â–Ž         | 131/4620 [31:09<15:22:24, 12.33s/it]  3%|â–Ž         | 132/4620 [31:21<15:07:58, 12.14s/it]  3%|â–Ž         | 133/4620 [31:33<15:03:36, 12.08s/it]  3%|â–Ž         | 134/4620 [31:47<15:42:49, 12.61s/it]  3%|â–Ž         | 135/4620 [31:58<15:17:56, 12.28s/it]  3%|â–Ž         | 136/4620 [32:11<15:32:29, 12.48s/it]  3%|â–Ž         | 137/4620 [32:25<15:51:15, 12.73s/it]  3%|â–Ž         | 138/4620 [32:36<15:21:09, 12.33s/it]  3%|â–Ž         | 139/4620 [32:48<15:16:26, 12.27s/it]  3%|â–Ž         | 140/4620 [33:01<15:34:36, 12.52s/it]                                                     {'loss': 0.1274, 'grad_norm': 0.671875, 'learning_rate': 0.000495575221238938, 'epoch': 0.15}
  3%|â–Ž         | 140/4620 [33:01<15:34:36, 12.52s/it]  3%|â–Ž         | 141/4620 [33:13<15:14:20, 12.25s/it]  3%|â–Ž         | 142/4620 [33:25<15:10:21, 12.20s/it]  3%|â–Ž         | 143/4620 [33:38<15:35:53, 12.54s/it]  3%|â–Ž         | 144/4620 [33:50<15:15:32, 12.27s/it]  3%|â–Ž         | 145/4620 [34:02<15:04:28, 12.13s/it]  3%|â–Ž         | 146/4620 [34:15<15:39:30, 12.60s/it]  3%|â–Ž         | 147/4620 [34:28<15:49:09, 12.73s/it]  3%|â–Ž         | 148/4620 [34:40<15:16:19, 12.29s/it]  3%|â–Ž         | 149/4620 [34:52<15:08:56, 12.20s/it]  3%|â–Ž         | 150/4620 [35:05<15:38:19, 12.60s/it]                                                     {'loss': 0.147, 'grad_norm': 0.78515625, 'learning_rate': 0.0004944690265486726, 'epoch': 0.16}
  3%|â–Ž         | 150/4620 [35:05<15:38:19, 12.60s/it]  3%|â–Ž         | 151/4620 [35:17<15:15:44, 12.29s/it]  3%|â–Ž         | 152/4620 [35:29<15:09:49, 12.22s/it]  3%|â–Ž         | 153/4620 [35:43<15:43:21, 12.67s/it]  3%|â–Ž         | 154/4620 [35:54<15:15:48, 12.30s/it]  3%|â–Ž         | 155/4620 [36:06<15:05:45, 12.17s/it]  3%|â–Ž         | 156/4620 [36:19<15:33:30, 12.55s/it]  3%|â–Ž         | 157/4620 [36:32<15:41:40, 12.66s/it]  3%|â–Ž         | 158/4620 [36:44<15:20:01, 12.37s/it]  3%|â–Ž         | 159/4620 [36:56<15:05:35, 12.18s/it]  3%|â–Ž         | 160/4620 [37:09<15:24:01, 12.43s/it]                                                     {'loss': 0.1352, 'grad_norm': 0.578125, 'learning_rate': 0.000493362831858407, 'epoch': 0.17}
  3%|â–Ž         | 160/4620 [37:09<15:24:01, 12.43s/it]  3%|â–Ž         | 161/4620 [37:20<15:09:03, 12.23s/it]  4%|â–Ž         | 162/4620 [37:32<15:01:35, 12.13s/it]  4%|â–Ž         | 163/4620 [37:46<15:30:10, 12.52s/it]  4%|â–Ž         | 164/4620 [37:57<15:10:20, 12.26s/it]  4%|â–Ž         | 165/4620 [38:10<15:26:31, 12.48s/it]  4%|â–Ž         | 166/4620 [38:22<15:06:16, 12.21s/it]  4%|â–Ž         | 167/4620 [38:36<15:39:08, 12.65s/it]  4%|â–Ž         | 168/4620 [38:47<15:17:33, 12.37s/it]  4%|â–Ž         | 169/4620 [38:59<15:09:54, 12.27s/it]  4%|â–Ž         | 170/4620 [39:12<15:26:35, 12.49s/it]                                                     {'loss': 0.1328, 'grad_norm': 1.1640625, 'learning_rate': 0.0004922566371681417, 'epoch': 0.18}
  4%|â–Ž         | 170/4620 [39:12<15:26:35, 12.49s/it]  4%|â–Ž         | 171/4620 [39:24<15:14:34, 12.33s/it]  4%|â–Ž         | 172/4620 [39:36<15:06:15, 12.22s/it]  4%|â–Ž         | 173/4620 [39:49<15:20:12, 12.42s/it]  4%|â–         | 174/4620 [40:01<15:01:33, 12.17s/it]  4%|â–         | 175/4620 [40:14<15:21:20, 12.44s/it]  4%|â–         | 176/4620 [40:26<15:15:21, 12.36s/it]  4%|â–         | 177/4620 [40:40<15:42:34, 12.73s/it]  4%|â–         | 178/4620 [40:51<15:08:34, 12.27s/it]  4%|â–         | 179/4620 [41:03<15:05:11, 12.23s/it]  4%|â–         | 180/4620 [41:16<15:29:40, 12.56s/it]                                                     {'loss': 0.152, 'grad_norm': 0.6875, 'learning_rate': 0.0004911504424778762, 'epoch': 0.19}
  4%|â–         | 180/4620 [41:16<15:29:40, 12.56s/it]  4%|â–         | 181/4620 [41:28<15:09:19, 12.29s/it]  4%|â–         | 182/4620 [41:41<15:26:14, 12.52s/it]  4%|â–         | 183/4620 [41:52<15:02:11, 12.20s/it]  4%|â–         | 184/4620 [42:06<15:24:09, 12.50s/it]  4%|â–         | 185/4620 [42:17<15:00:14, 12.18s/it]  4%|â–         | 186/4620 [42:29<14:56:06, 12.13s/it]  4%|â–         | 187/4620 [42:43<15:28:03, 12.56s/it]  4%|â–         | 188/4620 [42:55<15:13:51, 12.37s/it]  4%|â–         | 189/4620 [43:07<15:10:04, 12.32s/it]  4%|â–         | 190/4620 [43:20<15:30:23, 12.60s/it]                                                     {'loss': 0.1344, 'grad_norm': 0.7265625, 'learning_rate': 0.0004900442477876107, 'epoch': 0.21}
  4%|â–         | 190/4620 [43:20<15:30:23, 12.60s/it]  4%|â–         | 191/4620 [43:32<15:12:45, 12.37s/it]  4%|â–         | 192/4620 [43:45<15:39:19, 12.73s/it]  4%|â–         | 193/4620 [43:57<15:14:55, 12.40s/it]  4%|â–         | 194/4620 [44:10<15:28:07, 12.58s/it]  4%|â–         | 195/4620 [44:22<15:04:05, 12.26s/it]  4%|â–         | 196/4620 [44:34<15:17:10, 12.44s/it]  4%|â–         | 197/4620 [44:48<15:34:27, 12.68s/it]  4%|â–         | 198/4620 [44:59<15:05:53, 12.29s/it]  4%|â–         | 199/4620 [45:11<14:57:06, 12.18s/it]  4%|â–         | 200/4620 [45:24<15:23:21, 12.53s/it]                                                     {'loss': 0.1251, 'grad_norm': 0.4765625, 'learning_rate': 0.0004889380530973452, 'epoch': 0.22}
  4%|â–         | 200/4620 [45:24<15:23:21, 12.53s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:09<01:08,  4.89s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:11<00:46,  3.61s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:13<00:34,  2.86s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:14<00:26,  2.42s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:21,  2.18s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:18<00:18,  2.04s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:15,  1.92s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:21<00:12,  1.80s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.69s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:28<00:05,  1.73s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.70s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:31<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.87s/it][A                                                     
                                               [A{'eval_loss': 0.09531568735837936, 'eval_runtime': 96.1123, 'eval_samples_per_second': 5.202, 'eval_steps_per_second': 0.166, 'epoch': 0.22}
  4%|â–         | 200/4620 [47:01<15:23:21, 12.53s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.87s/it][A
                                               [A  4%|â–         | 201/4620 [47:11<49:56:28, 40.69s/it]  4%|â–         | 202/4620 [47:24<39:39:26, 32.31s/it]  4%|â–         | 203/4620 [47:37<32:47:32, 26.73s/it]  4%|â–         | 204/4620 [47:48<27:04:18, 22.07s/it]  4%|â–         | 205/4620 [48:01<23:23:58, 19.08s/it]  4%|â–         | 206/4620 [48:13<20:51:57, 17.02s/it]  4%|â–         | 207/4620 [48:27<19:47:22, 16.14s/it]  5%|â–         | 208/4620 [48:38<18:05:27, 14.76s/it]  5%|â–         | 209/4620 [48:50<17:05:24, 13.95s/it]  5%|â–         | 210/4620 [49:05<17:08:04, 13.99s/it]                                                     {'loss': 0.1132, 'grad_norm': 0.416015625, 'learning_rate': 0.00048783185840707967, 'epoch': 0.23}
  5%|â–         | 210/4620 [49:05<17:08:04, 13.99s/it]  5%|â–         | 211/4620 [49:17<16:44:02, 13.66s/it]  5%|â–         | 212/4620 [49:30<16:26:22, 13.43s/it]  5%|â–         | 213/4620 [49:42<15:45:39, 12.87s/it]  5%|â–         | 214/4620 [49:54<15:30:32, 12.67s/it]  5%|â–         | 215/4620 [50:07<15:26:42, 12.62s/it]  5%|â–         | 216/4620 [50:20<15:45:06, 12.88s/it]  5%|â–         | 217/4620 [50:32<15:14:33, 12.46s/it]  5%|â–         | 218/4620 [50:43<15:01:15, 12.28s/it]  5%|â–         | 219/4620 [50:56<14:59:05, 12.26s/it]  5%|â–         | 220/4620 [51:09<15:22:28, 12.58s/it]                                                     {'loss': 0.137, 'grad_norm': 0.259765625, 'learning_rate': 0.0004867256637168142, 'epoch': 0.24}
  5%|â–         | 220/4620 [51:09<15:22:28, 12.58s/it]  5%|â–         | 221/4620 [51:21<15:05:34, 12.35s/it]  5%|â–         | 222/4620 [51:33<14:58:11, 12.25s/it]  5%|â–         | 223/4620 [51:46<15:28:31, 12.67s/it]  5%|â–         | 224/4620 [51:58<15:09:40, 12.42s/it]  5%|â–         | 225/4620 [52:12<15:34:53, 12.76s/it]  5%|â–         | 226/4620 [52:24<15:17:39, 12.53s/it]  5%|â–         | 227/4620 [52:36<15:10:50, 12.44s/it]  5%|â–         | 228/4620 [52:48<15:04:44, 12.36s/it]  5%|â–         | 229/4620 [53:02<15:25:54, 12.65s/it]  5%|â–         | 230/4620 [53:15<15:32:50, 12.75s/it]                                                     {'loss': 0.1308, 'grad_norm': 1.3125, 'learning_rate': 0.0004856194690265487, 'epoch': 0.25}
  5%|â–         | 230/4620 [53:15<15:32:50, 12.75s/it]  5%|â–Œ         | 231/4620 [53:26<15:06:34, 12.39s/it]  5%|â–Œ         | 232/4620 [53:38<14:59:54, 12.31s/it]  5%|â–Œ         | 233/4620 [53:51<15:18:23, 12.56s/it]  5%|â–Œ         | 234/4620 [54:04<15:09:28, 12.44s/it]  5%|â–Œ         | 235/4620 [54:17<15:28:16, 12.70s/it]  5%|â–Œ         | 236/4620 [54:29<15:09:44, 12.45s/it]  5%|â–Œ         | 237/4620 [54:41<15:07:41, 12.43s/it]  5%|â–Œ         | 238/4620 [54:53<15:01:16, 12.34s/it]  5%|â–Œ         | 239/4620 [55:05<15:00:04, 12.33s/it]  5%|â–Œ         | 240/4620 [55:20<15:53:32, 13.06s/it]                                                     {'loss': 0.1196, 'grad_norm': 0.447265625, 'learning_rate': 0.0004845132743362832, 'epoch': 0.26}
  5%|â–Œ         | 240/4620 [55:20<15:53:32, 13.06s/it]  5%|â–Œ         | 241/4620 [55:32<15:14:35, 12.53s/it]  5%|â–Œ         | 242/4620 [55:44<15:06:05, 12.42s/it]  5%|â–Œ         | 243/4620 [55:58<15:45:55, 12.97s/it]  5%|â–Œ         | 244/4620 [56:10<15:16:14, 12.56s/it]  5%|â–Œ         | 245/4620 [56:23<15:37:20, 12.85s/it]  5%|â–Œ         | 246/4620 [56:35<15:15:58, 12.56s/it]  5%|â–Œ         | 247/4620 [56:47<14:58:27, 12.33s/it]  5%|â–Œ         | 248/4620 [56:59<14:57:00, 12.31s/it]  5%|â–Œ         | 249/4620 [57:11<14:51:49, 12.24s/it]  5%|â–Œ         | 250/4620 [57:25<15:20:09, 12.63s/it]                                                     {'loss': 0.1234, 'grad_norm': 0.609375, 'learning_rate': 0.0004834070796460177, 'epoch': 0.27}
  5%|â–Œ         | 250/4620 [57:25<15:20:09, 12.63s/it]  5%|â–Œ         | 251/4620 [57:36<15:01:48, 12.38s/it]  5%|â–Œ         | 252/4620 [57:49<14:53:56, 12.28s/it]  5%|â–Œ         | 253/4620 [58:01<14:48:23, 12.21s/it]  5%|â–Œ         | 254/4620 [58:15<15:28:02, 12.75s/it]  6%|â–Œ         | 255/4620 [58:26<15:05:53, 12.45s/it]  6%|â–Œ         | 256/4620 [58:39<15:02:22, 12.41s/it]  6%|â–Œ         | 257/4620 [58:51<15:02:52, 12.42s/it]  6%|â–Œ         | 258/4620 [59:03<14:55:37, 12.32s/it]  6%|â–Œ         | 259/4620 [59:16<14:59:56, 12.38s/it]  6%|â–Œ         | 260/4620 [59:29<15:21:16, 12.68s/it]                                                     {'loss': 0.1071, 'grad_norm': 0.4375, 'learning_rate': 0.0004823008849557522, 'epoch': 0.28}
  6%|â–Œ         | 260/4620 [59:29<15:21:16, 12.68s/it]  6%|â–Œ         | 261/4620 [59:41<14:57:34, 12.35s/it]  6%|â–Œ         | 262/4620 [59:53<14:51:30, 12.27s/it]  6%|â–Œ         | 263/4620 [1:00:05<14:50:26, 12.26s/it]  6%|â–Œ         | 264/4620 [1:00:18<15:16:52, 12.63s/it]  6%|â–Œ         | 265/4620 [1:00:30<14:56:32, 12.35s/it]  6%|â–Œ         | 266/4620 [1:00:42<14:51:39, 12.29s/it]  6%|â–Œ         | 267/4620 [1:00:55<14:52:51, 12.31s/it]  6%|â–Œ         | 268/4620 [1:01:07<14:49:44, 12.27s/it]  6%|â–Œ         | 269/4620 [1:01:19<14:50:42, 12.28s/it]  6%|â–Œ         | 270/4620 [1:01:33<15:14:43, 12.62s/it]                                                       {'loss': 0.112, 'grad_norm': 0.51171875, 'learning_rate': 0.00048119469026548675, 'epoch': 0.29}
  6%|â–Œ         | 270/4620 [1:01:33<15:14:43, 12.62s/it]  6%|â–Œ         | 271/4620 [1:01:46<15:32:15, 12.86s/it]  6%|â–Œ         | 272/4620 [1:01:58<15:08:48, 12.54s/it]  6%|â–Œ         | 273/4620 [1:02:10<15:01:37, 12.44s/it]  6%|â–Œ         | 274/4620 [1:02:24<15:24:48, 12.77s/it]  6%|â–Œ         | 275/4620 [1:02:35<14:57:57, 12.40s/it]  6%|â–Œ         | 276/4620 [1:02:47<14:46:41, 12.25s/it]  6%|â–Œ         | 277/4620 [1:02:59<14:43:21, 12.20s/it]  6%|â–Œ         | 278/4620 [1:03:11<14:39:32, 12.15s/it]  6%|â–Œ         | 279/4620 [1:03:23<14:42:08, 12.19s/it]  6%|â–Œ         | 280/4620 [1:03:37<15:14:44, 12.65s/it]                                                       {'loss': 0.126, 'grad_norm': 0.59375, 'learning_rate': 0.00048008849557522126, 'epoch': 0.3}
  6%|â–Œ         | 280/4620 [1:03:37<15:14:44, 12.65s/it]  6%|â–Œ         | 281/4620 [1:03:50<15:20:26, 12.73s/it]  6%|â–Œ         | 282/4620 [1:04:01<14:53:24, 12.36s/it]  6%|â–Œ         | 283/4620 [1:04:14<14:52:24, 12.35s/it]  6%|â–Œ         | 284/4620 [1:04:28<15:25:08, 12.80s/it]  6%|â–Œ         | 285/4620 [1:04:39<14:50:43, 12.33s/it]  6%|â–Œ         | 286/4620 [1:04:51<14:43:10, 12.23s/it]  6%|â–Œ         | 287/4620 [1:05:03<14:39:13, 12.17s/it]  6%|â–Œ         | 288/4620 [1:05:15<14:39:26, 12.18s/it]  6%|â–‹         | 289/4620 [1:05:27<14:40:54, 12.20s/it]  6%|â–‹         | 290/4620 [1:05:40<14:59:17, 12.46s/it]                                                       {'loss': 0.1075, 'grad_norm': 0.349609375, 'learning_rate': 0.00047898230088495576, 'epoch': 0.31}
  6%|â–‹         | 290/4620 [1:05:40<14:59:17, 12.46s/it]  6%|â–‹         | 291/4620 [1:05:54<15:25:41, 12.83s/it]  6%|â–‹         | 292/4620 [1:06:06<15:03:09, 12.52s/it]  6%|â–‹         | 293/4620 [1:06:20<15:27:29, 12.86s/it]  6%|â–‹         | 294/4620 [1:06:33<15:36:38, 12.99s/it]  6%|â–‹         | 295/4620 [1:06:44<15:03:08, 12.53s/it]  6%|â–‹         | 296/4620 [1:06:56<14:48:27, 12.33s/it]  6%|â–‹         | 297/4620 [1:07:08<14:37:10, 12.17s/it]  6%|â–‹         | 298/4620 [1:07:20<14:37:13, 12.18s/it]  6%|â–‹         | 299/4620 [1:07:32<14:37:01, 12.18s/it]  6%|â–‹         | 300/4620 [1:07:46<15:02:24, 12.53s/it]                                                       {'loss': 0.1245, 'grad_norm': 0.38671875, 'learning_rate': 0.0004778761061946903, 'epoch': 0.32}
  6%|â–‹         | 300/4620 [1:07:46<15:02:24, 12.53s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:10<01:09,  5.00s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:11<00:47,  3.63s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:13<00:34,  2.85s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:14<00:26,  2.41s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:21,  2.13s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:17<00:17,  1.93s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:14,  1.81s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:20<00:11,  1.71s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.69s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:27<00:05,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.93s/it][A                                                       
                                               [A{'eval_loss': 0.0792684480547905, 'eval_runtime': 87.9412, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 0.182, 'epoch': 0.32}
  6%|â–‹         | 300/4620 [1:09:14<15:02:24, 12.53s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.93s/it][A
                                               [A  7%|â–‹         | 301/4620 [1:09:25<46:22:17, 38.65s/it]  7%|â–‹         | 302/4620 [1:09:37<36:29:50, 30.43s/it]  7%|â–‹         | 303/4620 [1:09:50<30:20:31, 25.30s/it]  7%|â–‹         | 304/4620 [1:10:03<26:00:36, 21.70s/it]  7%|â–‹         | 305/4620 [1:10:15<22:27:55, 18.74s/it]  7%|â–‹         | 306/4620 [1:10:27<20:04:37, 16.75s/it]  7%|â–‹         | 307/4620 [1:10:39<18:27:20, 15.40s/it]  7%|â–‹         | 308/4620 [1:10:52<17:19:19, 14.46s/it]  7%|â–‹         | 309/4620 [1:11:04<16:29:03, 13.77s/it]  7%|â–‹         | 310/4620 [1:11:17<16:22:47, 13.68s/it]                                                       {'loss': 0.1294, 'grad_norm': 0.59765625, 'learning_rate': 0.00047676991150442477, 'epoch': 0.34}
  7%|â–‹         | 310/4620 [1:11:17<16:22:47, 13.68s/it]  7%|â–‹         | 311/4620 [1:11:30<16:07:23, 13.47s/it]  7%|â–‹         | 312/4620 [1:11:41<15:18:13, 12.79s/it]  7%|â–‹         | 313/4620 [1:11:55<15:32:53, 13.00s/it]  7%|â–‹         | 314/4620 [1:12:08<15:34:50, 13.03s/it]  7%|â–‹         | 315/4620 [1:12:19<14:57:04, 12.50s/it]  7%|â–‹         | 316/4620 [1:12:31<14:48:15, 12.38s/it]  7%|â–‹         | 317/4620 [1:12:44<14:47:20, 12.37s/it]  7%|â–‹         | 318/4620 [1:12:56<14:45:17, 12.35s/it]  7%|â–‹         | 319/4620 [1:13:09<15:02:34, 12.59s/it]  7%|â–‹         | 320/4620 [1:13:21<14:44:06, 12.34s/it]                                                       {'loss': 0.1201, 'grad_norm': 0.5078125, 'learning_rate': 0.00047566371681415933, 'epoch': 0.35}
  7%|â–‹         | 320/4620 [1:13:21<14:44:06, 12.34s/it]  7%|â–‹         | 321/4620 [1:13:35<15:14:40, 12.77s/it]  7%|â–‹         | 322/4620 [1:13:46<14:52:12, 12.46s/it]  7%|â–‹         | 323/4620 [1:14:00<15:06:47, 12.66s/it]  7%|â–‹         | 324/4620 [1:14:13<15:15:38, 12.79s/it]  7%|â–‹         | 325/4620 [1:14:25<14:59:04, 12.56s/it]  7%|â–‹         | 326/4620 [1:14:37<14:54:51, 12.50s/it]  7%|â–‹         | 327/4620 [1:14:49<14:46:12, 12.39s/it]  7%|â–‹         | 328/4620 [1:15:01<14:32:10, 12.19s/it]  7%|â–‹         | 329/4620 [1:15:14<14:57:39, 12.55s/it]  7%|â–‹         | 330/4620 [1:15:27<15:07:48, 12.70s/it]                                                       {'loss': 0.1247, 'grad_norm': 0.609375, 'learning_rate': 0.0004745575221238938, 'epoch': 0.36}
  7%|â–‹         | 330/4620 [1:15:27<15:07:48, 12.70s/it]  7%|â–‹         | 331/4620 [1:15:39<14:43:16, 12.36s/it]  7%|â–‹         | 332/4620 [1:15:52<14:49:42, 12.45s/it]  7%|â–‹         | 333/4620 [1:16:04<14:40:06, 12.32s/it]  7%|â–‹         | 334/4620 [1:16:16<14:50:53, 12.47s/it]  7%|â–‹         | 335/4620 [1:16:28<14:38:53, 12.31s/it]  7%|â–‹         | 336/4620 [1:16:40<14:29:28, 12.18s/it]  7%|â–‹         | 337/4620 [1:16:52<14:26:01, 12.13s/it]  7%|â–‹         | 338/4620 [1:17:06<15:00:36, 12.62s/it]  7%|â–‹         | 339/4620 [1:17:18<14:46:25, 12.42s/it]  7%|â–‹         | 340/4620 [1:17:31<15:01:10, 12.63s/it]                                                       {'loss': 0.1155, 'grad_norm': 0.4609375, 'learning_rate': 0.00047345132743362834, 'epoch': 0.37}
  7%|â–‹         | 340/4620 [1:17:31<15:01:10, 12.63s/it]  7%|â–‹         | 341/4620 [1:17:43<14:38:10, 12.31s/it]  7%|â–‹         | 342/4620 [1:17:56<14:59:20, 12.61s/it]  7%|â–‹         | 343/4620 [1:18:08<14:36:55, 12.30s/it]  7%|â–‹         | 344/4620 [1:18:21<14:54:17, 12.55s/it]  7%|â–‹         | 345/4620 [1:18:33<14:38:28, 12.33s/it]  7%|â–‹         | 346/4620 [1:18:45<14:33:52, 12.27s/it]  8%|â–Š         | 347/4620 [1:18:57<14:42:16, 12.39s/it]  8%|â–Š         | 348/4620 [1:19:09<14:31:09, 12.24s/it]  8%|â–Š         | 349/4620 [1:19:23<14:57:51, 12.61s/it]  8%|â–Š         | 350/4620 [1:19:36<15:09:55, 12.79s/it]                                                       {'loss': 0.122, 'grad_norm': 0.35546875, 'learning_rate': 0.00047234513274336284, 'epoch': 0.38}
  8%|â–Š         | 350/4620 [1:19:36<15:09:55, 12.79s/it]  8%|â–Š         | 351/4620 [1:19:49<15:15:14, 12.86s/it]  8%|â–Š         | 352/4620 [1:20:00<14:38:30, 12.35s/it]  8%|â–Š         | 353/4620 [1:20:13<14:57:54, 12.63s/it]  8%|â–Š         | 354/4620 [1:20:25<14:35:20, 12.31s/it]  8%|â–Š         | 355/4620 [1:20:37<14:29:49, 12.24s/it]  8%|â–Š         | 356/4620 [1:20:50<14:38:07, 12.36s/it]  8%|â–Š         | 357/4620 [1:21:02<14:37:11, 12.35s/it]  8%|â–Š         | 358/4620 [1:21:14<14:34:38, 12.31s/it]  8%|â–Š         | 359/4620 [1:21:27<14:55:23, 12.61s/it]  8%|â–Š         | 360/4620 [1:21:41<15:06:07, 12.76s/it]                                                       {'loss': 0.1235, 'grad_norm': 0.396484375, 'learning_rate': 0.00047123893805309735, 'epoch': 0.39}
  8%|â–Š         | 360/4620 [1:21:41<15:06:07, 12.76s/it]  8%|â–Š         | 361/4620 [1:21:54<15:19:02, 12.95s/it]  8%|â–Š         | 362/4620 [1:22:06<14:52:02, 12.57s/it]  8%|â–Š         | 363/4620 [1:22:19<15:01:27, 12.71s/it]  8%|â–Š         | 364/4620 [1:22:31<14:46:00, 12.49s/it]  8%|â–Š         | 365/4620 [1:22:43<14:37:00, 12.37s/it]  8%|â–Š         | 366/4620 [1:22:55<14:32:08, 12.30s/it]  8%|â–Š         | 367/4620 [1:23:07<14:37:35, 12.38s/it]  8%|â–Š         | 368/4620 [1:23:20<14:42:16, 12.45s/it]  8%|â–Š         | 369/4620 [1:23:34<15:03:27, 12.75s/it]  8%|â–Š         | 370/4620 [1:23:46<15:06:44, 12.80s/it]                                                       {'loss': 0.1182, 'grad_norm': 0.57421875, 'learning_rate': 0.00047013274336283185, 'epoch': 0.4}
  8%|â–Š         | 370/4620 [1:23:46<15:06:44, 12.80s/it]  8%|â–Š         | 371/4620 [1:23:58<14:38:45, 12.41s/it]  8%|â–Š         | 372/4620 [1:24:11<14:54:08, 12.63s/it]  8%|â–Š         | 373/4620 [1:24:23<14:32:32, 12.33s/it]  8%|â–Š         | 374/4620 [1:24:35<14:29:56, 12.29s/it]  8%|â–Š         | 375/4620 [1:24:47<14:30:00, 12.30s/it]  8%|â–Š         | 376/4620 [1:24:59<14:27:33, 12.27s/it]  8%|â–Š         | 377/4620 [1:25:11<14:22:14, 12.19s/it]  8%|â–Š         | 378/4620 [1:25:24<14:24:16, 12.22s/it]  8%|â–Š         | 379/4620 [1:25:36<14:29:30, 12.30s/it]  8%|â–Š         | 380/4620 [1:25:50<15:00:10, 12.74s/it]                                                       {'loss': 0.1139, 'grad_norm': 0.62890625, 'learning_rate': 0.0004690265486725664, 'epoch': 0.41}
  8%|â–Š         | 380/4620 [1:25:50<15:00:10, 12.74s/it]  8%|â–Š         | 381/4620 [1:26:01<14:31:37, 12.34s/it]  8%|â–Š         | 382/4620 [1:26:14<14:27:15, 12.28s/it]  8%|â–Š         | 383/4620 [1:26:27<14:52:29, 12.64s/it]  8%|â–Š         | 384/4620 [1:26:39<14:31:14, 12.34s/it]  8%|â–Š         | 385/4620 [1:26:51<14:29:11, 12.31s/it]  8%|â–Š         | 386/4620 [1:27:03<14:27:31, 12.29s/it]  8%|â–Š         | 387/4620 [1:27:15<14:25:31, 12.27s/it]  8%|â–Š         | 388/4620 [1:27:28<14:23:56, 12.25s/it]  8%|â–Š         | 389/4620 [1:27:41<14:44:14, 12.54s/it]  8%|â–Š         | 390/4620 [1:27:54<15:03:25, 12.81s/it]                                                       {'loss': 0.1079, 'grad_norm': 0.46875, 'learning_rate': 0.00046792035398230086, 'epoch': 0.42}
  8%|â–Š         | 390/4620 [1:27:54<15:03:25, 12.81s/it]  8%|â–Š         | 391/4620 [1:28:07<14:53:38, 12.68s/it]  8%|â–Š         | 392/4620 [1:28:19<14:45:24, 12.56s/it]  9%|â–Š         | 393/4620 [1:28:33<15:10:18, 12.92s/it]  9%|â–Š         | 394/4620 [1:28:44<14:46:08, 12.58s/it]  9%|â–Š         | 395/4620 [1:28:56<14:32:05, 12.38s/it]  9%|â–Š         | 396/4620 [1:29:08<14:23:11, 12.26s/it]  9%|â–Š         | 397/4620 [1:29:20<14:14:06, 12.14s/it]  9%|â–Š         | 398/4620 [1:29:34<14:45:47, 12.59s/it]  9%|â–Š         | 399/4620 [1:29:45<14:25:43, 12.31s/it]  9%|â–Š         | 400/4620 [1:29:59<14:48:03, 12.63s/it]                                                       {'loss': 0.1131, 'grad_norm': 0.48046875, 'learning_rate': 0.0004668141592920354, 'epoch': 0.43}
  9%|â–Š         | 400/4620 [1:29:59<14:48:03, 12.63s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:08<01:02,  4.46s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:10<00:43,  3.37s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:12<00:31,  2.65s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:25,  2.28s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:15<00:20,  2.05s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:16,  1.87s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:14,  1.76s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:11,  1.70s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:23<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:28<00:03,  1.58s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.90s/it][A                                                       
                                               [A{'eval_loss': 0.07905012369155884, 'eval_runtime': 86.0036, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 0.186, 'epoch': 0.43}
  9%|â–Š         | 400/4620 [1:31:25<14:48:03, 12.63s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.90s/it][A
                                               [A  9%|â–Š         | 401/4620 [1:31:35<44:15:14, 37.76s/it]  9%|â–Š         | 402/4620 [1:31:47<34:58:13, 29.85s/it]  9%|â–Š         | 403/4620 [1:32:00<29:12:56, 24.94s/it]  9%|â–Š         | 404/4620 [1:32:12<24:35:30, 21.00s/it]  9%|â–‰         | 405/4620 [1:32:24<21:29:14, 18.35s/it]  9%|â–‰         | 406/4620 [1:32:37<19:24:13, 16.58s/it]  9%|â–‰         | 407/4620 [1:32:49<17:53:19, 15.29s/it]  9%|â–‰         | 408/4620 [1:33:03<17:25:09, 14.89s/it]  9%|â–‰         | 409/4620 [1:33:16<16:53:02, 14.43s/it]  9%|â–‰         | 410/4620 [1:33:29<16:28:37, 14.09s/it]                                                       {'loss': 0.1036, 'grad_norm': 0.490234375, 'learning_rate': 0.00046570796460176987, 'epoch': 0.44}
  9%|â–‰         | 410/4620 [1:33:29<16:28:37, 14.09s/it]  9%|â–‰         | 411/4620 [1:33:41<15:31:23, 13.28s/it]  9%|â–‰         | 412/4620 [1:33:53<15:02:26, 12.87s/it]  9%|â–‰         | 413/4620 [1:34:07<15:24:26, 13.18s/it]  9%|â–‰         | 414/4620 [1:34:19<14:58:41, 12.82s/it]  9%|â–‰         | 415/4620 [1:34:31<14:43:10, 12.60s/it]  9%|â–‰         | 416/4620 [1:34:43<14:43:59, 12.62s/it]  9%|â–‰         | 417/4620 [1:34:56<14:39:41, 12.56s/it]  9%|â–‰         | 418/4620 [1:35:09<14:55:27, 12.79s/it]  9%|â–‰         | 419/4620 [1:35:22<14:54:08, 12.77s/it]  9%|â–‰         | 420/4620 [1:35:35<15:02:10, 12.89s/it]                                                       {'loss': 0.1309, 'grad_norm': 0.44140625, 'learning_rate': 0.00046460176991150443, 'epoch': 0.45}
  9%|â–‰         | 420/4620 [1:35:35<15:02:10, 12.89s/it]  9%|â–‰         | 421/4620 [1:35:47<14:45:28, 12.65s/it]  9%|â–‰         | 422/4620 [1:36:00<14:41:05, 12.59s/it]  9%|â–‰         | 423/4620 [1:36:13<15:01:30, 12.89s/it]  9%|â–‰         | 424/4620 [1:36:25<14:41:10, 12.60s/it]  9%|â–‰         | 425/4620 [1:36:38<14:38:08, 12.56s/it]  9%|â–‰         | 426/4620 [1:36:50<14:30:47, 12.46s/it]  9%|â–‰         | 427/4620 [1:37:03<14:37:55, 12.56s/it]  9%|â–‰         | 428/4620 [1:37:15<14:29:50, 12.45s/it]  9%|â–‰         | 429/4620 [1:37:29<14:58:43, 12.87s/it]  9%|â–‰         | 430/4620 [1:37:41<14:56:55, 12.84s/it]                                                       {'loss': 0.1099, 'grad_norm': 0.59765625, 'learning_rate': 0.000463495575221239, 'epoch': 0.47}
  9%|â–‰         | 430/4620 [1:37:41<14:56:55, 12.84s/it]  9%|â–‰         | 431/4620 [1:37:53<14:29:27, 12.45s/it]  9%|â–‰         | 432/4620 [1:38:05<14:24:30, 12.39s/it]  9%|â–‰         | 433/4620 [1:38:19<14:48:19, 12.73s/it]  9%|â–‰         | 434/4620 [1:38:30<14:19:44, 12.32s/it]  9%|â–‰         | 435/4620 [1:38:42<14:15:59, 12.27s/it]  9%|â–‰         | 436/4620 [1:38:56<14:50:00, 12.76s/it]  9%|â–‰         | 437/4620 [1:39:08<14:32:04, 12.51s/it]  9%|â–‰         | 438/4620 [1:39:20<14:26:16, 12.43s/it] 10%|â–‰         | 439/4620 [1:39:34<14:46:11, 12.72s/it] 10%|â–‰         | 440/4620 [1:39:46<14:46:18, 12.72s/it]                                                       {'loss': 0.1164, 'grad_norm': 0.6171875, 'learning_rate': 0.00046238938053097344, 'epoch': 0.48}
 10%|â–‰         | 440/4620 [1:39:47<14:46:18, 12.72s/it] 10%|â–‰         | 441/4620 [1:39:58<14:32:43, 12.53s/it] 10%|â–‰         | 442/4620 [1:40:11<14:25:55, 12.44s/it] 10%|â–‰         | 443/4620 [1:40:24<14:47:57, 12.75s/it] 10%|â–‰         | 444/4620 [1:40:36<14:26:51, 12.45s/it] 10%|â–‰         | 445/4620 [1:40:48<14:18:07, 12.33s/it] 10%|â–‰         | 446/4620 [1:41:00<14:13:03, 12.26s/it] 10%|â–‰         | 447/4620 [1:41:14<14:40:39, 12.66s/it] 10%|â–‰         | 448/4620 [1:41:27<14:45:53, 12.74s/it] 10%|â–‰         | 449/4620 [1:41:38<14:22:41, 12.41s/it] 10%|â–‰         | 450/4620 [1:41:51<14:36:05, 12.61s/it]                                                       {'loss': 0.1142, 'grad_norm': 0.34375, 'learning_rate': 0.000461283185840708, 'epoch': 0.49}
 10%|â–‰         | 450/4620 [1:41:51<14:36:05, 12.61s/it] 10%|â–‰         | 451/4620 [1:42:03<14:23:58, 12.43s/it] 10%|â–‰         | 452/4620 [1:42:15<14:12:10, 12.27s/it] 10%|â–‰         | 453/4620 [1:42:29<14:46:27, 12.76s/it] 10%|â–‰         | 454/4620 [1:42:41<14:23:19, 12.43s/it] 10%|â–‰         | 455/4620 [1:42:53<14:08:16, 12.22s/it] 10%|â–‰         | 456/4620 [1:43:05<14:07:24, 12.21s/it] 10%|â–‰         | 457/4620 [1:43:19<14:42:35, 12.72s/it] 10%|â–‰         | 458/4620 [1:43:31<14:44:01, 12.74s/it] 10%|â–‰         | 459/4620 [1:43:43<14:25:16, 12.48s/it] 10%|â–‰         | 460/4620 [1:43:57<14:40:32, 12.70s/it]                                                       {'loss': 0.099, 'grad_norm': 0.400390625, 'learning_rate': 0.0004601769911504425, 'epoch': 0.5}
 10%|â–‰         | 460/4620 [1:43:57<14:40:32, 12.70s/it] 10%|â–‰         | 461/4620 [1:44:08<14:23:21, 12.46s/it] 10%|â–ˆ         | 462/4620 [1:44:22<14:37:54, 12.67s/it] 10%|â–ˆ         | 463/4620 [1:44:33<14:16:48, 12.37s/it] 10%|â–ˆ         | 464/4620 [1:44:45<14:09:44, 12.27s/it] 10%|â–ˆ         | 465/4620 [1:44:57<14:07:13, 12.23s/it] 10%|â–ˆ         | 466/4620 [1:45:10<14:07:28, 12.24s/it] 10%|â–ˆ         | 467/4620 [1:45:23<14:32:31, 12.61s/it] 10%|â–ˆ         | 468/4620 [1:45:37<14:51:19, 12.88s/it] 10%|â–ˆ         | 469/4620 [1:45:48<14:25:16, 12.51s/it] 10%|â–ˆ         | 470/4620 [1:46:02<14:44:41, 12.79s/it]                                                       {'loss': 0.1064, 'grad_norm': 0.419921875, 'learning_rate': 0.000459070796460177, 'epoch': 0.51}
 10%|â–ˆ         | 470/4620 [1:46:02<14:44:41, 12.79s/it] 10%|â–ˆ         | 471/4620 [1:46:14<14:32:39, 12.62s/it] 10%|â–ˆ         | 472/4620 [1:46:28<14:56:32, 12.97s/it] 10%|â–ˆ         | 473/4620 [1:46:39<14:21:11, 12.46s/it] 10%|â–ˆ         | 474/4620 [1:46:51<14:09:24, 12.29s/it] 10%|â–ˆ         | 475/4620 [1:47:03<14:06:17, 12.25s/it] 10%|â–ˆ         | 476/4620 [1:47:16<14:10:05, 12.31s/it] 10%|â–ˆ         | 477/4620 [1:47:28<14:10:32, 12.32s/it] 10%|â–ˆ         | 478/4620 [1:47:41<14:29:48, 12.60s/it] 10%|â–ˆ         | 479/4620 [1:47:53<14:15:18, 12.39s/it] 10%|â–ˆ         | 480/4620 [1:48:07<14:40:04, 12.75s/it]                                                       {'loss': 0.1069, 'grad_norm': 0.41015625, 'learning_rate': 0.0004579646017699115, 'epoch': 0.52}
 10%|â–ˆ         | 480/4620 [1:48:07<14:40:04, 12.75s/it] 10%|â–ˆ         | 481/4620 [1:48:20<14:45:49, 12.84s/it] 10%|â–ˆ         | 482/4620 [1:48:31<14:23:19, 12.52s/it] 10%|â–ˆ         | 483/4620 [1:48:44<14:22:32, 12.51s/it] 10%|â–ˆ         | 484/4620 [1:48:56<14:21:26, 12.50s/it] 10%|â–ˆ         | 485/4620 [1:49:09<14:18:34, 12.46s/it] 11%|â–ˆ         | 486/4620 [1:49:21<14:17:18, 12.44s/it] 11%|â–ˆ         | 487/4620 [1:49:35<14:41:14, 12.79s/it] 11%|â–ˆ         | 488/4620 [1:49:48<14:48:44, 12.91s/it] 11%|â–ˆ         | 489/4620 [1:49:59<14:17:37, 12.46s/it] 11%|â–ˆ         | 490/4620 [1:50:13<14:36:48, 12.74s/it]                                                       {'loss': 0.1135, 'grad_norm': 0.494140625, 'learning_rate': 0.00045685840707964607, 'epoch': 0.53}
 11%|â–ˆ         | 490/4620 [1:50:13<14:36:48, 12.74s/it] 11%|â–ˆ         | 491/4620 [1:50:26<14:53:30, 12.98s/it] 11%|â–ˆ         | 492/4620 [1:50:38<14:22:28, 12.54s/it] 11%|â–ˆ         | 493/4620 [1:50:50<14:09:22, 12.35s/it] 11%|â–ˆ         | 494/4620 [1:51:01<13:57:58, 12.19s/it] 11%|â–ˆ         | 495/4620 [1:51:14<14:03:15, 12.27s/it] 11%|â–ˆ         | 496/4620 [1:51:26<14:02:04, 12.25s/it] 11%|â–ˆ         | 497/4620 [1:51:39<14:21:55, 12.54s/it] 11%|â–ˆ         | 498/4620 [1:51:53<14:33:30, 12.71s/it] 11%|â–ˆ         | 499/4620 [1:52:04<14:15:09, 12.45s/it] 11%|â–ˆ         | 500/4620 [1:52:18<14:38:29, 12.79s/it]                                                       {'loss': 0.1053, 'grad_norm': 0.5625, 'learning_rate': 0.0004557522123893805, 'epoch': 0.54}
 11%|â–ˆ         | 500/4620 [1:52:18<14:38:29, 12.79s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:09<01:07,  4.81s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:11<00:46,  3.58s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:12<00:33,  2.77s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:14<00:26,  2.41s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:21,  2.11s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:17<00:17,  1.94s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:14,  1.85s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:21<00:12,  1.81s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.77s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.77s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.69s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:27<00:04,  1.65s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.62s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.91s/it][A                                                       
                                               [A{'eval_loss': 0.07023412734270096, 'eval_runtime': 86.5259, 'eval_samples_per_second': 5.779, 'eval_steps_per_second': 0.185, 'epoch': 0.54}
 11%|â–ˆ         | 500/4620 [1:53:45<14:38:29, 12.79s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.91s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 11%|â–ˆ         | 501/4620 [1:53:57<44:23:24, 38.80s/it] 11%|â–ˆ         | 502/4620 [1:54:08<34:51:12, 30.47s/it] 11%|â–ˆ         | 503/4620 [1:54:20<28:24:01, 24.83s/it] 11%|â–ˆ         | 504/4620 [1:54:32<24:02:04, 21.02s/it] 11%|â–ˆ         | 505/4620 [1:54:45<21:04:53, 18.44s/it] 11%|â–ˆ         | 506/4620 [1:54:57<19:01:17, 16.64s/it] 11%|â–ˆ         | 507/4620 [1:55:11<18:04:52, 15.83s/it] 11%|â–ˆ         | 508/4620 [1:55:23<16:48:27, 14.71s/it] 11%|â–ˆ         | 509/4620 [1:55:35<15:51:45, 13.89s/it] 11%|â–ˆ         | 510/4620 [1:55:49<15:48:53, 13.85s/it]                                                       {'loss': 0.094, 'grad_norm': 0.59765625, 'learning_rate': 0.0004546460176991151, 'epoch': 0.55}
 11%|â–ˆ         | 510/4620 [1:55:49<15:48:53, 13.85s/it] 11%|â–ˆ         | 511/4620 [1:56:00<14:59:28, 13.13s/it] 11%|â–ˆ         | 512/4620 [1:56:13<14:39:23, 12.84s/it] 11%|â–ˆ         | 513/4620 [1:56:25<14:25:19, 12.64s/it] 11%|â–ˆ         | 514/4620 [1:56:37<14:20:38, 12.58s/it] 11%|â–ˆ         | 515/4620 [1:56:49<14:15:07, 12.50s/it] 11%|â–ˆ         | 516/4620 [1:57:03<14:36:49, 12.82s/it] 11%|â–ˆ         | 517/4620 [1:57:15<14:20:49, 12.59s/it] 11%|â–ˆ         | 518/4620 [1:57:27<14:11:39, 12.46s/it] 11%|â–ˆ         | 519/4620 [1:57:41<14:31:13, 12.75s/it] 11%|â–ˆâ–        | 520/4620 [1:57:54<14:41:06, 12.89s/it]                                                       {'loss': 0.1001, 'grad_norm': 0.263671875, 'learning_rate': 0.00045353982300884953, 'epoch': 0.56}
 11%|â–ˆâ–        | 520/4620 [1:57:54<14:41:06, 12.89s/it] 11%|â–ˆâ–        | 521/4620 [1:58:05<14:14:29, 12.51s/it] 11%|â–ˆâ–        | 522/4620 [1:58:18<14:06:45, 12.40s/it] 11%|â–ˆâ–        | 523/4620 [1:58:30<14:04:06, 12.36s/it] 11%|â–ˆâ–        | 524/4620 [1:58:42<13:58:53, 12.29s/it] 11%|â–ˆâ–        | 525/4620 [1:58:54<13:58:27, 12.29s/it] 11%|â–ˆâ–        | 526/4620 [1:59:08<14:21:36, 12.63s/it] 11%|â–ˆâ–        | 527/4620 [1:59:20<14:04:31, 12.38s/it] 11%|â–ˆâ–        | 528/4620 [1:59:31<13:55:00, 12.24s/it] 11%|â–ˆâ–        | 529/4620 [1:59:46<14:32:46, 12.80s/it] 11%|â–ˆâ–        | 530/4620 [1:59:59<14:36:20, 12.86s/it]                                                       {'loss': 0.098, 'grad_norm': 0.3359375, 'learning_rate': 0.0004524336283185841, 'epoch': 0.57}
 11%|â–ˆâ–        | 530/4620 [1:59:59<14:36:20, 12.86s/it] 11%|â–ˆâ–        | 531/4620 [2:00:10<14:12:42, 12.51s/it] 12%|â–ˆâ–        | 532/4620 [2:00:22<13:59:24, 12.32s/it] 12%|â–ˆâ–        | 533/4620 [2:00:35<14:06:27, 12.43s/it] 12%|â–ˆâ–        | 534/4620 [2:00:47<14:08:58, 12.47s/it] 12%|â–ˆâ–        | 535/4620 [2:01:01<14:28:19, 12.75s/it] 12%|â–ˆâ–        | 536/4620 [2:01:13<14:10:09, 12.49s/it] 12%|â–ˆâ–        | 537/4620 [2:01:25<14:03:44, 12.40s/it] 12%|â–ˆâ–        | 538/4620 [2:01:37<13:59:20, 12.34s/it] 12%|â–ˆâ–        | 539/4620 [2:01:50<14:18:44, 12.63s/it] 12%|â–ˆâ–        | 540/4620 [2:02:03<14:27:54, 12.76s/it]                                                       {'loss': 0.1067, 'grad_norm': 0.435546875, 'learning_rate': 0.0004513274336283186, 'epoch': 0.58}
 12%|â–ˆâ–        | 540/4620 [2:02:03<14:27:54, 12.76s/it] 12%|â–ˆâ–        | 541/4620 [2:02:15<14:09:21, 12.49s/it] 12%|â–ˆâ–        | 542/4620 [2:02:28<14:04:35, 12.43s/it] 12%|â–ˆâ–        | 543/4620 [2:02:40<13:56:34, 12.31s/it] 12%|â–ˆâ–        | 544/4620 [2:02:52<13:52:12, 12.25s/it] 12%|â–ˆâ–        | 545/4620 [2:03:05<14:19:19, 12.65s/it] 12%|â–ˆâ–        | 546/4620 [2:03:19<14:31:43, 12.84s/it] 12%|â–ˆâ–        | 547/4620 [2:03:30<14:08:24, 12.50s/it] 12%|â–ˆâ–        | 548/4620 [2:03:42<13:58:07, 12.35s/it] 12%|â–ˆâ–        | 549/4620 [2:03:56<14:32:51, 12.86s/it] 12%|â–ˆâ–        | 550/4620 [2:04:09<14:36:19, 12.92s/it]                                                       {'loss': 0.1041, 'grad_norm': 0.42578125, 'learning_rate': 0.0004502212389380531, 'epoch': 0.6}
 12%|â–ˆâ–        | 550/4620 [2:04:09<14:36:19, 12.92s/it] 12%|â–ˆâ–        | 551/4620 [2:04:21<14:14:38, 12.60s/it] 12%|â–ˆâ–        | 552/4620 [2:04:34<14:10:43, 12.55s/it] 12%|â–ˆâ–        | 553/4620 [2:04:46<14:09:18, 12.53s/it] 12%|â–ˆâ–        | 554/4620 [2:05:00<14:36:46, 12.94s/it] 12%|â–ˆâ–        | 555/4620 [2:05:12<14:08:05, 12.52s/it] 12%|â–ˆâ–        | 556/4620 [2:05:25<14:30:51, 12.86s/it] 12%|â–ˆâ–        | 557/4620 [2:05:37<14:05:07, 12.48s/it] 12%|â–ˆâ–        | 558/4620 [2:05:50<14:17:14, 12.66s/it] 12%|â–ˆâ–        | 559/4620 [2:06:02<14:01:28, 12.43s/it] 12%|â–ˆâ–        | 560/4620 [2:06:15<14:26:13, 12.80s/it]                                                       {'loss': 0.1181, 'grad_norm': 0.32421875, 'learning_rate': 0.0004491150442477876, 'epoch': 0.61}
 12%|â–ˆâ–        | 560/4620 [2:06:16<14:26:13, 12.80s/it] 12%|â–ˆâ–        | 561/4620 [2:06:27<14:03:09, 12.46s/it] 12%|â–ˆâ–        | 562/4620 [2:06:39<13:45:40, 12.21s/it] 12%|â–ˆâ–        | 563/4620 [2:06:51<13:42:00, 12.16s/it] 12%|â–ˆâ–        | 564/4620 [2:07:05<14:13:57, 12.63s/it] 12%|â–ˆâ–        | 565/4620 [2:07:17<14:01:00, 12.44s/it] 12%|â–ˆâ–        | 566/4620 [2:07:30<14:18:02, 12.70s/it] 12%|â–ˆâ–        | 567/4620 [2:07:42<14:02:31, 12.47s/it] 12%|â–ˆâ–        | 568/4620 [2:07:56<14:32:26, 12.92s/it] 12%|â–ˆâ–        | 569/4620 [2:08:08<14:12:21, 12.62s/it] 12%|â–ˆâ–        | 570/4620 [2:08:21<14:29:35, 12.88s/it]                                                       {'loss': 0.1038, 'grad_norm': 0.458984375, 'learning_rate': 0.00044800884955752216, 'epoch': 0.62}
 12%|â–ˆâ–        | 570/4620 [2:08:21<14:29:35, 12.88s/it] 12%|â–ˆâ–        | 571/4620 [2:08:33<14:11:01, 12.61s/it] 12%|â–ˆâ–        | 572/4620 [2:08:45<14:02:46, 12.49s/it] 12%|â–ˆâ–        | 573/4620 [2:08:59<14:24:22, 12.82s/it] 12%|â–ˆâ–        | 574/4620 [2:09:11<14:01:24, 12.48s/it] 12%|â–ˆâ–        | 575/4620 [2:09:23<13:53:04, 12.36s/it] 12%|â–ˆâ–        | 576/4620 [2:09:37<14:22:56, 12.80s/it] 12%|â–ˆâ–        | 577/4620 [2:09:49<14:25:05, 12.84s/it] 13%|â–ˆâ–Ž        | 578/4620 [2:10:01<14:08:46, 12.60s/it] 13%|â–ˆâ–Ž        | 579/4620 [2:10:15<14:27:08, 12.88s/it] 13%|â–ˆâ–Ž        | 580/4620 [2:10:27<14:11:44, 12.65s/it]                                                       {'loss': 0.1091, 'grad_norm': 0.9296875, 'learning_rate': 0.0004469026548672566, 'epoch': 0.63}
 13%|â–ˆâ–Ž        | 580/4620 [2:10:27<14:11:44, 12.65s/it] 13%|â–ˆâ–Ž        | 581/4620 [2:10:39<14:02:31, 12.52s/it] 13%|â–ˆâ–Ž        | 582/4620 [2:10:54<14:36:33, 13.02s/it] 13%|â–ˆâ–Ž        | 583/4620 [2:11:05<14:10:24, 12.64s/it] 13%|â–ˆâ–Ž        | 584/4620 [2:11:17<14:01:25, 12.51s/it] 13%|â–ˆâ–Ž        | 585/4620 [2:11:30<13:55:41, 12.43s/it] 13%|â–ˆâ–Ž        | 586/4620 [2:11:44<14:24:03, 12.85s/it] 13%|â–ˆâ–Ž        | 587/4620 [2:11:57<14:30:41, 12.95s/it] 13%|â–ˆâ–Ž        | 588/4620 [2:12:09<14:08:12, 12.62s/it] 13%|â–ˆâ–Ž        | 589/4620 [2:12:22<14:20:23, 12.81s/it] 13%|â–ˆâ–Ž        | 590/4620 [2:12:34<14:14:38, 12.72s/it]                                                       {'loss': 0.0953, 'grad_norm': 0.73828125, 'learning_rate': 0.00044579646017699117, 'epoch': 0.64}
 13%|â–ˆâ–Ž        | 590/4620 [2:12:34<14:14:38, 12.72s/it] 13%|â–ˆâ–Ž        | 591/4620 [2:12:46<13:56:04, 12.45s/it] 13%|â–ˆâ–Ž        | 592/4620 [2:13:00<14:13:49, 12.72s/it] 13%|â–ˆâ–Ž        | 593/4620 [2:13:11<13:54:19, 12.43s/it] 13%|â–ˆâ–Ž        | 594/4620 [2:13:23<13:47:28, 12.33s/it] 13%|â–ˆâ–Ž        | 595/4620 [2:13:36<13:46:16, 12.32s/it] 13%|â–ˆâ–Ž        | 596/4620 [2:13:49<14:05:54, 12.61s/it] 13%|â–ˆâ–Ž        | 597/4620 [2:14:01<13:51:26, 12.40s/it] 13%|â–ˆâ–Ž        | 598/4620 [2:14:14<14:15:03, 12.76s/it] 13%|â–ˆâ–Ž        | 599/4620 [2:14:26<13:57:26, 12.50s/it] 13%|â–ˆâ–Ž        | 600/4620 [2:14:39<13:52:30, 12.43s/it]                                                       {'loss': 0.1222, 'grad_norm': 0.2265625, 'learning_rate': 0.0004446902654867257, 'epoch': 0.65}
 13%|â–ˆâ–Ž        | 600/4620 [2:14:39<13:52:30, 12.43s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:54,  3.89s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:40,  3.12s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:30,  2.54s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:24,  2.20s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  1.98s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:16,  1.88s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.80s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.70s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.64s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.62s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.68s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.88s/it][A                                                       
                                               [A{'eval_loss': 0.08185714483261108, 'eval_runtime': 90.2444, 'eval_samples_per_second': 5.541, 'eval_steps_per_second': 0.177, 'epoch': 0.65}
 13%|â–ˆâ–Ž        | 600/4620 [2:16:09<13:52:30, 12.43s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.88s/it][A
                                               [A 13%|â–ˆâ–Ž        | 601/4620 [2:16:19<43:23:50, 38.87s/it] 13%|â–ˆâ–Ž        | 602/4620 [2:16:32<34:43:59, 31.12s/it] 13%|â–ˆâ–Ž        | 603/4620 [2:16:44<28:18:54, 25.38s/it] 13%|â–ˆâ–Ž        | 604/4620 [2:16:56<23:54:29, 21.43s/it] 13%|â–ˆâ–Ž        | 605/4620 [2:17:09<20:56:47, 18.78s/it] 13%|â–ˆâ–Ž        | 606/4620 [2:17:25<19:55:55, 17.88s/it] 13%|â–ˆâ–Ž        | 607/4620 [2:17:36<17:47:37, 15.96s/it] 13%|â–ˆâ–Ž        | 608/4620 [2:17:50<17:00:56, 15.27s/it] 13%|â–ˆâ–Ž        | 609/4620 [2:18:02<15:48:14, 14.18s/it] 13%|â–ˆâ–Ž        | 610/4620 [2:18:14<15:06:11, 13.56s/it]                                                       {'loss': 0.1094, 'grad_norm': 0.36328125, 'learning_rate': 0.0004435840707964602, 'epoch': 0.66}
 13%|â–ˆâ–Ž        | 610/4620 [2:18:14<15:06:11, 13.56s/it] 13%|â–ˆâ–Ž        | 611/4620 [2:18:28<15:21:01, 13.78s/it] 13%|â–ˆâ–Ž        | 612/4620 [2:18:40<14:38:27, 13.15s/it] 13%|â–ˆâ–Ž        | 613/4620 [2:18:52<14:17:20, 12.84s/it] 13%|â–ˆâ–Ž        | 614/4620 [2:19:05<14:25:33, 12.96s/it] 13%|â–ˆâ–Ž        | 615/4620 [2:19:18<14:27:39, 13.00s/it] 13%|â–ˆâ–Ž        | 616/4620 [2:19:31<14:24:37, 12.96s/it] 13%|â–ˆâ–Ž        | 617/4620 [2:19:43<14:01:43, 12.62s/it] 13%|â–ˆâ–Ž        | 618/4620 [2:19:57<14:23:16, 12.94s/it] 13%|â–ˆâ–Ž        | 619/4620 [2:20:08<13:54:54, 12.52s/it] 13%|â–ˆâ–Ž        | 620/4620 [2:20:22<14:16:25, 12.85s/it]                                                       {'loss': 0.1007, 'grad_norm': 0.41796875, 'learning_rate': 0.00044247787610619474, 'epoch': 0.67}
 13%|â–ˆâ–Ž        | 620/4620 [2:20:22<14:16:25, 12.85s/it] 13%|â–ˆâ–Ž        | 621/4620 [2:20:34<14:00:56, 12.62s/it] 13%|â–ˆâ–Ž        | 622/4620 [2:20:45<13:43:15, 12.36s/it] 13%|â–ˆâ–Ž        | 623/4620 [2:20:58<13:40:19, 12.31s/it] 14%|â–ˆâ–Ž        | 624/4620 [2:21:10<13:42:43, 12.35s/it] 14%|â–ˆâ–Ž        | 625/4620 [2:21:24<14:06:06, 12.71s/it] 14%|â–ˆâ–Ž        | 626/4620 [2:21:37<14:23:55, 12.98s/it] 14%|â–ˆâ–Ž        | 627/4620 [2:21:49<13:59:44, 12.62s/it] 14%|â–ˆâ–Ž        | 628/4620 [2:22:02<14:14:29, 12.84s/it] 14%|â–ˆâ–Ž        | 629/4620 [2:22:16<14:24:13, 12.99s/it] 14%|â–ˆâ–Ž        | 630/4620 [2:22:28<14:07:15, 12.74s/it]                                                       {'loss': 0.1096, 'grad_norm': 0.427734375, 'learning_rate': 0.0004413716814159292, 'epoch': 0.68}
 14%|â–ˆâ–Ž        | 630/4620 [2:22:28<14:07:15, 12.74s/it] 14%|â–ˆâ–Ž        | 631/4620 [2:22:40<13:55:42, 12.57s/it] 14%|â–ˆâ–Ž        | 632/4620 [2:22:52<13:44:34, 12.41s/it] 14%|â–ˆâ–Ž        | 633/4620 [2:23:05<13:50:55, 12.50s/it] 14%|â–ˆâ–Ž        | 634/4620 [2:23:17<13:48:14, 12.47s/it] 14%|â–ˆâ–Ž        | 635/4620 [2:23:31<14:10:32, 12.81s/it] 14%|â–ˆâ–        | 636/4620 [2:23:44<14:13:13, 12.85s/it] 14%|â–ˆâ–        | 637/4620 [2:23:56<13:57:25, 12.61s/it] 14%|â–ˆâ–        | 638/4620 [2:24:10<14:18:10, 12.93s/it] 14%|â–ˆâ–        | 639/4620 [2:24:21<13:44:17, 12.42s/it] 14%|â–ˆâ–        | 640/4620 [2:24:34<13:59:41, 12.66s/it]                                                       {'loss': 0.097, 'grad_norm': 0.1875, 'learning_rate': 0.00044026548672566375, 'epoch': 0.69}
 14%|â–ˆâ–        | 640/4620 [2:24:34<13:59:41, 12.66s/it] 14%|â–ˆâ–        | 641/4620 [2:24:46<13:41:52, 12.39s/it] 14%|â–ˆâ–        | 642/4620 [2:24:58<13:39:13, 12.36s/it] 14%|â–ˆâ–        | 643/4620 [2:25:11<13:46:37, 12.47s/it] 14%|â–ˆâ–        | 644/4620 [2:25:24<14:05:54, 12.77s/it] 14%|â–ˆâ–        | 645/4620 [2:25:37<14:15:43, 12.92s/it] 14%|â–ˆâ–        | 646/4620 [2:25:49<13:53:50, 12.59s/it] 14%|â–ˆâ–        | 647/4620 [2:26:02<13:47:47, 12.50s/it] 14%|â–ˆâ–        | 648/4620 [2:26:16<14:18:38, 12.97s/it] 14%|â–ˆâ–        | 649/4620 [2:26:28<14:15:29, 12.93s/it] 14%|â–ˆâ–        | 650/4620 [2:26:42<14:21:03, 13.01s/it]                                                       {'loss': 0.0925, 'grad_norm': 0.302734375, 'learning_rate': 0.00043915929203539825, 'epoch': 0.7}
 14%|â–ˆâ–        | 650/4620 [2:26:42<14:21:03, 13.01s/it] 14%|â–ˆâ–        | 651/4620 [2:26:54<14:03:47, 12.76s/it] 14%|â–ˆâ–        | 652/4620 [2:27:06<13:56:57, 12.66s/it] 14%|â–ˆâ–        | 653/4620 [2:27:19<13:55:00, 12.63s/it] 14%|â–ˆâ–        | 654/4620 [2:27:32<14:13:49, 12.92s/it] 14%|â–ˆâ–        | 655/4620 [2:27:46<14:21:36, 13.04s/it] 14%|â–ˆâ–        | 656/4620 [2:27:57<13:52:15, 12.60s/it] 14%|â–ˆâ–        | 657/4620 [2:28:09<13:43:10, 12.46s/it] 14%|â–ˆâ–        | 658/4620 [2:28:23<14:10:23, 12.88s/it] 14%|â–ˆâ–        | 659/4620 [2:28:35<13:52:36, 12.61s/it] 14%|â–ˆâ–        | 660/4620 [2:28:49<14:10:11, 12.88s/it]                                                       {'loss': 0.1289, 'grad_norm': 0.26953125, 'learning_rate': 0.00043805309734513276, 'epoch': 0.71}
 14%|â–ˆâ–        | 660/4620 [2:28:49<14:10:11, 12.88s/it] 14%|â–ˆâ–        | 661/4620 [2:29:00<13:45:37, 12.51s/it] 14%|â–ˆâ–        | 662/4620 [2:29:13<13:38:10, 12.40s/it] 14%|â–ˆâ–        | 663/4620 [2:29:27<14:09:36, 12.88s/it] 14%|â–ˆâ–        | 664/4620 [2:29:40<14:22:41, 13.08s/it] 14%|â–ˆâ–        | 665/4620 [2:29:52<13:56:39, 12.69s/it] 14%|â–ˆâ–        | 666/4620 [2:30:04<13:48:49, 12.58s/it] 14%|â–ˆâ–        | 667/4620 [2:30:17<13:45:49, 12.53s/it] 14%|â–ˆâ–        | 668/4620 [2:30:29<13:45:29, 12.53s/it] 14%|â–ˆâ–        | 669/4620 [2:30:43<14:12:33, 12.95s/it] 15%|â–ˆâ–        | 670/4620 [2:30:55<13:54:33, 12.68s/it]                                                       {'loss': 0.1197, 'grad_norm': 0.59375, 'learning_rate': 0.00043694690265486726, 'epoch': 0.73}
 15%|â–ˆâ–        | 670/4620 [2:30:55<13:54:33, 12.68s/it] 15%|â–ˆâ–        | 671/4620 [2:31:09<14:11:12, 12.93s/it] 15%|â–ˆâ–        | 672/4620 [2:31:20<13:46:26, 12.56s/it] 15%|â–ˆâ–        | 673/4620 [2:31:34<14:05:28, 12.85s/it] 15%|â–ˆâ–        | 674/4620 [2:31:47<14:01:05, 12.79s/it] 15%|â–ˆâ–        | 675/4620 [2:31:59<13:48:51, 12.61s/it] 15%|â–ˆâ–        | 676/4620 [2:32:11<13:44:16, 12.54s/it] 15%|â–ˆâ–        | 677/4620 [2:32:24<13:42:26, 12.51s/it] 15%|â–ˆâ–        | 678/4620 [2:32:36<13:43:15, 12.53s/it] 15%|â–ˆâ–        | 679/4620 [2:32:50<14:03:10, 12.84s/it] 15%|â–ˆâ–        | 680/4620 [2:33:01<13:40:22, 12.49s/it]                                                       {'loss': 0.1028, 'grad_norm': 0.423828125, 'learning_rate': 0.00043584070796460177, 'epoch': 0.74}
 15%|â–ˆâ–        | 680/4620 [2:33:01<13:40:22, 12.49s/it] 15%|â–ˆâ–        | 681/4620 [2:33:15<14:06:51, 12.90s/it] 15%|â–ˆâ–        | 682/4620 [2:33:28<14:05:34, 12.88s/it] 15%|â–ˆâ–        | 683/4620 [2:33:41<14:10:07, 12.96s/it] 15%|â–ˆâ–        | 684/4620 [2:33:53<13:49:25, 12.64s/it] 15%|â–ˆâ–        | 685/4620 [2:34:05<13:42:31, 12.54s/it] 15%|â–ˆâ–        | 686/4620 [2:34:18<13:41:53, 12.54s/it] 15%|â–ˆâ–        | 687/4620 [2:34:30<13:40:13, 12.51s/it] 15%|â–ˆâ–        | 688/4620 [2:34:43<13:35:08, 12.44s/it] 15%|â–ˆâ–        | 689/4620 [2:34:57<14:06:28, 12.92s/it] 15%|â–ˆâ–        | 690/4620 [2:35:09<13:46:35, 12.62s/it]                                                       {'loss': 0.1039, 'grad_norm': 0.61328125, 'learning_rate': 0.00043473451327433627, 'epoch': 0.75}
 15%|â–ˆâ–        | 690/4620 [2:35:09<13:46:35, 12.62s/it] 15%|â–ˆâ–        | 691/4620 [2:35:22<14:02:35, 12.87s/it] 15%|â–ˆâ–        | 692/4620 [2:35:35<14:08:10, 12.96s/it] 15%|â–ˆâ–Œ        | 693/4620 [2:35:48<14:04:47, 12.91s/it] 15%|â–ˆâ–Œ        | 694/4620 [2:36:00<13:45:47, 12.62s/it] 15%|â–ˆâ–Œ        | 695/4620 [2:36:12<13:34:57, 12.46s/it] 15%|â–ˆâ–Œ        | 696/4620 [2:36:25<13:38:09, 12.51s/it] 15%|â–ˆâ–Œ        | 697/4620 [2:36:37<13:36:22, 12.49s/it] 15%|â–ˆâ–Œ        | 698/4620 [2:36:50<13:38:35, 12.52s/it] 15%|â–ˆâ–Œ        | 699/4620 [2:37:03<14:00:11, 12.86s/it] 15%|â–ˆâ–Œ        | 700/4620 [2:37:16<13:46:04, 12.64s/it]                                                       {'loss': 0.1158, 'grad_norm': 0.3046875, 'learning_rate': 0.00043362831858407083, 'epoch': 0.76}
 15%|â–ˆâ–Œ        | 700/4620 [2:37:16<13:46:04, 12.64s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:10<01:12,  5.16s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:12<00:52,  4.07s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:14<00:37,  3.13s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:15<00:28,  2.57s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:17<00:22,  2.24s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:19<00:18,  2.00s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:20<00:15,  1.90s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:22<00:12,  1.84s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:24<00:10,  1.79s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:25<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:27<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:28<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:30<00:03,  1.62s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:31<00:01,  1.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:34<00:00,  1.92s/it][A                                                       
                                               [A{'eval_loss': 0.07709811627864838, 'eval_runtime': 90.532, 'eval_samples_per_second': 5.523, 'eval_steps_per_second': 0.177, 'epoch': 0.76}
 15%|â–ˆâ–Œ        | 700/4620 [2:38:46<13:46:04, 12.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:34<00:00,  1.92s/it][A
                                               [A 15%|â–ˆâ–Œ        | 701/4620 [2:38:57<42:52:14, 39.38s/it] 15%|â–ˆâ–Œ        | 702/4620 [2:39:10<34:18:23, 31.52s/it] 15%|â–ˆâ–Œ        | 703/4620 [2:39:22<27:53:41, 25.64s/it] 15%|â–ˆâ–Œ        | 704/4620 [2:39:35<23:33:09, 21.65s/it] 15%|â–ˆâ–Œ        | 705/4620 [2:39:47<20:34:59, 18.93s/it] 15%|â–ˆâ–Œ        | 706/4620 [2:40:00<18:30:56, 17.03s/it] 15%|â–ˆâ–Œ        | 707/4620 [2:40:12<17:03:05, 15.69s/it] 15%|â–ˆâ–Œ        | 708/4620 [2:40:26<16:29:07, 15.17s/it] 15%|â–ˆâ–Œ        | 709/4620 [2:40:38<15:24:49, 14.19s/it] 15%|â–ˆâ–Œ        | 710/4620 [2:40:52<15:14:20, 14.03s/it]                                                       {'loss': 0.0928, 'grad_norm': 0.416015625, 'learning_rate': 0.0004325221238938053, 'epoch': 0.77}
 15%|â–ˆâ–Œ        | 710/4620 [2:40:52<15:14:20, 14.03s/it] 15%|â–ˆâ–Œ        | 711/4620 [2:41:05<14:56:10, 13.76s/it] 15%|â–ˆâ–Œ        | 712/4620 [2:41:18<14:42:18, 13.55s/it] 15%|â–ˆâ–Œ        | 713/4620 [2:41:30<14:13:40, 13.11s/it] 15%|â–ˆâ–Œ        | 714/4620 [2:41:42<13:52:53, 12.79s/it] 15%|â–ˆâ–Œ        | 715/4620 [2:41:55<13:48:56, 12.74s/it] 15%|â–ˆâ–Œ        | 716/4620 [2:42:08<13:47:38, 12.72s/it] 16%|â–ˆâ–Œ        | 717/4620 [2:42:20<13:44:02, 12.67s/it] 16%|â–ˆâ–Œ        | 718/4620 [2:42:32<13:35:32, 12.54s/it] 16%|â–ˆâ–Œ        | 719/4620 [2:42:46<14:04:19, 12.99s/it] 16%|â–ˆâ–Œ        | 720/4620 [2:43:00<14:08:25, 13.05s/it]                                                       {'loss': 0.1054, 'grad_norm': 0.416015625, 'learning_rate': 0.00043141592920353984, 'epoch': 0.78}
 16%|â–ˆâ–Œ        | 720/4620 [2:43:00<14:08:25, 13.05s/it] 16%|â–ˆâ–Œ        | 721/4620 [2:43:12<14:00:22, 12.93s/it] 16%|â–ˆâ–Œ        | 722/4620 [2:43:26<14:06:04, 13.02s/it] 16%|â–ˆâ–Œ        | 723/4620 [2:43:37<13:44:19, 12.69s/it] 16%|â–ˆâ–Œ        | 724/4620 [2:43:50<13:35:46, 12.56s/it] 16%|â–ˆâ–Œ        | 725/4620 [2:44:02<13:28:52, 12.46s/it] 16%|â–ˆâ–Œ        | 726/4620 [2:44:15<13:32:48, 12.52s/it] 16%|â–ˆâ–Œ        | 727/4620 [2:44:28<13:43:19, 12.69s/it] 16%|â–ˆâ–Œ        | 728/4620 [2:44:40<13:41:12, 12.66s/it] 16%|â–ˆâ–Œ        | 729/4620 [2:44:54<14:04:24, 13.02s/it] 16%|â–ˆâ–Œ        | 730/4620 [2:45:09<14:46:24, 13.67s/it]                                                       {'loss': 0.0933, 'grad_norm': 0.66015625, 'learning_rate': 0.00043030973451327434, 'epoch': 0.79}
 16%|â–ˆâ–Œ        | 730/4620 [2:45:09<14:46:24, 13.67s/it] 16%|â–ˆâ–Œ        | 731/4620 [2:45:21<13:59:00, 12.94s/it] 16%|â–ˆâ–Œ        | 732/4620 [2:45:34<14:13:19, 13.17s/it] 16%|â–ˆâ–Œ        | 733/4620 [2:45:46<13:48:40, 12.79s/it] 16%|â–ˆâ–Œ        | 734/4620 [2:45:58<13:38:06, 12.63s/it] 16%|â–ˆâ–Œ        | 735/4620 [2:46:11<13:30:00, 12.51s/it] 16%|â–ˆâ–Œ        | 736/4620 [2:46:23<13:29:12, 12.50s/it] 16%|â–ˆâ–Œ        | 737/4620 [2:46:36<13:40:32, 12.68s/it] 16%|â–ˆâ–Œ        | 738/4620 [2:46:49<13:39:40, 12.67s/it] 16%|â–ˆâ–Œ        | 739/4620 [2:47:03<14:07:38, 13.10s/it] 16%|â–ˆâ–Œ        | 740/4620 [2:47:17<14:16:13, 13.24s/it]                                                       {'loss': 0.1034, 'grad_norm': 0.349609375, 'learning_rate': 0.00042920353982300885, 'epoch': 0.8}
 16%|â–ˆâ–Œ        | 740/4620 [2:47:17<14:16:13, 13.24s/it] 16%|â–ˆâ–Œ        | 741/4620 [2:47:28<13:48:44, 12.82s/it] 16%|â–ˆâ–Œ        | 742/4620 [2:47:42<13:58:50, 12.98s/it] 16%|â–ˆâ–Œ        | 743/4620 [2:47:54<13:38:39, 12.67s/it] 16%|â–ˆâ–Œ        | 744/4620 [2:48:06<13:27:11, 12.50s/it] 16%|â–ˆâ–Œ        | 745/4620 [2:48:18<13:30:30, 12.55s/it] 16%|â–ˆâ–Œ        | 746/4620 [2:48:31<13:29:03, 12.53s/it] 16%|â–ˆâ–Œ        | 747/4620 [2:48:43<13:25:22, 12.48s/it] 16%|â–ˆâ–Œ        | 748/4620 [2:48:56<13:26:21, 12.50s/it] 16%|â–ˆâ–Œ        | 749/4620 [2:49:10<13:57:27, 12.98s/it] 16%|â–ˆâ–Œ        | 750/4620 [2:49:24<14:12:50, 13.22s/it]                                                       {'loss': 0.0983, 'grad_norm': 0.353515625, 'learning_rate': 0.0004280973451327434, 'epoch': 0.81}
 16%|â–ˆâ–Œ        | 750/4620 [2:49:24<14:12:50, 13.22s/it] 16%|â–ˆâ–‹        | 751/4620 [2:49:36<13:46:13, 12.81s/it] 16%|â–ˆâ–‹        | 752/4620 [2:49:49<14:03:27, 13.08s/it] 16%|â–ˆâ–‹        | 753/4620 [2:50:01<13:33:56, 12.63s/it] 16%|â–ˆâ–‹        | 754/4620 [2:50:13<13:30:01, 12.57s/it] 16%|â–ˆâ–‹        | 755/4620 [2:50:26<13:32:21, 12.61s/it] 16%|â–ˆâ–‹        | 756/4620 [2:50:38<13:29:09, 12.56s/it] 16%|â–ˆâ–‹        | 757/4620 [2:50:51<13:28:41, 12.56s/it] 16%|â–ˆâ–‹        | 758/4620 [2:51:05<13:50:52, 12.91s/it] 16%|â–ˆâ–‹        | 759/4620 [2:51:18<13:58:19, 13.03s/it] 16%|â–ˆâ–‹        | 760/4620 [2:51:31<14:00:35, 13.07s/it]                                                       {'loss': 0.096, 'grad_norm': 0.375, 'learning_rate': 0.00042699115044247786, 'epoch': 0.82}
 16%|â–ˆâ–‹        | 760/4620 [2:51:31<14:00:35, 13.07s/it] 16%|â–ˆâ–‹        | 761/4620 [2:51:43<13:45:02, 12.83s/it] 16%|â–ˆâ–‹        | 762/4620 [2:51:57<14:03:34, 13.12s/it] 17%|â–ˆâ–‹        | 763/4620 [2:52:09<13:37:47, 12.72s/it] 17%|â–ˆâ–‹        | 764/4620 [2:52:21<13:27:00, 12.56s/it] 17%|â–ˆâ–‹        | 765/4620 [2:52:34<13:34:20, 12.67s/it] 17%|â–ˆâ–‹        | 766/4620 [2:52:47<13:32:29, 12.65s/it] 17%|â–ˆâ–‹        | 767/4620 [2:52:59<13:31:39, 12.64s/it] 17%|â–ˆâ–‹        | 768/4620 [2:53:13<13:51:17, 12.95s/it] 17%|â–ˆâ–‹        | 769/4620 [2:53:27<14:00:47, 13.10s/it] 17%|â–ˆâ–‹        | 770/4620 [2:53:39<13:56:17, 13.03s/it]                                                       {'loss': 0.1078, 'grad_norm': 0.49609375, 'learning_rate': 0.0004258849557522124, 'epoch': 0.83}
 17%|â–ˆâ–‹        | 770/4620 [2:53:39<13:56:17, 13.03s/it] 17%|â–ˆâ–‹        | 771/4620 [2:53:52<13:38:29, 12.76s/it] 17%|â–ˆâ–‹        | 772/4620 [2:54:05<13:51:39, 12.97s/it] 17%|â–ˆâ–‹        | 773/4620 [2:54:17<13:32:22, 12.67s/it] 17%|â–ˆâ–‹        | 774/4620 [2:54:29<13:22:52, 12.53s/it] 17%|â–ˆâ–‹        | 775/4620 [2:54:42<13:22:21, 12.52s/it] 17%|â–ˆâ–‹        | 776/4620 [2:54:54<13:23:49, 12.55s/it] 17%|â–ˆâ–‹        | 777/4620 [2:55:08<13:48:51, 12.94s/it] 17%|â–ˆâ–‹        | 778/4620 [2:55:22<13:59:25, 13.11s/it] 17%|â–ˆâ–‹        | 779/4620 [2:55:33<13:35:17, 12.74s/it] 17%|â–ˆâ–‹        | 780/4620 [2:55:47<13:52:26, 13.01s/it]                                                       {'loss': 0.109, 'grad_norm': 0.5390625, 'learning_rate': 0.0004247787610619469, 'epoch': 0.84}
 17%|â–ˆâ–‹        | 780/4620 [2:55:47<13:52:26, 13.01s/it] 17%|â–ˆâ–‹        | 781/4620 [2:55:59<13:37:25, 12.78s/it] 17%|â–ˆâ–‹        | 782/4620 [2:56:13<13:54:13, 13.04s/it] 17%|â–ˆâ–‹        | 783/4620 [2:56:25<13:33:47, 12.73s/it] 17%|â–ˆâ–‹        | 784/4620 [2:56:37<13:25:27, 12.60s/it] 17%|â–ˆâ–‹        | 785/4620 [2:56:49<13:16:32, 12.46s/it] 17%|â–ˆâ–‹        | 786/4620 [2:57:02<13:11:34, 12.39s/it] 17%|â–ˆâ–‹        | 787/4620 [2:57:16<13:43:39, 12.89s/it] 17%|â–ˆâ–‹        | 788/4620 [2:57:29<13:54:18, 13.06s/it] 17%|â–ˆâ–‹        | 789/4620 [2:57:41<13:27:19, 12.64s/it] 17%|â–ˆâ–‹        | 790/4620 [2:57:55<13:50:21, 13.01s/it]                                                       {'loss': 0.127, 'grad_norm': 0.7890625, 'learning_rate': 0.0004236725663716814, 'epoch': 0.85}
 17%|â–ˆâ–‹        | 790/4620 [2:57:55<13:50:21, 13.01s/it] 17%|â–ˆâ–‹        | 791/4620 [2:58:08<13:55:16, 13.09s/it] 17%|â–ˆâ–‹        | 792/4620 [2:58:20<13:29:22, 12.69s/it] 17%|â–ˆâ–‹        | 793/4620 [2:58:32<13:20:54, 12.56s/it] 17%|â–ˆâ–‹        | 794/4620 [2:58:45<13:23:45, 12.60s/it] 17%|â–ˆâ–‹        | 795/4620 [2:58:57<13:18:29, 12.53s/it] 17%|â–ˆâ–‹        | 796/4620 [2:59:11<13:42:01, 12.90s/it] 17%|â–ˆâ–‹        | 797/4620 [2:59:24<13:50:13, 13.03s/it] 17%|â–ˆâ–‹        | 798/4620 [2:59:36<13:33:12, 12.77s/it] 17%|â–ˆâ–‹        | 799/4620 [2:59:49<13:23:07, 12.61s/it] 17%|â–ˆâ–‹        | 800/4620 [3:00:02<13:45:27, 12.97s/it]                                                       {'loss': 0.1224, 'grad_norm': 0.51171875, 'learning_rate': 0.00042256637168141593, 'epoch': 0.87}
 17%|â–ˆâ–‹        | 800/4620 [3:00:02<13:45:27, 12.97s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:41,  3.00s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:32,  2.47s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:25,  2.09s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:20,  1.88s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:17,  1.79s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.75s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:14,  1.79s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:12,  1.74s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:10,  1.67s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.65s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.62s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.66s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.70s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:27<00:01,  1.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.93s/it][A                                                       
                                               [A{'eval_loss': 0.07791630923748016, 'eval_runtime': 86.0889, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 0.186, 'epoch': 0.87}
 17%|â–ˆâ–‹        | 800/4620 [3:01:28<13:45:27, 12.97s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.93s/it][A
                                               [A 17%|â–ˆâ–‹        | 801/4620 [3:01:40<40:38:25, 38.31s/it] 17%|â–ˆâ–‹        | 802/4620 [3:01:51<32:04:34, 30.24s/it] 17%|â–ˆâ–‹        | 803/4620 [3:02:03<26:18:53, 24.82s/it] 17%|â–ˆâ–‹        | 804/4620 [3:02:16<22:17:47, 21.03s/it] 17%|â–ˆâ–‹        | 805/4620 [3:02:28<19:34:18, 18.47s/it] 17%|â–ˆâ–‹        | 806/4620 [3:02:42<18:07:46, 17.11s/it] 17%|â–ˆâ–‹        | 807/4620 [3:02:56<17:12:18, 16.24s/it] 17%|â–ˆâ–‹        | 808/4620 [3:03:08<15:49:15, 14.94s/it] 18%|â–ˆâ–Š        | 809/4620 [3:03:22<15:20:02, 14.48s/it] 18%|â–ˆâ–Š        | 810/4620 [3:03:34<14:39:29, 13.85s/it]                                                       {'loss': 0.1078, 'grad_norm': 0.53515625, 'learning_rate': 0.0004214601769911505, 'epoch': 0.88}
 18%|â–ˆâ–Š        | 810/4620 [3:03:34<14:39:29, 13.85s/it] 18%|â–ˆâ–Š        | 811/4620 [3:03:48<14:42:36, 13.90s/it] 18%|â–ˆâ–Š        | 812/4620 [3:04:00<14:08:10, 13.36s/it] 18%|â–ˆâ–Š        | 813/4620 [3:04:12<13:43:59, 12.99s/it] 18%|â–ˆâ–Š        | 814/4620 [3:04:25<13:39:22, 12.92s/it] 18%|â–ˆâ–Š        | 815/4620 [3:04:38<13:44:02, 12.99s/it] 18%|â–ˆâ–Š        | 816/4620 [3:04:52<13:52:13, 13.13s/it] 18%|â–ˆâ–Š        | 817/4620 [3:05:05<14:06:36, 13.36s/it] 18%|â–ˆâ–Š        | 818/4620 [3:05:18<13:44:24, 13.01s/it] 18%|â–ˆâ–Š        | 819/4620 [3:05:31<13:50:52, 13.12s/it] 18%|â–ˆâ–Š        | 820/4620 [3:05:43<13:29:02, 12.77s/it]                                                       {'loss': 0.1149, 'grad_norm': 0.546875, 'learning_rate': 0.00042035398230088494, 'epoch': 0.89}
 18%|â–ˆâ–Š        | 820/4620 [3:05:43<13:29:02, 12.77s/it] 18%|â–ˆâ–Š        | 821/4620 [3:05:57<13:47:25, 13.07s/it] 18%|â–ˆâ–Š        | 822/4620 [3:06:09<13:28:47, 12.78s/it] 18%|â–ˆâ–Š        | 823/4620 [3:06:21<13:13:05, 12.53s/it] 18%|â–ˆâ–Š        | 824/4620 [3:06:33<13:16:15, 12.59s/it] 18%|â–ˆâ–Š        | 825/4620 [3:06:48<13:51:23, 13.14s/it] 18%|â–ˆâ–Š        | 826/4620 [3:07:00<13:30:04, 12.81s/it] 18%|â–ˆâ–Š        | 827/4620 [3:07:14<13:49:14, 13.12s/it] 18%|â–ˆâ–Š        | 828/4620 [3:07:27<13:57:28, 13.25s/it] 18%|â–ˆâ–Š        | 829/4620 [3:07:39<13:27:31, 12.78s/it] 18%|â–ˆâ–Š        | 830/4620 [3:07:51<13:12:31, 12.55s/it]                                                       {'loss': 0.1019, 'grad_norm': 0.44140625, 'learning_rate': 0.0004192477876106195, 'epoch': 0.9}
 18%|â–ˆâ–Š        | 830/4620 [3:07:51<13:12:31, 12.55s/it] 18%|â–ˆâ–Š        | 831/4620 [3:08:04<13:26:52, 12.78s/it] 18%|â–ˆâ–Š        | 832/4620 [3:08:18<13:42:03, 13.02s/it] 18%|â–ˆâ–Š        | 833/4620 [3:08:30<13:17:49, 12.64s/it] 18%|â–ˆâ–Š        | 834/4620 [3:08:42<13:04:33, 12.43s/it] 18%|â–ˆâ–Š        | 835/4620 [3:08:56<13:32:57, 12.89s/it] 18%|â–ˆâ–Š        | 836/4620 [3:09:08<13:23:01, 12.73s/it] 18%|â–ˆâ–Š        | 837/4620 [3:09:22<13:40:40, 13.02s/it] 18%|â–ˆâ–Š        | 838/4620 [3:09:36<13:59:19, 13.32s/it] 18%|â–ˆâ–Š        | 839/4620 [3:09:48<13:39:19, 13.00s/it] 18%|â–ˆâ–Š        | 840/4620 [3:10:00<13:27:21, 12.82s/it]                                                       {'loss': 0.0972, 'grad_norm': 0.478515625, 'learning_rate': 0.00041814159292035395, 'epoch': 0.91}
 18%|â–ˆâ–Š        | 840/4620 [3:10:00<13:27:21, 12.82s/it] 18%|â–ˆâ–Š        | 841/4620 [3:10:12<13:12:33, 12.58s/it] 18%|â–ˆâ–Š        | 842/4620 [3:10:27<13:45:37, 13.11s/it] 18%|â–ˆâ–Š        | 843/4620 [3:10:39<13:32:11, 12.90s/it] 18%|â–ˆâ–Š        | 844/4620 [3:10:53<13:49:32, 13.18s/it] 18%|â–ˆâ–Š        | 845/4620 [3:11:05<13:29:43, 12.87s/it] 18%|â–ˆâ–Š        | 846/4620 [3:11:17<13:20:19, 12.72s/it] 18%|â–ˆâ–Š        | 847/4620 [3:11:32<13:51:56, 13.23s/it] 18%|â–ˆâ–Š        | 848/4620 [3:11:44<13:30:22, 12.89s/it] 18%|â–ˆâ–Š        | 849/4620 [3:11:56<13:20:38, 12.74s/it] 18%|â–ˆâ–Š        | 850/4620 [3:12:09<13:10:49, 12.59s/it]                                                       {'loss': 0.1082, 'grad_norm': 0.37890625, 'learning_rate': 0.0004170353982300885, 'epoch': 0.92}
 18%|â–ˆâ–Š        | 850/4620 [3:12:09<13:10:49, 12.59s/it] 18%|â–ˆâ–Š        | 851/4620 [3:12:23<13:47:08, 13.17s/it] 18%|â–ˆâ–Š        | 852/4620 [3:12:35<13:26:14, 12.84s/it] 18%|â–ˆâ–Š        | 853/4620 [3:12:48<13:22:08, 12.78s/it] 18%|â–ˆâ–Š        | 854/4620 [3:13:00<13:13:48, 12.65s/it] 19%|â–ˆâ–Š        | 855/4620 [3:13:14<13:39:32, 13.06s/it] 19%|â–ˆâ–Š        | 856/4620 [3:13:27<13:25:47, 12.84s/it] 19%|â–ˆâ–Š        | 857/4620 [3:13:41<13:47:13, 13.19s/it] 19%|â–ˆâ–Š        | 858/4620 [3:13:52<13:17:32, 12.72s/it] 19%|â–ˆâ–Š        | 859/4620 [3:14:05<13:13:48, 12.66s/it] 19%|â–ˆâ–Š        | 860/4620 [3:14:18<13:17:37, 12.73s/it]                                                       {'loss': 0.1094, 'grad_norm': 0.25390625, 'learning_rate': 0.000415929203539823, 'epoch': 0.93}
 19%|â–ˆâ–Š        | 860/4620 [3:14:18<13:17:37, 12.73s/it] 19%|â–ˆâ–Š        | 861/4620 [3:14:31<13:31:15, 12.95s/it] 19%|â–ˆâ–Š        | 862/4620 [3:14:43<13:17:10, 12.73s/it] 19%|â–ˆâ–Š        | 863/4620 [3:14:56<13:13:50, 12.68s/it] 19%|â–ˆâ–Š        | 864/4620 [3:15:08<13:08:57, 12.60s/it] 19%|â–ˆâ–Š        | 865/4620 [3:15:23<13:49:26, 13.25s/it] 19%|â–ˆâ–Š        | 866/4620 [3:15:37<14:00:53, 13.44s/it] 19%|â–ˆâ–‰        | 867/4620 [3:15:50<13:56:21, 13.37s/it] 19%|â–ˆâ–‰        | 868/4620 [3:16:02<13:32:17, 12.99s/it] 19%|â–ˆâ–‰        | 869/4620 [3:16:15<13:19:35, 12.79s/it] 19%|â–ˆâ–‰        | 870/4620 [3:16:27<13:18:40, 12.78s/it]                                                       {'loss': 0.1113, 'grad_norm': 0.53125, 'learning_rate': 0.0004148230088495575, 'epoch': 0.94}
 19%|â–ˆâ–‰        | 870/4620 [3:16:27<13:18:40, 12.78s/it] 19%|â–ˆâ–‰        | 871/4620 [3:16:42<13:45:58, 13.22s/it] 19%|â–ˆâ–‰        | 872/4620 [3:16:54<13:36:58, 13.08s/it] 19%|â–ˆâ–‰        | 873/4620 [3:17:07<13:21:57, 12.84s/it] 19%|â–ˆâ–‰        | 874/4620 [3:17:19<13:18:50, 12.80s/it] 19%|â–ˆâ–‰        | 875/4620 [3:17:34<13:49:13, 13.29s/it] 19%|â–ˆâ–‰        | 876/4620 [3:17:46<13:24:42, 12.90s/it] 19%|â–ˆâ–‰        | 877/4620 [3:18:00<13:49:39, 13.30s/it] 19%|â–ˆâ–‰        | 878/4620 [3:18:12<13:32:46, 13.03s/it] 19%|â–ˆâ–‰        | 879/4620 [3:18:25<13:22:58, 12.88s/it] 19%|â–ˆâ–‰        | 880/4620 [3:18:38<13:20:45, 12.85s/it]                                                       {'loss': 0.0906, 'grad_norm': 0.4140625, 'learning_rate': 0.000413716814159292, 'epoch': 0.95}
 19%|â–ˆâ–‰        | 880/4620 [3:18:38<13:20:45, 12.85s/it] 19%|â–ˆâ–‰        | 881/4620 [3:18:51<13:38:35, 13.14s/it] 19%|â–ˆâ–‰        | 882/4620 [3:19:04<13:23:38, 12.90s/it] 19%|â–ˆâ–‰        | 883/4620 [3:19:16<13:11:10, 12.70s/it] 19%|â–ˆâ–‰        | 884/4620 [3:19:29<13:11:30, 12.71s/it] 19%|â–ˆâ–‰        | 885/4620 [3:19:43<13:44:33, 13.25s/it] 19%|â–ˆâ–‰        | 886/4620 [3:19:55<13:17:02, 12.81s/it] 19%|â–ˆâ–‰        | 887/4620 [3:20:09<13:35:35, 13.11s/it] 19%|â–ˆâ–‰        | 888/4620 [3:20:21<13:16:06, 12.80s/it] 19%|â–ˆâ–‰        | 889/4620 [3:20:33<13:05:20, 12.63s/it] 19%|â–ˆâ–‰        | 890/4620 [3:20:47<13:34:03, 13.09s/it]                                                       {'loss': 0.093, 'grad_norm': 0.3515625, 'learning_rate': 0.0004126106194690266, 'epoch': 0.96}
 19%|â–ˆâ–‰        | 890/4620 [3:20:47<13:34:03, 13.09s/it] 19%|â–ˆâ–‰        | 891/4620 [3:20:59<13:14:11, 12.78s/it] 19%|â–ˆâ–‰        | 892/4620 [3:21:11<12:59:09, 12.54s/it] 19%|â–ˆâ–‰        | 893/4620 [3:21:24<13:02:25, 12.60s/it] 19%|â–ˆâ–‰        | 894/4620 [3:21:39<13:37:58, 13.17s/it] 19%|â–ˆâ–‰        | 895/4620 [3:21:52<13:43:51, 13.27s/it] 19%|â–ˆâ–‰        | 896/4620 [3:22:04<13:24:05, 12.96s/it] 19%|â–ˆâ–‰        | 897/4620 [3:22:18<13:40:36, 13.22s/it] 19%|â–ˆâ–‰        | 898/4620 [3:22:30<13:21:38, 12.92s/it] 19%|â–ˆâ–‰        | 899/4620 [3:22:42<13:05:25, 12.66s/it] 19%|â–ˆâ–‰        | 900/4620 [3:22:57<13:38:11, 13.20s/it]                                                       {'loss': 0.1031, 'grad_norm': 0.349609375, 'learning_rate': 0.00041150442477876103, 'epoch': 0.97}
 19%|â–ˆâ–‰        | 900/4620 [3:22:57<13:38:11, 13.20s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:48,  3.48s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:36,  2.80s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.41s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.11s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.91s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.79s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:16<00:13,  1.72s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:11,  1.64s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:19<00:09,  1.65s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.70s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:24<00:05,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.65s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.93s/it][A                                                       
                                               [A{'eval_loss': 0.06943890452384949, 'eval_runtime': 87.2962, 'eval_samples_per_second': 5.728, 'eval_steps_per_second': 0.183, 'epoch': 0.97}
 19%|â–ˆâ–‰        | 900/4620 [3:24:24<13:38:11, 13.20s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.93s/it][A
                                               [A 20%|â–ˆâ–‰        | 901/4620 [3:24:35<39:54:34, 38.63s/it] 20%|â–ˆâ–‰        | 902/4620 [3:24:46<31:30:07, 30.50s/it] 20%|â–ˆâ–‰        | 903/4620 [3:24:59<25:52:37, 25.06s/it] 20%|â–ˆâ–‰        | 904/4620 [3:25:14<22:43:16, 22.01s/it] 20%|â–ˆâ–‰        | 905/4620 [3:25:27<20:07:33, 19.50s/it] 20%|â–ˆâ–‰        | 906/4620 [3:25:40<17:52:47, 17.33s/it] 20%|â–ˆâ–‰        | 907/4620 [3:25:54<17:03:25, 16.54s/it] 20%|â–ˆâ–‰        | 908/4620 [3:26:04<15:01:08, 14.57s/it] 20%|â–ˆâ–‰        | 909/4620 [3:26:13<13:13:48, 12.83s/it] 20%|â–ˆâ–‰        | 910/4620 [3:26:22<12:05:09, 11.73s/it]                                                       {'loss': 0.0953, 'grad_norm': 0.1962890625, 'learning_rate': 0.0004103982300884956, 'epoch': 0.98}
 20%|â–ˆâ–‰        | 910/4620 [3:26:22<12:05:09, 11.73s/it] 20%|â–ˆâ–‰        | 911/4620 [3:26:31<11:10:37, 10.85s/it] 20%|â–ˆâ–‰        | 912/4620 [3:26:40<10:28:51, 10.18s/it] 20%|â–ˆâ–‰        | 913/4620 [3:26:49<10:09:57,  9.87s/it] 20%|â–ˆâ–‰        | 914/4620 [3:26:57<9:47:27,  9.51s/it]  20%|â–ˆâ–‰        | 915/4620 [3:27:07<9:42:53,  9.44s/it] 20%|â–ˆâ–‰        | 916/4620 [3:27:15<9:29:51,  9.23s/it] 20%|â–ˆâ–‰        | 917/4620 [3:27:25<9:31:11,  9.26s/it] 20%|â–ˆâ–‰        | 918/4620 [3:27:33<9:22:15,  9.11s/it] 20%|â–ˆâ–‰        | 919/4620 [3:27:42<9:14:57,  9.00s/it] 20%|â–ˆâ–‰        | 920/4620 [3:27:51<9:16:09,  9.02s/it]                                                      {'loss': 0.1185, 'grad_norm': 0.53515625, 'learning_rate': 0.00040929203539823015, 'epoch': 1.0}
 20%|â–ˆâ–‰        | 920/4620 [3:27:51<9:16:09,  9.02s/it] 20%|â–ˆâ–‰        | 921/4620 [3:28:00<9:11:36,  8.95s/it] 20%|â–ˆâ–‰        | 922/4620 [3:28:09<9:14:33,  9.00s/it] 20%|â–ˆâ–‰        | 923/4620 [3:28:18<9:09:01,  8.91s/it] 20%|â–ˆâ–ˆ        | 924/4620 [3:28:28<9:30:20,  9.26s/it] 20%|â–ˆâ–ˆ        | 925/4620 [3:30:44<48:27:57, 47.22s/it] 20%|â–ˆâ–ˆ        | 926/4620 [3:31:11<42:16:05, 41.19s/it] 20%|â–ˆâ–ˆ        | 927/4620 [3:31:29<35:00:14, 34.12s/it] 20%|â–ˆâ–ˆ        | 928/4620 [3:31:46<29:47:20, 29.05s/it] 20%|â–ˆâ–ˆ        | 929/4620 [3:31:59<24:56:02, 24.32s/it] 20%|â–ˆâ–ˆ        | 930/4620 [3:32:14<22:07:02, 21.58s/it]                                                       {'loss': 0.0868, 'grad_norm': 0.29296875, 'learning_rate': 0.0004081858407079646, 'epoch': 1.01}
 20%|â–ˆâ–ˆ        | 930/4620 [3:32:14<22:07:02, 21.58s/it] 20%|â–ˆâ–ˆ        | 931/4620 [3:32:26<19:15:53, 18.80s/it] 20%|â–ˆâ–ˆ        | 932/4620 [3:32:40<17:42:27, 17.29s/it] 20%|â–ˆâ–ˆ        | 933/4620 [3:32:52<16:01:25, 15.65s/it] 20%|â–ˆâ–ˆ        | 934/4620 [3:33:04<14:58:20, 14.62s/it] 20%|â–ˆâ–ˆ        | 935/4620 [3:33:19<14:54:40, 14.57s/it] 20%|â–ˆâ–ˆ        | 936/4620 [3:33:31<14:10:17, 13.85s/it] 20%|â–ˆâ–ˆ        | 937/4620 [3:33:45<14:17:29, 13.97s/it] 20%|â–ˆâ–ˆ        | 938/4620 [3:33:57<13:47:01, 13.48s/it] 20%|â–ˆâ–ˆ        | 939/4620 [3:34:10<13:29:47, 13.20s/it] 20%|â–ˆâ–ˆ        | 940/4620 [3:34:24<13:46:02, 13.47s/it]                                                       {'loss': 0.0882, 'grad_norm': 0.2255859375, 'learning_rate': 0.00040707964601769916, 'epoch': 1.02}
 20%|â–ˆâ–ˆ        | 940/4620 [3:34:24<13:46:02, 13.47s/it] 20%|â–ˆâ–ˆ        | 941/4620 [3:34:36<13:15:41, 12.98s/it] 20%|â–ˆâ–ˆ        | 942/4620 [3:34:50<13:27:12, 13.17s/it] 20%|â–ˆâ–ˆ        | 943/4620 [3:35:01<12:58:46, 12.71s/it] 20%|â–ˆâ–ˆ        | 944/4620 [3:35:15<13:23:54, 13.12s/it] 20%|â–ˆâ–ˆ        | 945/4620 [3:35:28<13:12:20, 12.94s/it] 20%|â–ˆâ–ˆ        | 946/4620 [3:35:40<13:02:57, 12.79s/it] 20%|â–ˆâ–ˆ        | 947/4620 [3:35:55<13:29:45, 13.23s/it] 21%|â–ˆâ–ˆ        | 948/4620 [3:36:07<13:09:43, 12.90s/it] 21%|â–ˆâ–ˆ        | 949/4620 [3:36:19<12:57:03, 12.70s/it] 21%|â–ˆâ–ˆ        | 950/4620 [3:36:33<13:27:38, 13.20s/it]                                                       {'loss': 0.0925, 'grad_norm': 0.39453125, 'learning_rate': 0.0004059734513274336, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 950/4620 [3:36:33<13:27:38, 13.20s/it] 21%|â–ˆâ–ˆ        | 951/4620 [3:36:46<13:13:18, 12.97s/it] 21%|â–ˆâ–ˆ        | 952/4620 [3:36:59<13:24:44, 13.16s/it] 21%|â–ˆâ–ˆ        | 953/4620 [3:37:12<13:08:08, 12.90s/it] 21%|â–ˆâ–ˆ        | 954/4620 [3:37:26<13:32:52, 13.30s/it] 21%|â–ˆâ–ˆ        | 955/4620 [3:37:38<13:18:55, 13.08s/it] 21%|â–ˆâ–ˆ        | 956/4620 [3:37:51<13:03:58, 12.84s/it] 21%|â–ˆâ–ˆ        | 957/4620 [3:38:05<13:27:15, 13.22s/it] 21%|â–ˆâ–ˆ        | 958/4620 [3:38:17<13:06:56, 12.89s/it] 21%|â–ˆâ–ˆ        | 959/4620 [3:38:29<12:59:46, 12.78s/it] 21%|â–ˆâ–ˆ        | 960/4620 [3:38:43<13:20:10, 13.12s/it]                                                       {'loss': 0.0954, 'grad_norm': 0.345703125, 'learning_rate': 0.00040486725663716817, 'epoch': 1.04}
 21%|â–ˆâ–ˆ        | 960/4620 [3:38:43<13:20:10, 13.12s/it] 21%|â–ˆâ–ˆ        | 961/4620 [3:38:57<13:26:57, 13.23s/it] 21%|â–ˆâ–ˆ        | 962/4620 [3:39:09<13:05:38, 12.89s/it] 21%|â–ˆâ–ˆ        | 963/4620 [3:39:22<13:16:36, 13.07s/it] 21%|â–ˆâ–ˆ        | 964/4620 [3:39:35<13:01:20, 12.82s/it] 21%|â–ˆâ–ˆ        | 965/4620 [3:39:47<12:51:48, 12.67s/it] 21%|â–ˆâ–ˆ        | 966/4620 [3:40:00<12:50:33, 12.65s/it] 21%|â–ˆâ–ˆ        | 967/4620 [3:40:14<13:22:13, 13.18s/it] 21%|â–ˆâ–ˆ        | 968/4620 [3:40:26<13:04:55, 12.90s/it] 21%|â–ˆâ–ˆ        | 969/4620 [3:40:39<12:56:16, 12.76s/it] 21%|â–ˆâ–ˆ        | 970/4620 [3:40:53<13:18:10, 13.12s/it]                                                       {'loss': 0.0815, 'grad_norm': 0.42578125, 'learning_rate': 0.00040376106194690267, 'epoch': 1.05}
 21%|â–ˆâ–ˆ        | 970/4620 [3:40:53<13:18:10, 13.12s/it] 21%|â–ˆâ–ˆ        | 971/4620 [3:41:06<13:19:24, 13.14s/it] 21%|â–ˆâ–ˆ        | 972/4620 [3:41:19<13:23:35, 13.22s/it] 21%|â–ˆâ–ˆ        | 973/4620 [3:41:31<13:03:26, 12.89s/it] 21%|â–ˆâ–ˆ        | 974/4620 [3:41:44<12:53:56, 12.74s/it] 21%|â–ˆâ–ˆ        | 975/4620 [3:41:57<12:57:51, 12.80s/it] 21%|â–ˆâ–ˆ        | 976/4620 [3:42:09<12:56:41, 12.79s/it] 21%|â–ˆâ–ˆ        | 977/4620 [3:42:23<13:16:03, 13.11s/it] 21%|â–ˆâ–ˆ        | 978/4620 [3:42:35<12:57:36, 12.81s/it] 21%|â–ˆâ–ˆ        | 979/4620 [3:42:48<12:48:34, 12.67s/it] 21%|â–ˆâ–ˆ        | 980/4620 [3:43:02<13:12:42, 13.07s/it]                                                       {'loss': 0.0842, 'grad_norm': 0.234375, 'learning_rate': 0.0004026548672566372, 'epoch': 1.06}
 21%|â–ˆâ–ˆ        | 980/4620 [3:43:02<13:12:42, 13.07s/it] 21%|â–ˆâ–ˆ        | 981/4620 [3:43:14<12:57:08, 12.81s/it] 21%|â–ˆâ–ˆâ–       | 982/4620 [3:43:26<12:49:01, 12.68s/it] 21%|â–ˆâ–ˆâ–       | 983/4620 [3:43:41<13:17:57, 13.16s/it] 21%|â–ˆâ–ˆâ–       | 984/4620 [3:43:53<12:59:44, 12.87s/it] 21%|â–ˆâ–ˆâ–       | 985/4620 [3:44:05<12:48:31, 12.69s/it] 21%|â–ˆâ–ˆâ–       | 986/4620 [3:44:18<12:52:55, 12.76s/it] 21%|â–ˆâ–ˆâ–       | 987/4620 [3:44:32<13:13:25, 13.10s/it] 21%|â–ˆâ–ˆâ–       | 988/4620 [3:44:44<12:53:44, 12.78s/it] 21%|â–ˆâ–ˆâ–       | 989/4620 [3:44:56<12:45:23, 12.65s/it] 21%|â–ˆâ–ˆâ–       | 990/4620 [3:45:10<13:05:17, 12.98s/it]                                                       {'loss': 0.1039, 'grad_norm': 0.5859375, 'learning_rate': 0.0004015486725663717, 'epoch': 1.07}
 21%|â–ˆâ–ˆâ–       | 990/4620 [3:45:10<13:05:17, 12.98s/it] 21%|â–ˆâ–ˆâ–       | 991/4620 [3:45:22<12:46:18, 12.67s/it] 21%|â–ˆâ–ˆâ–       | 992/4620 [3:45:35<12:44:32, 12.64s/it] 21%|â–ˆâ–ˆâ–       | 993/4620 [3:45:48<13:02:04, 12.94s/it] 22%|â–ˆâ–ˆâ–       | 994/4620 [3:46:00<12:49:04, 12.73s/it] 22%|â–ˆâ–ˆâ–       | 995/4620 [3:46:13<12:46:41, 12.69s/it] 22%|â–ˆâ–ˆâ–       | 996/4620 [3:46:26<12:47:59, 12.72s/it] 22%|â–ˆâ–ˆâ–       | 997/4620 [3:46:40<13:21:16, 13.27s/it] 22%|â–ˆâ–ˆâ–       | 998/4620 [3:46:52<12:54:45, 12.83s/it] 22%|â–ˆâ–ˆâ–       | 999/4620 [3:47:06<13:13:16, 13.14s/it] 22%|â–ˆâ–ˆâ–       | 1000/4620 [3:47:19<13:14:34, 13.17s/it]                                                        {'loss': 0.0889, 'grad_norm': 0.337890625, 'learning_rate': 0.00040044247787610624, 'epoch': 1.08}
 22%|â–ˆâ–ˆâ–       | 1000/4620 [3:47:19<13:14:34, 13.17s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:35,  2.57s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:06<00:28,  2.22s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:24,  2.08s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:20,  1.89s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:18,  1.81s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.70s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:16<00:11,  1.71s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:10,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.67s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.64s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.60s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:24<00:03,  1.59s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06818743795156479, 'eval_runtime': 66.4341, 'eval_samples_per_second': 7.526, 'eval_steps_per_second': 0.241, 'epoch': 1.08}
 22%|â–ˆâ–ˆâ–       | 1000/4620 [3:48:26<13:14:34, 13.17s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.86s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 1001/4620 [3:48:39<33:14:09, 33.06s/it] 22%|â–ˆâ–ˆâ–       | 1002/4620 [3:48:50<26:32:21, 26.41s/it] 22%|â–ˆâ–ˆâ–       | 1003/4620 [3:49:03<22:32:14, 22.43s/it] 22%|â–ˆâ–ˆâ–       | 1004/4620 [3:49:15<19:27:25, 19.37s/it] 22%|â–ˆâ–ˆâ–       | 1005/4620 [3:49:27<17:18:44, 17.24s/it] 22%|â–ˆâ–ˆâ–       | 1006/4620 [3:49:41<16:19:15, 16.26s/it] 22%|â–ˆâ–ˆâ–       | 1007/4620 [3:49:54<15:09:16, 15.10s/it] 22%|â–ˆâ–ˆâ–       | 1008/4620 [3:50:06<14:21:14, 14.31s/it] 22%|â–ˆâ–ˆâ–       | 1009/4620 [3:50:20<14:15:43, 14.22s/it] 22%|â–ˆâ–ˆâ–       | 1010/4620 [3:50:34<14:03:40, 14.02s/it]                                                        {'loss': 0.0815, 'grad_norm': 0.2392578125, 'learning_rate': 0.0003993362831858407, 'epoch': 1.09}
 22%|â–ˆâ–ˆâ–       | 1010/4620 [3:50:34<14:03:40, 14.02s/it] 22%|â–ˆâ–ˆâ–       | 1011/4620 [3:50:46<13:24:23, 13.37s/it] 22%|â–ˆâ–ˆâ–       | 1012/4620 [3:50:58<13:07:59, 13.10s/it] 22%|â–ˆâ–ˆâ–       | 1013/4620 [3:51:12<13:15:18, 13.23s/it] 22%|â–ˆâ–ˆâ–       | 1014/4620 [3:51:24<12:53:51, 12.88s/it] 22%|â–ˆâ–ˆâ–       | 1015/4620 [3:51:38<13:13:00, 13.20s/it] 22%|â–ˆâ–ˆâ–       | 1016/4620 [3:51:50<12:53:05, 12.87s/it] 22%|â–ˆâ–ˆâ–       | 1017/4620 [3:52:02<12:39:35, 12.65s/it] 22%|â–ˆâ–ˆâ–       | 1018/4620 [3:52:14<12:38:51, 12.64s/it] 22%|â–ˆâ–ˆâ–       | 1019/4620 [3:52:29<13:07:26, 13.12s/it] 22%|â–ˆâ–ˆâ–       | 1020/4620 [3:52:41<12:54:54, 12.92s/it]                                                        {'loss': 0.0715, 'grad_norm': 0.3984375, 'learning_rate': 0.00039823008849557525, 'epoch': 1.1}
 22%|â–ˆâ–ˆâ–       | 1020/4620 [3:52:41<12:54:54, 12.92s/it] 22%|â–ˆâ–ˆâ–       | 1021/4620 [3:52:53<12:42:49, 12.72s/it] 22%|â–ˆâ–ˆâ–       | 1022/4620 [3:53:06<12:40:27, 12.68s/it] 22%|â–ˆâ–ˆâ–       | 1023/4620 [3:53:20<13:03:15, 13.07s/it] 22%|â–ˆâ–ˆâ–       | 1024/4620 [3:53:32<12:47:29, 12.81s/it] 22%|â–ˆâ–ˆâ–       | 1025/4620 [3:53:44<12:39:35, 12.68s/it] 22%|â–ˆâ–ˆâ–       | 1026/4620 [3:53:59<13:07:36, 13.15s/it] 22%|â–ˆâ–ˆâ–       | 1027/4620 [3:54:11<12:43:32, 12.75s/it] 22%|â–ˆâ–ˆâ–       | 1028/4620 [3:54:23<12:35:42, 12.62s/it] 22%|â–ˆâ–ˆâ–       | 1029/4620 [3:54:38<13:25:05, 13.45s/it] 22%|â–ˆâ–ˆâ–       | 1030/4620 [3:54:50<12:48:09, 12.84s/it]                                                        {'loss': 0.0827, 'grad_norm': 0.439453125, 'learning_rate': 0.0003971238938053097, 'epoch': 1.11}
 22%|â–ˆâ–ˆâ–       | 1030/4620 [3:54:50<12:48:09, 12.84s/it] 22%|â–ˆâ–ˆâ–       | 1031/4620 [3:55:02<12:33:44, 12.60s/it] 22%|â–ˆâ–ˆâ–       | 1032/4620 [3:55:15<12:39:40, 12.70s/it] 22%|â–ˆâ–ˆâ–       | 1033/4620 [3:55:29<13:06:17, 13.15s/it] 22%|â–ˆâ–ˆâ–       | 1034/4620 [3:55:41<12:44:01, 12.78s/it] 22%|â–ˆâ–ˆâ–       | 1035/4620 [3:55:53<12:32:09, 12.59s/it] 22%|â–ˆâ–ˆâ–       | 1036/4620 [3:56:07<12:55:47, 12.99s/it] 22%|â–ˆâ–ˆâ–       | 1037/4620 [3:56:19<12:41:31, 12.75s/it] 22%|â–ˆâ–ˆâ–       | 1038/4620 [3:56:33<12:56:18, 13.00s/it] 22%|â–ˆâ–ˆâ–       | 1039/4620 [3:56:46<13:00:26, 13.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 1040/4620 [3:56:58<12:44:00, 12.80s/it]                                                        {'loss': 0.0893, 'grad_norm': 0.65234375, 'learning_rate': 0.00039601769911504426, 'epoch': 1.13}
 23%|â–ˆâ–ˆâ–Ž       | 1040/4620 [3:56:58<12:44:00, 12.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1041/4620 [3:57:10<12:32:47, 12.62s/it] 23%|â–ˆâ–ˆâ–Ž       | 1042/4620 [3:57:22<12:26:21, 12.52s/it] 23%|â–ˆâ–ˆâ–Ž       | 1043/4620 [3:57:36<12:47:23, 12.87s/it] 23%|â–ˆâ–ˆâ–Ž       | 1044/4620 [3:57:48<12:32:25, 12.62s/it] 23%|â–ˆâ–ˆâ–Ž       | 1045/4620 [3:58:00<12:23:58, 12.49s/it] 23%|â–ˆâ–ˆâ–Ž       | 1046/4620 [3:58:15<12:52:55, 12.98s/it] 23%|â–ˆâ–ˆâ–Ž       | 1047/4620 [3:58:27<12:43:20, 12.82s/it] 23%|â–ˆâ–ˆâ–Ž       | 1048/4620 [3:58:40<12:55:50, 13.03s/it] 23%|â–ˆâ–ˆâ–Ž       | 1049/4620 [3:58:54<13:01:10, 13.13s/it] 23%|â–ˆâ–ˆâ–Ž       | 1050/4620 [3:59:06<12:37:02, 12.72s/it]                                                        {'loss': 0.0983, 'grad_norm': 0.435546875, 'learning_rate': 0.00039491150442477876, 'epoch': 1.14}
 23%|â–ˆâ–ˆâ–Ž       | 1050/4620 [3:59:06<12:37:02, 12.72s/it] 23%|â–ˆâ–ˆâ–Ž       | 1051/4620 [3:59:18<12:26:22, 12.55s/it] 23%|â–ˆâ–ˆâ–Ž       | 1052/4620 [3:59:30<12:27:18, 12.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 1053/4620 [3:59:45<13:00:24, 13.13s/it] 23%|â–ˆâ–ˆâ–Ž       | 1054/4620 [3:59:57<12:45:52, 12.89s/it] 23%|â–ˆâ–ˆâ–Ž       | 1055/4620 [4:00:10<12:40:48, 12.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1056/4620 [4:00:24<13:01:32, 13.16s/it] 23%|â–ˆâ–ˆâ–Ž       | 1057/4620 [4:00:37<13:01:00, 13.15s/it] 23%|â–ˆâ–ˆâ–Ž       | 1058/4620 [4:00:49<12:36:39, 12.75s/it] 23%|â–ˆâ–ˆâ–Ž       | 1059/4620 [4:01:02<12:48:41, 12.95s/it] 23%|â–ˆâ–ˆâ–Ž       | 1060/4620 [4:01:14<12:38:47, 12.79s/it]                                                        {'loss': 0.0799, 'grad_norm': 0.275390625, 'learning_rate': 0.00039380530973451327, 'epoch': 1.15}
 23%|â–ˆâ–ˆâ–Ž       | 1060/4620 [4:01:15<12:38:47, 12.79s/it] 23%|â–ˆâ–ˆâ–Ž       | 1061/4620 [4:01:27<12:34:03, 12.71s/it] 23%|â–ˆâ–ˆâ–Ž       | 1062/4620 [4:01:41<13:00:01, 13.15s/it] 23%|â–ˆâ–ˆâ–Ž       | 1063/4620 [4:01:54<12:47:24, 12.94s/it] 23%|â–ˆâ–ˆâ–Ž       | 1064/4620 [4:02:07<12:45:28, 12.92s/it] 23%|â–ˆâ–ˆâ–Ž       | 1065/4620 [4:02:19<12:34:55, 12.74s/it] 23%|â–ˆâ–ˆâ–Ž       | 1066/4620 [4:02:33<12:54:57, 13.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 1067/4620 [4:02:47<13:09:32, 13.33s/it] 23%|â–ˆâ–ˆâ–Ž       | 1068/4620 [4:02:59<12:47:32, 12.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 1069/4620 [4:03:13<13:02:14, 13.22s/it] 23%|â–ˆâ–ˆâ–Ž       | 1070/4620 [4:03:25<12:45:18, 12.93s/it]                                                        {'loss': 0.0853, 'grad_norm': 0.318359375, 'learning_rate': 0.0003926991150442478, 'epoch': 1.16}
 23%|â–ˆâ–ˆâ–Ž       | 1070/4620 [4:03:25<12:45:18, 12.93s/it] 23%|â–ˆâ–ˆâ–Ž       | 1071/4620 [4:03:37<12:35:28, 12.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 1072/4620 [4:03:51<12:49:54, 13.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1073/4620 [4:04:03<12:29:28, 12.68s/it] 23%|â–ˆâ–ˆâ–Ž       | 1074/4620 [4:04:15<12:22:41, 12.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 1075/4620 [4:04:28<12:26:11, 12.63s/it] 23%|â–ˆâ–ˆâ–Ž       | 1076/4620 [4:04:40<12:24:39, 12.61s/it] 23%|â–ˆâ–ˆâ–Ž       | 1077/4620 [4:04:55<12:57:31, 13.17s/it] 23%|â–ˆâ–ˆâ–Ž       | 1078/4620 [4:05:08<12:59:17, 13.20s/it] 23%|â–ˆâ–ˆâ–Ž       | 1079/4620 [4:05:20<12:36:32, 12.82s/it] 23%|â–ˆâ–ˆâ–Ž       | 1080/4620 [4:05:32<12:28:49, 12.69s/it]                                                        {'loss': 0.0858, 'grad_norm': 0.232421875, 'learning_rate': 0.00039159292035398233, 'epoch': 1.17}
 23%|â–ˆâ–ˆâ–Ž       | 1080/4620 [4:05:32<12:28:49, 12.69s/it] 23%|â–ˆâ–ˆâ–Ž       | 1081/4620 [4:05:45<12:29:59, 12.72s/it] 23%|â–ˆâ–ˆâ–Ž       | 1082/4620 [4:05:59<12:51:31, 13.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 1083/4620 [4:06:11<12:31:26, 12.75s/it] 23%|â–ˆâ–ˆâ–Ž       | 1084/4620 [4:06:24<12:29:07, 12.71s/it] 23%|â–ˆâ–ˆâ–Ž       | 1085/4620 [4:06:36<12:28:23, 12.70s/it] 24%|â–ˆâ–ˆâ–Ž       | 1086/4620 [4:06:49<12:26:24, 12.67s/it] 24%|â–ˆâ–ˆâ–Ž       | 1087/4620 [4:07:04<13:10:23, 13.42s/it] 24%|â–ˆâ–ˆâ–Ž       | 1088/4620 [4:07:17<13:05:35, 13.35s/it] 24%|â–ˆâ–ˆâ–Ž       | 1089/4620 [4:07:29<12:32:11, 12.78s/it] 24%|â–ˆâ–ˆâ–Ž       | 1090/4620 [4:07:41<12:20:59, 12.59s/it]                                                        {'loss': 0.0892, 'grad_norm': 0.365234375, 'learning_rate': 0.00039048672566371683, 'epoch': 1.18}
 24%|â–ˆâ–ˆâ–Ž       | 1090/4620 [4:07:41<12:20:59, 12.59s/it] 24%|â–ˆâ–ˆâ–Ž       | 1091/4620 [4:07:53<12:13:27, 12.47s/it] 24%|â–ˆâ–ˆâ–Ž       | 1092/4620 [4:08:08<12:46:56, 13.04s/it] 24%|â–ˆâ–ˆâ–Ž       | 1093/4620 [4:08:20<12:32:35, 12.80s/it] 24%|â–ˆâ–ˆâ–Ž       | 1094/4620 [4:08:32<12:21:59, 12.63s/it] 24%|â–ˆâ–ˆâ–Ž       | 1095/4620 [4:08:45<12:26:46, 12.71s/it] 24%|â–ˆâ–ˆâ–Ž       | 1096/4620 [4:08:57<12:24:28, 12.68s/it] 24%|â–ˆâ–ˆâ–Ž       | 1097/4620 [4:09:14<13:30:48, 13.81s/it] 24%|â–ˆâ–ˆâ–       | 1098/4620 [4:09:27<13:18:43, 13.61s/it] 24%|â–ˆâ–ˆâ–       | 1099/4620 [4:09:39<12:42:06, 12.99s/it] 24%|â–ˆâ–ˆâ–       | 1100/4620 [4:09:51<12:27:15, 12.74s/it]                                                        {'loss': 0.0863, 'grad_norm': 0.56640625, 'learning_rate': 0.00038938053097345134, 'epoch': 1.19}
 24%|â–ˆâ–ˆâ–       | 1100/4620 [4:09:51<12:27:15, 12.74s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:08<00:59,  4.27s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:10<00:42,  3.26s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:31,  2.62s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:24,  2.26s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:15<00:20,  2.04s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:17,  1.93s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:14,  1.86s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:20<00:12,  1.82s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:23<00:08,  1.70s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:28<00:03,  1.67s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.88s/it][A                                                        
                                               [A{'eval_loss': 0.06565675139427185, 'eval_runtime': 67.2632, 'eval_samples_per_second': 7.433, 'eval_steps_per_second': 0.238, 'epoch': 1.19}
 24%|â–ˆâ–ˆâ–       | 1100/4620 [4:10:58<12:27:15, 12.74s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.88s/it][A
                                               [A 24%|â–ˆâ–ˆâ–       | 1101/4620 [4:11:09<31:47:57, 32.53s/it] 24%|â–ˆâ–ˆâ–       | 1102/4620 [4:11:21<25:37:18, 26.22s/it] 24%|â–ˆâ–ˆâ–       | 1103/4620 [4:11:33<21:32:58, 22.06s/it] 24%|â–ˆâ–ˆâ–       | 1104/4620 [4:11:46<18:45:36, 19.21s/it] 24%|â–ˆâ–ˆâ–       | 1105/4620 [4:11:59<16:51:12, 17.26s/it] 24%|â–ˆâ–ˆâ–       | 1106/4620 [4:12:11<15:20:41, 15.72s/it] 24%|â–ˆâ–ˆâ–       | 1107/4620 [4:12:27<15:29:15, 15.87s/it] 24%|â–ˆâ–ˆâ–       | 1108/4620 [4:12:40<14:41:42, 15.06s/it] 24%|â–ˆâ–ˆâ–       | 1109/4620 [4:12:51<13:34:55, 13.93s/it] 24%|â–ˆâ–ˆâ–       | 1110/4620 [4:13:04<13:06:42, 13.45s/it]                                                        {'loss': 0.066, 'grad_norm': 0.498046875, 'learning_rate': 0.00038827433628318584, 'epoch': 1.2}
 24%|â–ˆâ–ˆâ–       | 1110/4620 [4:13:04<13:06:42, 13.45s/it] 24%|â–ˆâ–ˆâ–       | 1111/4620 [4:13:17<13:10:41, 13.52s/it] 24%|â–ˆâ–ˆâ–       | 1112/4620 [4:13:29<12:44:29, 13.08s/it] 24%|â–ˆâ–ˆâ–       | 1113/4620 [4:13:42<12:26:28, 12.77s/it] 24%|â–ˆâ–ˆâ–       | 1114/4620 [4:13:54<12:24:33, 12.74s/it] 24%|â–ˆâ–ˆâ–       | 1115/4620 [4:14:07<12:21:27, 12.69s/it] 24%|â–ˆâ–ˆâ–       | 1116/4620 [4:14:21<12:42:15, 13.05s/it] 24%|â–ˆâ–ˆâ–       | 1117/4620 [4:14:34<12:51:06, 13.21s/it] 24%|â–ˆâ–ˆâ–       | 1118/4620 [4:14:47<12:49:58, 13.19s/it] 24%|â–ˆâ–ˆâ–       | 1119/4620 [4:15:00<12:31:59, 12.89s/it] 24%|â–ˆâ–ˆâ–       | 1120/4620 [4:15:13<12:33:54, 12.92s/it]                                                        {'loss': 0.091, 'grad_norm': 0.625, 'learning_rate': 0.00038716814159292035, 'epoch': 1.21}
 24%|â–ˆâ–ˆâ–       | 1120/4620 [4:15:13<12:33:54, 12.92s/it] 24%|â–ˆâ–ˆâ–       | 1121/4620 [4:15:25<12:22:56, 12.74s/it] 24%|â–ˆâ–ˆâ–       | 1122/4620 [4:15:37<12:18:20, 12.66s/it] 24%|â–ˆâ–ˆâ–       | 1123/4620 [4:15:50<12:13:23, 12.58s/it] 24%|â–ˆâ–ˆâ–       | 1124/4620 [4:16:02<12:07:43, 12.49s/it] 24%|â–ˆâ–ˆâ–       | 1125/4620 [4:16:15<12:15:31, 12.63s/it] 24%|â–ˆâ–ˆâ–       | 1126/4620 [4:16:28<12:16:47, 12.65s/it] 24%|â–ˆâ–ˆâ–       | 1127/4620 [4:16:43<13:02:29, 13.44s/it] 24%|â–ˆâ–ˆâ–       | 1128/4620 [4:16:54<12:27:52, 12.85s/it] 24%|â–ˆâ–ˆâ–       | 1129/4620 [4:17:07<12:20:32, 12.73s/it] 24%|â–ˆâ–ˆâ–       | 1130/4620 [4:17:21<12:36:17, 13.00s/it]                                                        {'loss': 0.0821, 'grad_norm': 0.34765625, 'learning_rate': 0.0003860619469026549, 'epoch': 1.22}
 24%|â–ˆâ–ˆâ–       | 1130/4620 [4:17:21<12:36:17, 13.00s/it] 24%|â–ˆâ–ˆâ–       | 1131/4620 [4:17:33<12:22:37, 12.77s/it] 25%|â–ˆâ–ˆâ–       | 1132/4620 [4:17:45<12:18:04, 12.70s/it] 25%|â–ˆâ–ˆâ–       | 1133/4620 [4:17:58<12:19:57, 12.73s/it] 25%|â–ˆâ–ˆâ–       | 1134/4620 [4:18:11<12:17:30, 12.69s/it] 25%|â–ˆâ–ˆâ–       | 1135/4620 [4:18:23<12:13:07, 12.62s/it] 25%|â–ˆâ–ˆâ–       | 1136/4620 [4:18:37<12:30:24, 12.92s/it] 25%|â–ˆâ–ˆâ–       | 1137/4620 [4:18:50<12:35:45, 13.02s/it] 25%|â–ˆâ–ˆâ–       | 1138/4620 [4:19:02<12:16:23, 12.69s/it] 25%|â–ˆâ–ˆâ–       | 1139/4620 [4:19:16<12:33:31, 12.99s/it] 25%|â–ˆâ–ˆâ–       | 1140/4620 [4:19:28<12:18:15, 12.73s/it]                                                        {'loss': 0.0956, 'grad_norm': 0.38671875, 'learning_rate': 0.00038495575221238936, 'epoch': 1.23}
 25%|â–ˆâ–ˆâ–       | 1140/4620 [4:19:28<12:18:15, 12.73s/it] 25%|â–ˆâ–ˆâ–       | 1141/4620 [4:19:40<12:07:19, 12.54s/it] 25%|â–ˆâ–ˆâ–       | 1142/4620 [4:19:52<11:59:26, 12.41s/it] 25%|â–ˆâ–ˆâ–       | 1143/4620 [4:20:05<12:12:59, 12.65s/it] 25%|â–ˆâ–ˆâ–       | 1144/4620 [4:20:16<11:37:17, 12.04s/it] 25%|â–ˆâ–ˆâ–       | 1145/4620 [4:20:25<10:41:00, 11.07s/it] 25%|â–ˆâ–ˆâ–       | 1146/4620 [4:24:19<75:14:38, 77.97s/it] 25%|â–ˆâ–ˆâ–       | 1147/4620 [4:24:35<57:27:13, 59.55s/it] 25%|â–ˆâ–ˆâ–       | 1148/4620 [4:24:59<46:58:14, 48.70s/it] 25%|â–ˆâ–ˆâ–       | 1149/4620 [4:25:19<38:43:36, 40.17s/it] 25%|â–ˆâ–ˆâ–       | 1150/4620 [4:25:31<30:38:26, 31.79s/it]                                                        {'loss': 0.0864, 'grad_norm': 0.359375, 'learning_rate': 0.0003838495575221239, 'epoch': 1.24}
 25%|â–ˆâ–ˆâ–       | 1150/4620 [4:25:31<30:38:26, 31.79s/it] 25%|â–ˆâ–ˆâ–       | 1151/4620 [4:25:44<25:03:55, 26.01s/it] 25%|â–ˆâ–ˆâ–       | 1152/4620 [4:25:56<21:09:51, 21.97s/it] 25%|â–ˆâ–ˆâ–       | 1153/4620 [4:26:08<18:21:35, 19.06s/it] 25%|â–ˆâ–ˆâ–       | 1154/4620 [4:26:21<16:27:40, 17.10s/it] 25%|â–ˆâ–ˆâ–Œ       | 1155/4620 [4:26:35<15:40:47, 16.29s/it] 25%|â–ˆâ–ˆâ–Œ       | 1156/4620 [4:26:47<14:21:36, 14.92s/it] 25%|â–ˆâ–ˆâ–Œ       | 1157/4620 [4:26:59<13:31:12, 14.05s/it] 25%|â–ˆâ–ˆâ–Œ       | 1158/4620 [4:27:15<14:01:14, 14.58s/it] 25%|â–ˆâ–ˆâ–Œ       | 1159/4620 [4:27:26<13:07:32, 13.65s/it] 25%|â–ˆâ–ˆâ–Œ       | 1160/4620 [4:27:39<12:41:43, 13.21s/it]                                                        {'loss': 0.0758, 'grad_norm': 0.25390625, 'learning_rate': 0.0003827433628318584, 'epoch': 1.26}
 25%|â–ˆâ–ˆâ–Œ       | 1160/4620 [4:27:39<12:41:43, 13.21s/it] 25%|â–ˆâ–ˆâ–Œ       | 1161/4620 [4:27:51<12:25:53, 12.94s/it] 25%|â–ˆâ–ˆâ–Œ       | 1162/4620 [4:28:03<12:12:32, 12.71s/it] 25%|â–ˆâ–ˆâ–Œ       | 1163/4620 [4:28:16<12:13:47, 12.74s/it] 25%|â–ˆâ–ˆâ–Œ       | 1164/4620 [4:28:29<12:11:07, 12.69s/it] 25%|â–ˆâ–ˆâ–Œ       | 1165/4620 [4:28:43<12:43:23, 13.26s/it] 25%|â–ˆâ–ˆâ–Œ       | 1166/4620 [4:28:55<12:24:25, 12.93s/it] 25%|â–ˆâ–ˆâ–Œ       | 1167/4620 [4:29:07<12:07:42, 12.64s/it] 25%|â–ˆâ–ˆâ–Œ       | 1168/4620 [4:29:21<12:35:17, 13.13s/it] 25%|â–ˆâ–ˆâ–Œ       | 1169/4620 [4:29:33<12:15:11, 12.78s/it] 25%|â–ˆâ–ˆâ–Œ       | 1170/4620 [4:29:46<12:06:07, 12.63s/it]                                                        {'loss': 0.0779, 'grad_norm': 0.455078125, 'learning_rate': 0.0003816371681415929, 'epoch': 1.27}
 25%|â–ˆâ–ˆâ–Œ       | 1170/4620 [4:29:46<12:06:07, 12.63s/it] 25%|â–ˆâ–ˆâ–Œ       | 1171/4620 [4:29:59<12:09:19, 12.69s/it] 25%|â–ˆâ–ˆâ–Œ       | 1172/4620 [4:30:11<11:57:38, 12.49s/it] 25%|â–ˆâ–ˆâ–Œ       | 1173/4620 [4:30:23<12:03:08, 12.59s/it] 25%|â–ˆâ–ˆâ–Œ       | 1174/4620 [4:30:38<12:36:58, 13.18s/it] 25%|â–ˆâ–ˆâ–Œ       | 1175/4620 [4:30:50<12:17:16, 12.84s/it] 25%|â–ˆâ–ˆâ–Œ       | 1176/4620 [4:31:02<12:07:10, 12.67s/it] 25%|â–ˆâ–ˆâ–Œ       | 1177/4620 [4:31:17<12:39:08, 13.23s/it] 25%|â–ˆâ–ˆâ–Œ       | 1178/4620 [4:31:30<12:41:36, 13.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 1179/4620 [4:31:42<12:12:54, 12.78s/it] 26%|â–ˆâ–ˆâ–Œ       | 1180/4620 [4:31:54<12:04:58, 12.64s/it]                                                        {'loss': 0.077, 'grad_norm': 0.2060546875, 'learning_rate': 0.00038053097345132743, 'epoch': 1.28}
 26%|â–ˆâ–ˆâ–Œ       | 1180/4620 [4:31:54<12:04:58, 12.64s/it] 26%|â–ˆâ–ˆâ–Œ       | 1181/4620 [4:32:07<12:08:14, 12.71s/it] 26%|â–ˆâ–ˆâ–Œ       | 1182/4620 [4:32:20<12:08:42, 12.72s/it] 26%|â–ˆâ–ˆâ–Œ       | 1183/4620 [4:32:32<12:05:03, 12.66s/it] 26%|â–ˆâ–ˆâ–Œ       | 1184/4620 [4:32:47<12:41:18, 13.29s/it] 26%|â–ˆâ–ˆâ–Œ       | 1185/4620 [4:32:59<12:11:40, 12.78s/it] 26%|â–ˆâ–ˆâ–Œ       | 1186/4620 [4:33:10<11:54:09, 12.48s/it] 26%|â–ˆâ–ˆâ–Œ       | 1187/4620 [4:33:24<12:15:58, 12.86s/it] 26%|â–ˆâ–ˆâ–Œ       | 1188/4620 [4:33:38<12:31:58, 13.15s/it] 26%|â–ˆâ–ˆâ–Œ       | 1189/4620 [4:33:50<12:07:39, 12.73s/it] 26%|â–ˆâ–ˆâ–Œ       | 1190/4620 [4:34:01<11:51:25, 12.44s/it]                                                        {'loss': 0.0914, 'grad_norm': 0.1591796875, 'learning_rate': 0.00037942477876106194, 'epoch': 1.29}
 26%|â–ˆâ–ˆâ–Œ       | 1190/4620 [4:34:02<11:51:25, 12.44s/it] 26%|â–ˆâ–ˆâ–Œ       | 1191/4620 [4:34:14<11:55:46, 12.52s/it] 26%|â–ˆâ–ˆâ–Œ       | 1192/4620 [4:34:27<11:51:54, 12.46s/it] 26%|â–ˆâ–ˆâ–Œ       | 1193/4620 [4:34:41<12:18:45, 12.93s/it] 26%|â–ˆâ–ˆâ–Œ       | 1194/4620 [4:34:52<11:57:48, 12.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 1195/4620 [4:35:04<11:48:51, 12.42s/it] 26%|â–ˆâ–ˆâ–Œ       | 1196/4620 [4:35:18<12:17:13, 12.92s/it] 26%|â–ˆâ–ˆâ–Œ       | 1197/4620 [4:35:31<12:06:52, 12.74s/it] 26%|â–ˆâ–ˆâ–Œ       | 1198/4620 [4:35:45<12:33:51, 13.22s/it] 26%|â–ˆâ–ˆâ–Œ       | 1199/4620 [4:35:57<12:17:24, 12.93s/it] 26%|â–ˆâ–ˆâ–Œ       | 1200/4620 [4:36:10<12:07:33, 12.76s/it]                                                        {'loss': 0.0787, 'grad_norm': 0.92578125, 'learning_rate': 0.00037831858407079644, 'epoch': 1.3}
 26%|â–ˆâ–ˆâ–Œ       | 1200/4620 [4:36:10<12:07:33, 12.76s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:51,  3.66s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:37,  2.89s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:29,  2.42s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.16s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  1.97s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.83s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.77s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.76s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.70s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.64s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.61s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.64s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.92s/it][A                                                        
                                               [A{'eval_loss': 0.0681130513548851, 'eval_runtime': 67.1072, 'eval_samples_per_second': 7.451, 'eval_steps_per_second': 0.238, 'epoch': 1.3}
 26%|â–ˆâ–ˆâ–Œ       | 1200/4620 [4:37:17<12:07:33, 12.76s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.92s/it][A
                                               [A 26%|â–ˆâ–ˆâ–Œ       | 1201/4620 [4:37:27<30:26:10, 32.05s/it] 26%|â–ˆâ–ˆâ–Œ       | 1202/4620 [4:37:38<24:37:59, 25.94s/it] 26%|â–ˆâ–ˆâ–Œ       | 1203/4620 [4:37:52<21:03:08, 22.18s/it] 26%|â–ˆâ–ˆâ–Œ       | 1204/4620 [4:38:04<18:17:29, 19.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 1205/4620 [4:38:17<16:29:38, 17.39s/it] 26%|â–ˆâ–ˆâ–Œ       | 1206/4620 [4:38:31<15:27:08, 16.29s/it] 26%|â–ˆâ–ˆâ–Œ       | 1207/4620 [4:38:45<14:42:28, 15.51s/it] 26%|â–ˆâ–ˆâ–Œ       | 1208/4620 [4:38:57<13:37:40, 14.38s/it] 26%|â–ˆâ–ˆâ–Œ       | 1209/4620 [4:39:09<13:01:41, 13.75s/it] 26%|â–ˆâ–ˆâ–Œ       | 1210/4620 [4:39:21<12:43:30, 13.43s/it]                                                        {'loss': 0.0709, 'grad_norm': 0.44921875, 'learning_rate': 0.000377212389380531, 'epoch': 1.31}
 26%|â–ˆâ–ˆâ–Œ       | 1210/4620 [4:39:22<12:43:30, 13.43s/it] 26%|â–ˆâ–ˆâ–Œ       | 1211/4620 [4:39:34<12:30:56, 13.22s/it] 26%|â–ˆâ–ˆâ–Œ       | 1212/4620 [4:39:47<12:24:51, 13.11s/it] 26%|â–ˆâ–ˆâ–‹       | 1213/4620 [4:40:01<12:39:22, 13.37s/it] 26%|â–ˆâ–ˆâ–‹       | 1214/4620 [4:40:13<12:07:19, 12.81s/it] 26%|â–ˆâ–ˆâ–‹       | 1215/4620 [4:40:27<12:28:01, 13.18s/it] 26%|â–ˆâ–ˆâ–‹       | 1216/4620 [4:40:39<12:13:59, 12.94s/it] 26%|â–ˆâ–ˆâ–‹       | 1217/4620 [4:40:53<12:29:24, 13.21s/it] 26%|â–ˆâ–ˆâ–‹       | 1218/4620 [4:41:05<12:11:44, 12.91s/it] 26%|â–ˆâ–ˆâ–‹       | 1219/4620 [4:41:17<12:02:03, 12.74s/it] 26%|â–ˆâ–ˆâ–‹       | 1220/4620 [4:41:30<11:52:33, 12.57s/it]                                                        {'loss': 0.079, 'grad_norm': 0.30078125, 'learning_rate': 0.00037610619469026545, 'epoch': 1.32}
 26%|â–ˆâ–ˆâ–‹       | 1220/4620 [4:41:30<11:52:33, 12.57s/it] 26%|â–ˆâ–ˆâ–‹       | 1221/4620 [4:41:42<11:45:47, 12.46s/it] 26%|â–ˆâ–ˆâ–‹       | 1222/4620 [4:41:56<12:14:23, 12.97s/it] 26%|â–ˆâ–ˆâ–‹       | 1223/4620 [4:42:08<12:00:24, 12.72s/it] 26%|â–ˆâ–ˆâ–‹       | 1224/4620 [4:42:20<11:52:03, 12.58s/it] 27%|â–ˆâ–ˆâ–‹       | 1225/4620 [4:42:34<12:17:54, 13.04s/it] 27%|â–ˆâ–ˆâ–‹       | 1226/4620 [4:42:48<12:34:29, 13.34s/it] 27%|â–ˆâ–ˆâ–‹       | 1227/4620 [4:43:00<12:10:16, 12.91s/it] 27%|â–ˆâ–ˆâ–‹       | 1228/4620 [4:43:13<11:57:06, 12.68s/it] 27%|â–ˆâ–ˆâ–‹       | 1229/4620 [4:43:26<12:02:06, 12.78s/it] 27%|â–ˆâ–ˆâ–‹       | 1230/4620 [4:43:38<11:58:53, 12.72s/it]                                                        {'loss': 0.1068, 'grad_norm': 0.23828125, 'learning_rate': 0.000375, 'epoch': 1.33}
 27%|â–ˆâ–ˆâ–‹       | 1230/4620 [4:43:38<11:58:53, 12.72s/it] 27%|â–ˆâ–ˆâ–‹       | 1231/4620 [4:43:51<12:02:11, 12.79s/it] 27%|â–ˆâ–ˆâ–‹       | 1232/4620 [4:44:04<11:57:14, 12.70s/it] 27%|â–ˆâ–ˆâ–‹       | 1233/4620 [4:44:18<12:22:02, 13.15s/it] 27%|â–ˆâ–ˆâ–‹       | 1234/4620 [4:44:31<12:19:58, 13.11s/it] 27%|â–ˆâ–ˆâ–‹       | 1235/4620 [4:44:44<12:25:27, 13.21s/it] 27%|â–ˆâ–ˆâ–‹       | 1236/4620 [4:44:56<11:59:58, 12.77s/it] 27%|â–ˆâ–ˆâ–‹       | 1237/4620 [4:45:08<11:53:11, 12.65s/it] 27%|â–ˆâ–ˆâ–‹       | 1238/4620 [4:45:21<11:53:49, 12.66s/it] 27%|â–ˆâ–ˆâ–‹       | 1239/4620 [4:45:34<11:51:45, 12.63s/it] 27%|â–ˆâ–ˆâ–‹       | 1240/4620 [4:45:46<11:50:54, 12.62s/it]                                                        {'loss': 0.0946, 'grad_norm': 0.384765625, 'learning_rate': 0.00037389380530973457, 'epoch': 1.34}
 27%|â–ˆâ–ˆâ–‹       | 1240/4620 [4:45:46<11:50:54, 12.62s/it] 27%|â–ˆâ–ˆâ–‹       | 1241/4620 [4:45:59<11:55:54, 12.71s/it] 27%|â–ˆâ–ˆâ–‹       | 1242/4620 [4:46:11<11:41:27, 12.46s/it] 27%|â–ˆâ–ˆâ–‹       | 1243/4620 [4:46:25<12:12:28, 13.01s/it] 27%|â–ˆâ–ˆâ–‹       | 1244/4620 [4:46:38<12:11:02, 12.99s/it] 27%|â–ˆâ–ˆâ–‹       | 1245/4620 [4:46:51<12:13:40, 13.04s/it] 27%|â–ˆâ–ˆâ–‹       | 1246/4620 [4:47:04<12:14:46, 13.07s/it] 27%|â–ˆâ–ˆâ–‹       | 1247/4620 [4:47:16<11:56:28, 12.74s/it] 27%|â–ˆâ–ˆâ–‹       | 1248/4620 [4:47:28<11:42:50, 12.51s/it] 27%|â–ˆâ–ˆâ–‹       | 1249/4620 [4:47:41<11:44:16, 12.54s/it] 27%|â–ˆâ–ˆâ–‹       | 1250/4620 [4:47:54<11:48:00, 12.61s/it]                                                        {'loss': 0.0891, 'grad_norm': 0.392578125, 'learning_rate': 0.000372787610619469, 'epoch': 1.35}
 27%|â–ˆâ–ˆâ–‹       | 1250/4620 [4:47:54<11:48:00, 12.61s/it] 27%|â–ˆâ–ˆâ–‹       | 1251/4620 [4:48:06<11:46:20, 12.58s/it] 27%|â–ˆâ–ˆâ–‹       | 1252/4620 [4:48:19<11:45:29, 12.57s/it] 27%|â–ˆâ–ˆâ–‹       | 1253/4620 [4:48:33<12:10:10, 13.01s/it] 27%|â–ˆâ–ˆâ–‹       | 1254/4620 [4:48:46<12:12:01, 13.05s/it] 27%|â–ˆâ–ˆâ–‹       | 1255/4620 [4:48:58<11:57:48, 12.80s/it] 27%|â–ˆâ–ˆâ–‹       | 1256/4620 [4:49:12<12:08:50, 13.00s/it] 27%|â–ˆâ–ˆâ–‹       | 1257/4620 [4:49:24<11:49:53, 12.67s/it] 27%|â–ˆâ–ˆâ–‹       | 1258/4620 [4:49:36<11:40:28, 12.50s/it] 27%|â–ˆâ–ˆâ–‹       | 1259/4620 [4:49:48<11:41:02, 12.51s/it] 27%|â–ˆâ–ˆâ–‹       | 1260/4620 [4:50:01<11:39:35, 12.49s/it]                                                        {'loss': 0.088, 'grad_norm': 0.255859375, 'learning_rate': 0.0003716814159292036, 'epoch': 1.36}
 27%|â–ˆâ–ˆâ–‹       | 1260/4620 [4:50:01<11:39:35, 12.49s/it] 27%|â–ˆâ–ˆâ–‹       | 1261/4620 [4:50:13<11:37:14, 12.45s/it] 27%|â–ˆâ–ˆâ–‹       | 1262/4620 [4:50:26<11:41:26, 12.53s/it] 27%|â–ˆâ–ˆâ–‹       | 1263/4620 [4:50:40<12:02:03, 12.91s/it] 27%|â–ˆâ–ˆâ–‹       | 1264/4620 [4:50:53<12:12:58, 13.10s/it] 27%|â–ˆâ–ˆâ–‹       | 1265/4620 [4:51:05<11:53:43, 12.76s/it] 27%|â–ˆâ–ˆâ–‹       | 1266/4620 [4:51:19<12:05:18, 12.98s/it] 27%|â–ˆâ–ˆâ–‹       | 1267/4620 [4:51:31<11:51:17, 12.73s/it] 27%|â–ˆâ–ˆâ–‹       | 1268/4620 [4:51:43<11:45:09, 12.62s/it] 27%|â–ˆâ–ˆâ–‹       | 1269/4620 [4:51:56<11:48:02, 12.68s/it] 27%|â–ˆâ–ˆâ–‹       | 1270/4620 [4:52:09<11:51:28, 12.74s/it]                                                        {'loss': 0.0811, 'grad_norm': 0.185546875, 'learning_rate': 0.0003705752212389381, 'epoch': 1.37}
 27%|â–ˆâ–ˆâ–‹       | 1270/4620 [4:52:09<11:51:28, 12.74s/it] 28%|â–ˆâ–ˆâ–Š       | 1271/4620 [4:52:22<11:51:31, 12.75s/it] 28%|â–ˆâ–ˆâ–Š       | 1272/4620 [4:52:36<12:17:31, 13.22s/it] 28%|â–ˆâ–ˆâ–Š       | 1273/4620 [4:52:49<12:22:33, 13.31s/it] 28%|â–ˆâ–ˆâ–Š       | 1274/4620 [4:53:03<12:26:45, 13.39s/it] 28%|â–ˆâ–ˆâ–Š       | 1275/4620 [4:53:15<12:01:17, 12.94s/it] 28%|â–ˆâ–ˆâ–Š       | 1276/4620 [4:53:29<12:19:30, 13.27s/it] 28%|â–ˆâ–ˆâ–Š       | 1277/4620 [4:53:41<11:55:47, 12.85s/it] 28%|â–ˆâ–ˆâ–Š       | 1278/4620 [4:53:53<11:41:57, 12.60s/it] 28%|â–ˆâ–ˆâ–Š       | 1279/4620 [4:54:06<11:52:36, 12.80s/it] 28%|â–ˆâ–ˆâ–Š       | 1280/4620 [4:54:19<11:52:03, 12.79s/it]                                                        {'loss': 0.1003, 'grad_norm': 0.267578125, 'learning_rate': 0.0003694690265486726, 'epoch': 1.39}
 28%|â–ˆâ–ˆâ–Š       | 1280/4620 [4:54:19<11:52:03, 12.79s/it] 28%|â–ˆâ–ˆâ–Š       | 1281/4620 [4:54:31<11:43:43, 12.65s/it] 28%|â–ˆâ–ˆâ–Š       | 1282/4620 [4:54:46<12:14:32, 13.20s/it] 28%|â–ˆâ–ˆâ–Š       | 1283/4620 [4:54:59<12:11:01, 13.14s/it] 28%|â–ˆâ–ˆâ–Š       | 1284/4620 [4:55:12<12:11:30, 13.16s/it] 28%|â–ˆâ–ˆâ–Š       | 1285/4620 [4:55:24<11:57:07, 12.90s/it] 28%|â–ˆâ–ˆâ–Š       | 1286/4620 [4:55:38<12:13:37, 13.20s/it] 28%|â–ˆâ–ˆâ–Š       | 1287/4620 [4:55:50<11:52:45, 12.83s/it] 28%|â–ˆâ–ˆâ–Š       | 1288/4620 [4:56:02<11:38:06, 12.57s/it] 28%|â–ˆâ–ˆâ–Š       | 1289/4620 [4:56:15<11:46:05, 12.72s/it] 28%|â–ˆâ–ˆâ–Š       | 1290/4620 [4:56:28<11:50:48, 12.81s/it]                                                        {'loss': 0.0947, 'grad_norm': 0.416015625, 'learning_rate': 0.0003683628318584071, 'epoch': 1.4}
 28%|â–ˆâ–ˆâ–Š       | 1290/4620 [4:56:28<11:50:48, 12.81s/it] 28%|â–ˆâ–ˆâ–Š       | 1291/4620 [4:56:42<12:01:46, 13.01s/it] 28%|â–ˆâ–ˆâ–Š       | 1292/4620 [4:56:55<12:10:50, 13.18s/it] 28%|â–ˆâ–ˆâ–Š       | 1293/4620 [4:57:07<11:47:29, 12.76s/it] 28%|â–ˆâ–ˆâ–Š       | 1294/4620 [4:57:20<12:00:46, 13.00s/it] 28%|â–ˆâ–ˆâ–Š       | 1295/4620 [4:57:32<11:37:33, 12.59s/it] 28%|â–ˆâ–ˆâ–Š       | 1296/4620 [4:57:46<11:57:29, 12.95s/it] 28%|â–ˆâ–ˆâ–Š       | 1297/4620 [4:57:58<11:50:58, 12.84s/it] 28%|â–ˆâ–ˆâ–Š       | 1298/4620 [4:58:11<11:42:21, 12.69s/it] 28%|â–ˆâ–ˆâ–Š       | 1299/4620 [4:58:24<11:44:34, 12.73s/it] 28%|â–ˆâ–ˆâ–Š       | 1300/4620 [4:58:38<12:11:18, 13.22s/it]                                                        {'loss': 0.0981, 'grad_norm': 0.369140625, 'learning_rate': 0.0003672566371681416, 'epoch': 1.41}
 28%|â–ˆâ–ˆâ–Š       | 1300/4620 [4:58:38<12:11:18, 13.22s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:34,  2.50s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:06<00:27,  2.13s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:24,  2.00s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.92s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:18,  1.81s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.72s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:14<00:13,  1.65s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:16<00:11,  1.63s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.65s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:19<00:08,  1.69s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.65s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:24<00:03,  1.61s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.87s/it][A                                                        
                                               [A{'eval_loss': 0.07517875730991364, 'eval_runtime': 65.777, 'eval_samples_per_second': 7.601, 'eval_steps_per_second': 0.243, 'epoch': 1.41}
 28%|â–ˆâ–ˆâ–Š       | 1300/4620 [4:59:44<12:11:18, 13.22s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.87s/it][A
                                               [A 28%|â–ˆâ–ˆâ–Š       | 1301/4620 [4:59:56<29:58:49, 32.52s/it] 28%|â–ˆâ–ˆâ–Š       | 1302/4620 [5:00:07<24:05:12, 26.13s/it] 28%|â–ˆâ–ˆâ–Š       | 1303/4620 [5:00:18<20:05:19, 21.80s/it] 28%|â–ˆâ–ˆâ–Š       | 1304/4620 [5:00:31<17:25:26, 18.92s/it] 28%|â–ˆâ–ˆâ–Š       | 1305/4620 [5:00:45<16:06:28, 17.49s/it] 28%|â–ˆâ–ˆâ–Š       | 1306/4620 [5:00:57<14:34:52, 15.84s/it] 28%|â–ˆâ–ˆâ–Š       | 1307/4620 [5:01:11<14:05:29, 15.31s/it] 28%|â–ˆâ–ˆâ–Š       | 1308/4620 [5:01:23<13:11:46, 14.34s/it] 28%|â–ˆâ–ˆâ–Š       | 1309/4620 [5:01:35<12:38:45, 13.75s/it] 28%|â–ˆâ–ˆâ–Š       | 1310/4620 [5:01:49<12:44:39, 13.86s/it]                                                        {'loss': 0.0912, 'grad_norm': 0.263671875, 'learning_rate': 0.0003661504424778761, 'epoch': 1.42}
 28%|â–ˆâ–ˆâ–Š       | 1310/4620 [5:01:49<12:44:39, 13.86s/it] 28%|â–ˆâ–ˆâ–Š       | 1311/4620 [5:02:02<12:29:40, 13.59s/it] 28%|â–ˆâ–ˆâ–Š       | 1312/4620 [5:02:14<12:02:22, 13.10s/it] 28%|â–ˆâ–ˆâ–Š       | 1313/4620 [5:02:27<11:48:53, 12.86s/it] 28%|â–ˆâ–ˆâ–Š       | 1314/4620 [5:02:39<11:42:09, 12.74s/it] 28%|â–ˆâ–ˆâ–Š       | 1315/4620 [5:02:54<12:10:36, 13.26s/it] 28%|â–ˆâ–ˆâ–Š       | 1316/4620 [5:03:06<11:49:12, 12.88s/it] 29%|â–ˆâ–ˆâ–Š       | 1317/4620 [5:03:19<12:01:30, 13.11s/it] 29%|â–ˆâ–ˆâ–Š       | 1318/4620 [5:03:31<11:41:02, 12.74s/it] 29%|â–ˆâ–ˆâ–Š       | 1319/4620 [5:03:43<11:31:39, 12.57s/it] 29%|â–ˆâ–ˆâ–Š       | 1320/4620 [5:03:58<12:01:00, 13.11s/it]                                                        {'loss': 0.0881, 'grad_norm': 0.1962890625, 'learning_rate': 0.00036504424778761066, 'epoch': 1.43}
 29%|â–ˆâ–ˆâ–Š       | 1320/4620 [5:03:58<12:01:00, 13.11s/it] 29%|â–ˆâ–ˆâ–Š       | 1321/4620 [5:04:10<11:42:56, 12.78s/it] 29%|â–ˆâ–ˆâ–Š       | 1322/4620 [5:04:22<11:29:39, 12.55s/it] 29%|â–ˆâ–ˆâ–Š       | 1323/4620 [5:04:35<11:36:07, 12.67s/it] 29%|â–ˆâ–ˆâ–Š       | 1324/4620 [5:04:47<11:32:01, 12.60s/it] 29%|â–ˆâ–ˆâ–Š       | 1325/4620 [5:05:01<11:52:16, 12.97s/it] 29%|â–ˆâ–ˆâ–Š       | 1326/4620 [5:05:13<11:36:01, 12.68s/it] 29%|â–ˆâ–ˆâ–Š       | 1327/4620 [5:05:26<11:49:38, 12.93s/it] 29%|â–ˆâ–ˆâ–Š       | 1328/4620 [5:05:39<11:41:11, 12.78s/it] 29%|â–ˆâ–ˆâ–‰       | 1329/4620 [5:05:51<11:33:16, 12.64s/it] 29%|â–ˆâ–ˆâ–‰       | 1330/4620 [5:06:07<12:24:57, 13.59s/it]                                                        {'loss': 0.0855, 'grad_norm': 0.40234375, 'learning_rate': 0.0003639380530973451, 'epoch': 1.44}
 29%|â–ˆâ–ˆâ–‰       | 1330/4620 [5:06:07<12:24:57, 13.59s/it] 29%|â–ˆâ–ˆâ–‰       | 1331/4620 [5:06:19<11:52:04, 12.99s/it] 29%|â–ˆâ–ˆâ–‰       | 1332/4620 [5:06:30<11:33:56, 12.66s/it] 29%|â–ˆâ–ˆâ–‰       | 1333/4620 [5:06:43<11:27:35, 12.55s/it] 29%|â–ˆâ–ˆâ–‰       | 1334/4620 [5:06:55<11:29:28, 12.59s/it] 29%|â–ˆâ–ˆâ–‰       | 1335/4620 [5:07:10<11:55:06, 13.06s/it] 29%|â–ˆâ–ˆâ–‰       | 1336/4620 [5:07:21<11:34:24, 12.69s/it] 29%|â–ˆâ–ˆâ–‰       | 1337/4620 [5:07:35<11:52:22, 13.02s/it] 29%|â–ˆâ–ˆâ–‰       | 1338/4620 [5:07:47<11:39:11, 12.78s/it] 29%|â–ˆâ–ˆâ–‰       | 1339/4620 [5:08:01<11:46:57, 12.93s/it] 29%|â–ˆâ–ˆâ–‰       | 1340/4620 [5:08:14<11:52:45, 13.04s/it]                                                        {'loss': 0.0907, 'grad_norm': 0.28125, 'learning_rate': 0.00036283185840707967, 'epoch': 1.45}
 29%|â–ˆâ–ˆâ–‰       | 1340/4620 [5:08:14<11:52:45, 13.04s/it] 29%|â–ˆâ–ˆâ–‰       | 1341/4620 [5:08:26<11:31:38, 12.66s/it] 29%|â–ˆâ–ˆâ–‰       | 1342/4620 [5:08:38<11:24:55, 12.54s/it] 29%|â–ˆâ–ˆâ–‰       | 1343/4620 [5:08:50<11:22:07, 12.49s/it] 29%|â–ˆâ–ˆâ–‰       | 1344/4620 [5:09:03<11:20:12, 12.46s/it] 29%|â–ˆâ–ˆâ–‰       | 1345/4620 [5:09:17<11:45:09, 12.92s/it] 29%|â–ˆâ–ˆâ–‰       | 1346/4620 [5:09:29<11:40:06, 12.83s/it] 29%|â–ˆâ–ˆâ–‰       | 1347/4620 [5:09:42<11:31:32, 12.68s/it] 29%|â–ˆâ–ˆâ–‰       | 1348/4620 [5:09:56<12:00:22, 13.21s/it] 29%|â–ˆâ–ˆâ–‰       | 1349/4620 [5:10:08<11:37:16, 12.79s/it] 29%|â–ˆâ–ˆâ–‰       | 1350/4620 [5:10:21<11:46:07, 12.96s/it]                                                        {'loss': 0.083, 'grad_norm': 0.326171875, 'learning_rate': 0.00036172566371681417, 'epoch': 1.46}
 29%|â–ˆâ–ˆâ–‰       | 1350/4620 [5:10:21<11:46:07, 12.96s/it] 29%|â–ˆâ–ˆâ–‰       | 1351/4620 [5:10:33<11:27:18, 12.61s/it] 29%|â–ˆâ–ˆâ–‰       | 1352/4620 [5:10:45<11:21:56, 12.52s/it] 29%|â–ˆâ–ˆâ–‰       | 1353/4620 [5:10:58<11:22:01, 12.53s/it] 29%|â–ˆâ–ˆâ–‰       | 1354/4620 [5:11:11<11:27:27, 12.63s/it] 29%|â–ˆâ–ˆâ–‰       | 1355/4620 [5:11:25<11:51:03, 13.07s/it] 29%|â–ˆâ–ˆâ–‰       | 1356/4620 [5:11:37<11:33:56, 12.76s/it] 29%|â–ˆâ–ˆâ–‰       | 1357/4620 [5:11:49<11:27:21, 12.64s/it] 29%|â–ˆâ–ˆâ–‰       | 1358/4620 [5:12:03<11:44:24, 12.96s/it] 29%|â–ˆâ–ˆâ–‰       | 1359/4620 [5:12:15<11:30:42, 12.71s/it] 29%|â–ˆâ–ˆâ–‰       | 1360/4620 [5:12:28<11:39:06, 12.87s/it]                                                        {'loss': 0.084, 'grad_norm': 0.2216796875, 'learning_rate': 0.0003606194690265487, 'epoch': 1.47}
 29%|â–ˆâ–ˆâ–‰       | 1360/4620 [5:12:28<11:39:06, 12.87s/it] 29%|â–ˆâ–ˆâ–‰       | 1361/4620 [5:12:40<11:26:23, 12.64s/it] 29%|â–ˆâ–ˆâ–‰       | 1362/4620 [5:12:53<11:18:52, 12.50s/it] 30%|â–ˆâ–ˆâ–‰       | 1363/4620 [5:13:05<11:22:14, 12.57s/it] 30%|â–ˆâ–ˆâ–‰       | 1364/4620 [5:13:18<11:24:33, 12.61s/it] 30%|â–ˆâ–ˆâ–‰       | 1365/4620 [5:13:32<11:42:29, 12.95s/it] 30%|â–ˆâ–ˆâ–‰       | 1366/4620 [5:13:44<11:33:57, 12.80s/it] 30%|â–ˆâ–ˆâ–‰       | 1367/4620 [5:13:58<11:44:45, 13.00s/it] 30%|â–ˆâ–ˆâ–‰       | 1368/4620 [5:14:11<11:43:52, 12.99s/it] 30%|â–ˆâ–ˆâ–‰       | 1369/4620 [5:14:23<11:26:57, 12.68s/it] 30%|â–ˆâ–ˆâ–‰       | 1370/4620 [5:14:36<11:42:42, 12.97s/it]                                                        {'loss': 0.0924, 'grad_norm': 0.52734375, 'learning_rate': 0.0003595132743362832, 'epoch': 1.48}
 30%|â–ˆâ–ˆâ–‰       | 1370/4620 [5:14:36<11:42:42, 12.97s/it] 30%|â–ˆâ–ˆâ–‰       | 1371/4620 [5:14:49<11:33:08, 12.80s/it] 30%|â–ˆâ–ˆâ–‰       | 1372/4620 [5:15:01<11:23:58, 12.64s/it] 30%|â–ˆâ–ˆâ–‰       | 1373/4620 [5:15:13<11:20:36, 12.58s/it] 30%|â–ˆâ–ˆâ–‰       | 1374/4620 [5:15:26<11:21:34, 12.60s/it] 30%|â–ˆâ–ˆâ–‰       | 1375/4620 [5:15:40<11:41:47, 12.98s/it] 30%|â–ˆâ–ˆâ–‰       | 1376/4620 [5:15:52<11:32:03, 12.80s/it] 30%|â–ˆâ–ˆâ–‰       | 1377/4620 [5:16:06<11:47:59, 13.10s/it] 30%|â–ˆâ–ˆâ–‰       | 1378/4620 [5:16:19<11:45:56, 13.06s/it] 30%|â–ˆâ–ˆâ–‰       | 1379/4620 [5:16:31<11:24:17, 12.67s/it] 30%|â–ˆâ–ˆâ–‰       | 1380/4620 [5:16:44<11:37:33, 12.92s/it]                                                        {'loss': 0.0706, 'grad_norm': 0.3203125, 'learning_rate': 0.0003584070796460177, 'epoch': 1.49}
 30%|â–ˆâ–ˆâ–‰       | 1380/4620 [5:16:44<11:37:33, 12.92s/it] 30%|â–ˆâ–ˆâ–‰       | 1381/4620 [5:16:56<11:18:53, 12.58s/it] 30%|â–ˆâ–ˆâ–‰       | 1382/4620 [5:17:09<11:15:38, 12.52s/it] 30%|â–ˆâ–ˆâ–‰       | 1383/4620 [5:17:21<11:18:45, 12.58s/it] 30%|â–ˆâ–ˆâ–‰       | 1384/4620 [5:17:34<11:21:34, 12.64s/it] 30%|â–ˆâ–ˆâ–‰       | 1385/4620 [5:17:47<11:21:30, 12.64s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1386/4620 [5:18:01<11:40:49, 13.00s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1387/4620 [5:18:12<11:20:52, 12.64s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1388/4620 [5:18:26<11:38:52, 12.97s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1389/4620 [5:18:38<11:23:36, 12.69s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1390/4620 [5:18:51<11:34:43, 12.91s/it]                                                        {'loss': 0.0932, 'grad_norm': 0.35546875, 'learning_rate': 0.0003573008849557522, 'epoch': 1.5}
 30%|â–ˆâ–ˆâ–ˆ       | 1390/4620 [5:18:52<11:34:43, 12.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1391/4620 [5:19:03<11:17:48, 12.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1392/4620 [5:19:16<11:13:49, 12.52s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1393/4620 [5:19:28<11:15:19, 12.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1394/4620 [5:19:41<11:13:56, 12.53s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1395/4620 [5:19:54<11:31:20, 12.86s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1396/4620 [5:20:08<11:44:06, 13.10s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1397/4620 [5:20:20<11:26:52, 12.79s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1398/4620 [5:20:34<11:42:27, 13.08s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1399/4620 [5:20:47<11:43:15, 13.10s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1400/4620 [5:20:59<11:21:55, 12.71s/it]                                                        {'loss': 0.0926, 'grad_norm': 0.51171875, 'learning_rate': 0.00035619469026548675, 'epoch': 1.52}
 30%|â–ˆâ–ˆâ–ˆ       | 1400/4620 [5:20:59<11:21:55, 12.71s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:44,  3.15s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:33,  2.57s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:26,  2.22s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:21,  1.98s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.85s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:14<00:16,  1.79s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:16<00:14,  1.78s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:19<00:09,  1.66s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.61s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.61s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:24<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.69s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:27<00:01,  1.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.89s/it][A                                                        
                                               [A{'eval_loss': 0.0741143524646759, 'eval_runtime': 79.1227, 'eval_samples_per_second': 6.319, 'eval_steps_per_second': 0.202, 'epoch': 1.52}
 30%|â–ˆâ–ˆâ–ˆ       | 1400/4620 [5:22:18<11:21:55, 12.71s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.89s/it][A
                                               [A 30%|â–ˆâ–ˆâ–ˆ       | 1401/4620 [5:22:28<31:58:03, 35.75s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1402/4620 [5:22:40<25:26:22, 28.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1403/4620 [5:22:52<21:03:12, 23.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1404/4620 [5:23:05<18:08:54, 20.32s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1405/4620 [5:23:19<16:38:25, 18.63s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1406/4620 [5:23:33<15:19:58, 17.17s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1407/4620 [5:23:46<14:15:55, 15.98s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1408/4620 [5:23:58<13:05:37, 14.68s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1409/4620 [5:24:10<12:24:07, 13.90s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1410/4620 [5:24:24<12:19:11, 13.82s/it]                                                        {'loss': 0.0851, 'grad_norm': 0.119140625, 'learning_rate': 0.00035508849557522125, 'epoch': 1.53}
 31%|â–ˆâ–ˆâ–ˆ       | 1410/4620 [5:24:24<12:19:11, 13.82s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1411/4620 [5:24:36<11:53:44, 13.35s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1412/4620 [5:24:49<11:40:29, 13.10s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1413/4620 [5:25:01<11:31:29, 12.94s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1414/4620 [5:25:14<11:30:03, 12.91s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1415/4620 [5:25:30<12:13:42, 13.74s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1416/4620 [5:25:42<11:54:49, 13.39s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1417/4620 [5:25:54<11:31:34, 12.95s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1418/4620 [5:26:07<11:22:40, 12.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1419/4620 [5:26:19<11:20:47, 12.76s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1420/4620 [5:26:33<11:30:56, 12.96s/it]                                                        {'loss': 0.0748, 'grad_norm': 0.1728515625, 'learning_rate': 0.00035398230088495576, 'epoch': 1.54}
 31%|â–ˆâ–ˆâ–ˆ       | 1420/4620 [5:26:33<11:30:56, 12.96s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1421/4620 [5:26:45<11:24:36, 12.84s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1422/4620 [5:26:57<11:13:09, 12.63s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1423/4620 [5:27:10<11:11:52, 12.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1424/4620 [5:27:24<11:34:33, 13.04s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1425/4620 [5:27:37<11:36:47, 13.09s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1426/4620 [5:27:49<11:18:26, 12.74s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1427/4620 [5:28:03<11:37:08, 13.10s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1428/4620 [5:28:15<11:21:11, 12.80s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1429/4620 [5:28:27<11:13:24, 12.66s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1430/4620 [5:28:41<11:26:08, 12.91s/it]                                                        {'loss': 0.0722, 'grad_norm': 0.23046875, 'learning_rate': 0.0003528761061946903, 'epoch': 1.55}
 31%|â–ˆâ–ˆâ–ˆ       | 1430/4620 [5:28:41<11:26:08, 12.91s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1431/4620 [5:28:53<11:13:35, 12.67s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1432/4620 [5:29:05<11:08:46, 12.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1433/4620 [5:29:18<11:05:54, 12.54s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1434/4620 [5:29:33<11:47:35, 13.33s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1435/4620 [5:29:44<11:16:19, 12.74s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1436/4620 [5:29:56<11:01:41, 12.47s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1437/4620 [5:30:10<11:26:58, 12.95s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1438/4620 [5:30:23<11:23:08, 12.88s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1439/4620 [5:30:35<11:15:50, 12.75s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1440/4620 [5:30:49<11:32:21, 13.06s/it]                                                        {'loss': 0.0784, 'grad_norm': 0.1982421875, 'learning_rate': 0.00035176991150442477, 'epoch': 1.56}
 31%|â–ˆâ–ˆâ–ˆ       | 1440/4620 [5:30:49<11:32:21, 13.06s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1441/4620 [5:31:01<11:14:23, 12.73s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1442/4620 [5:31:14<11:08:50, 12.63s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1443/4620 [5:31:28<11:37:33, 13.17s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1444/4620 [5:31:41<11:39:25, 13.21s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1445/4620 [5:31:54<11:23:20, 12.91s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1446/4620 [5:32:06<11:14:54, 12.76s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1447/4620 [5:32:20<11:33:25, 13.11s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1448/4620 [5:32:32<11:12:13, 12.72s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1449/4620 [5:32:44<11:06:18, 12.61s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1450/4620 [5:32:57<11:04:57, 12.59s/it]                                                        {'loss': 0.0958, 'grad_norm': 0.51953125, 'learning_rate': 0.0003506637168141593, 'epoch': 1.57}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1450/4620 [5:32:57<11:04:57, 12.59s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1451/4620 [5:33:10<11:21:28, 12.90s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1452/4620 [5:33:24<11:32:43, 13.12s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1453/4620 [5:33:37<11:39:57, 13.26s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1454/4620 [5:33:49<11:15:17, 12.80s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1455/4620 [5:34:01<11:03:30, 12.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1456/4620 [5:34:14<11:01:22, 12.54s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1457/4620 [5:34:26<11:00:35, 12.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1458/4620 [5:34:40<11:21:08, 12.92s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1459/4620 [5:34:52<11:04:52, 12.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1460/4620 [5:35:05<11:04:17, 12.61s/it]                                                        {'loss': 0.0828, 'grad_norm': 0.2255859375, 'learning_rate': 0.0003495575221238938, 'epoch': 1.58}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1460/4620 [5:35:05<11:04:17, 12.61s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1461/4620 [5:35:18<11:24:06, 12.99s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1462/4620 [5:35:32<11:27:41, 13.07s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1463/4620 [5:35:45<11:31:42, 13.15s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1464/4620 [5:35:57<11:12:49, 12.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1465/4620 [5:36:09<11:01:10, 12.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1466/4620 [5:36:21<10:53:07, 12.42s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1467/4620 [5:36:34<10:55:56, 12.48s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1468/4620 [5:36:48<11:23:48, 13.02s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1469/4620 [5:37:00<11:08:23, 12.73s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1470/4620 [5:37:14<11:28:09, 13.11s/it]                                                        {'loss': 0.0853, 'grad_norm': 0.32421875, 'learning_rate': 0.00034845132743362834, 'epoch': 1.59}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1470/4620 [5:37:14<11:28:09, 13.11s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1471/4620 [5:37:27<11:32:56, 13.20s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1472/4620 [5:37:41<11:30:34, 13.16s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1473/4620 [5:37:52<11:07:12, 12.72s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1474/4620 [5:38:04<11:00:10, 12.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1475/4620 [5:38:17<11:02:22, 12.64s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1476/4620 [5:38:30<11:01:57, 12.63s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1477/4620 [5:38:43<11:04:24, 12.68s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1478/4620 [5:38:56<11:22:13, 13.03s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1479/4620 [5:39:09<11:08:39, 12.77s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1480/4620 [5:39:22<11:15:26, 12.91s/it]                                                        {'loss': 0.09, 'grad_norm': 0.39453125, 'learning_rate': 0.00034734513274336284, 'epoch': 1.6}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1480/4620 [5:39:22<11:15:26, 12.91s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1481/4620 [5:39:37<11:47:40, 13.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1482/4620 [5:39:48<11:11:22, 12.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1483/4620 [5:40:00<10:59:48, 12.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1484/4620 [5:40:13<10:57:47, 12.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1485/4620 [5:40:25<10:57:39, 12.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1486/4620 [5:40:38<10:56:23, 12.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1487/4620 [5:40:50<10:48:54, 12.43s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1488/4620 [5:41:04<11:14:26, 12.92s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1489/4620 [5:41:16<11:00:18, 12.65s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1490/4620 [5:41:30<11:13:25, 12.91s/it]                                                        {'loss': 0.0834, 'grad_norm': 0.4453125, 'learning_rate': 0.00034623893805309734, 'epoch': 1.61}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1490/4620 [5:41:30<11:13:25, 12.91s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1491/4620 [5:41:41<10:54:25, 12.55s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1492/4620 [5:41:55<11:17:18, 12.99s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1493/4620 [5:42:08<11:08:39, 12.83s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1494/4620 [5:42:20<11:01:44, 12.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1495/4620 [5:42:32<10:55:58, 12.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1496/4620 [5:42:45<10:55:36, 12.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1497/4620 [5:42:58<10:54:54, 12.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1498/4620 [5:43:11<11:12:49, 12.93s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1499/4620 [5:43:23<10:57:01, 12.63s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1500/4620 [5:43:37<11:13:22, 12.95s/it]                                                        {'loss': 0.0837, 'grad_norm': 0.57421875, 'learning_rate': 0.00034513274336283185, 'epoch': 1.62}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1500/4620 [5:43:37<11:13:22, 12.95s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:40,  2.91s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:31,  2.40s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:25,  2.14s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.92s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:17,  1.79s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.77s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:14,  1.79s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:11,  1.69s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.67s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.64s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.61s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.66s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:27<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.89s/it][A                                                        
                                               [A{'eval_loss': 0.0794643983244896, 'eval_runtime': 81.9607, 'eval_samples_per_second': 6.1, 'eval_steps_per_second': 0.195, 'epoch': 1.62}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1500/4620 [5:44:59<11:13:22, 12.95s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.89s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 32%|â–ˆâ–ˆâ–ˆâ–      | 1501/4620 [5:45:12<32:31:08, 37.53s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1502/4620 [5:45:25<26:11:31, 30.24s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1503/4620 [5:45:37<21:19:38, 24.63s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1504/4620 [5:45:49<18:07:17, 20.94s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1505/4620 [5:46:01<15:56:00, 18.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1506/4620 [5:46:14<14:25:07, 16.67s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1507/4620 [5:46:29<13:50:13, 16.00s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1508/4620 [5:46:41<12:47:30, 14.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1509/4620 [5:46:54<12:34:33, 14.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1510/4620 [5:47:07<11:55:06, 13.80s/it]                                                        {'loss': 0.0739, 'grad_norm': 0.404296875, 'learning_rate': 0.0003440265486725664, 'epoch': 1.63}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1510/4620 [5:47:07<11:55:06, 13.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1511/4620 [5:47:19<11:30:17, 13.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1512/4620 [5:47:33<11:39:25, 13.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1513/4620 [5:47:45<11:24:10, 13.21s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1514/4620 [5:47:58<11:15:29, 13.05s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1515/4620 [5:48:10<11:04:33, 12.84s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1516/4620 [5:48:23<11:00:27, 12.77s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1517/4620 [5:48:37<11:18:02, 13.11s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1518/4620 [5:48:50<11:18:58, 13.13s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1519/4620 [5:49:03<11:18:51, 13.13s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1520/4620 [5:49:15<11:01:30, 12.80s/it]                                                        {'loss': 0.0792, 'grad_norm': 0.294921875, 'learning_rate': 0.00034292035398230086, 'epoch': 1.65}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1520/4620 [5:49:15<11:01:30, 12.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1521/4620 [5:49:27<10:54:48, 12.68s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1522/4620 [5:49:42<11:17:48, 13.13s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1523/4620 [5:49:54<10:59:30, 12.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1524/4620 [5:50:06<10:51:21, 12.62s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1525/4620 [5:50:18<10:48:59, 12.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1526/4620 [5:50:31<10:50:13, 12.61s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1527/4620 [5:50:45<11:10:22, 13.00s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1528/4620 [5:51:00<11:39:31, 13.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1529/4620 [5:51:11<11:03:57, 12.89s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1530/4620 [5:51:23<10:52:55, 12.68s/it]                                                        {'loss': 0.0841, 'grad_norm': 0.326171875, 'learning_rate': 0.0003418141592920354, 'epoch': 1.66}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1530/4620 [5:51:23<10:52:55, 12.68s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1531/4620 [5:51:36<10:52:37, 12.68s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1532/4620 [5:51:50<11:12:49, 13.07s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1533/4620 [5:52:02<10:53:12, 12.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1534/4620 [5:52:14<10:50:38, 12.65s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1535/4620 [5:52:26<10:42:33, 12.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1536/4620 [5:52:39<10:47:19, 12.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1537/4620 [5:52:53<11:09:07, 13.02s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1538/4620 [5:53:07<11:14:16, 13.13s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1539/4620 [5:53:19<10:54:08, 12.74s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1540/4620 [5:53:31<10:43:33, 12.54s/it]                                                        {'loss': 0.087, 'grad_norm': 0.421875, 'learning_rate': 0.00034070796460176987, 'epoch': 1.67}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1540/4620 [5:53:31<10:43:33, 12.54s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1541/4620 [5:53:43<10:44:30, 12.56s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1542/4620 [5:53:57<11:07:39, 13.01s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1543/4620 [5:54:09<10:51:45, 12.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1544/4620 [5:54:21<10:42:35, 12.53s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1545/4620 [5:54:34<10:42:00, 12.53s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1546/4620 [5:54:46<10:41:23, 12.52s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1547/4620 [5:55:02<11:26:45, 13.41s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1548/4620 [5:55:15<11:22:52, 13.34s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1549/4620 [5:55:27<10:54:35, 12.79s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1550/4620 [5:55:39<10:49:07, 12.69s/it]                                                        {'loss': 0.0831, 'grad_norm': 0.2734375, 'learning_rate': 0.0003396017699115044, 'epoch': 1.68}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1550/4620 [5:55:39<10:49:07, 12.69s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1551/4620 [5:55:51<10:44:28, 12.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1552/4620 [5:56:06<11:10:01, 13.10s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1553/4620 [5:56:18<10:54:56, 12.81s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1554/4620 [5:56:30<10:46:45, 12.66s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1555/4620 [5:56:43<10:47:36, 12.68s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1556/4620 [5:56:57<11:10:29, 13.13s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1557/4620 [5:57:09<10:52:10, 12.78s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1558/4620 [5:57:23<11:11:55, 13.17s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1559/4620 [5:57:35<10:54:58, 12.84s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1560/4620 [5:57:47<10:46:01, 12.67s/it]                                                        {'loss': 0.0747, 'grad_norm': 0.4609375, 'learning_rate': 0.000338495575221239, 'epoch': 1.69}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1560/4620 [5:57:47<10:46:01, 12.67s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1561/4620 [5:58:01<10:55:42, 12.86s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1562/4620 [5:58:13<10:46:38, 12.69s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1563/4620 [5:58:26<10:45:08, 12.66s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1564/4620 [5:58:38<10:46:51, 12.70s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1565/4620 [5:58:51<10:42:05, 12.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1566/4620 [5:59:05<11:01:58, 13.01s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1567/4620 [5:59:16<10:41:38, 12.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1568/4620 [5:59:30<10:53:05, 12.84s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1569/4620 [5:59:42<10:39:44, 12.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1570/4620 [5:59:54<10:38:06, 12.55s/it]                                                        {'loss': 0.0825, 'grad_norm': 0.1904296875, 'learning_rate': 0.00033738938053097344, 'epoch': 1.7}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1570/4620 [5:59:54<10:38:06, 12.55s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1571/4620 [6:00:08<10:55:48, 12.91s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1572/4620 [6:00:20<10:39:51, 12.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1573/4620 [6:00:32<10:32:57, 12.46s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1574/4620 [6:00:45<10:37:50, 12.56s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1575/4620 [6:00:58<10:53:10, 12.87s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1576/4620 [6:01:10<10:36:57, 12.56s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1577/4620 [6:01:22<10:32:11, 12.47s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1578/4620 [6:01:36<10:54:49, 12.92s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1579/4620 [6:01:49<10:41:57, 12.67s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1580/4620 [6:02:02<10:49:30, 12.82s/it]                                                        {'loss': 0.0806, 'grad_norm': 0.33203125, 'learning_rate': 0.000336283185840708, 'epoch': 1.71}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1580/4620 [6:02:02<10:49:30, 12.82s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1581/4620 [6:02:13<10:33:03, 12.50s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1582/4620 [6:02:26<10:32:01, 12.48s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1583/4620 [6:02:39<10:36:08, 12.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1584/4620 [6:02:52<10:49:13, 12.83s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1585/4620 [6:03:05<10:53:12, 12.91s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1586/4620 [6:03:17<10:42:40, 12.71s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1587/4620 [6:03:30<10:34:29, 12.55s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1588/4620 [6:03:43<10:53:54, 12.94s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1589/4620 [6:03:57<10:59:30, 13.06s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1590/4620 [6:04:09<10:42:15, 12.72s/it]                                                        {'loss': 0.0821, 'grad_norm': 0.154296875, 'learning_rate': 0.0003351769911504425, 'epoch': 1.72}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1590/4620 [6:04:09<10:42:15, 12.72s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1591/4620 [6:04:21<10:36:42, 12.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1592/4620 [6:04:34<10:42:27, 12.73s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1593/4620 [6:04:46<10:34:43, 12.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1594/4620 [6:05:02<11:14:26, 13.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1595/4620 [6:05:13<10:48:06, 12.85s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1596/4620 [6:05:26<10:41:28, 12.73s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1597/4620 [6:05:38<10:36:39, 12.64s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1598/4620 [6:05:52<10:54:59, 13.00s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1599/4620 [6:06:04<10:36:57, 12.65s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1600/4620 [6:06:17<10:49:38, 12.91s/it]                                                        {'loss': 0.0802, 'grad_norm': 0.181640625, 'learning_rate': 0.000334070796460177, 'epoch': 1.73}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1600/4620 [6:06:17<10:49:38, 12.91s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:55,  3.99s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:40,  3.11s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:30,  2.54s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:24,  2.20s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  2.00s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:16,  1.88s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.81s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.68s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.64s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.59s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.67s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.89s/it][A                                                        
                                               [A{'eval_loss': 0.06338265538215637, 'eval_runtime': 80.5032, 'eval_samples_per_second': 6.211, 'eval_steps_per_second': 0.199, 'epoch': 1.73}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1600/4620 [6:07:38<10:49:38, 12.91s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.89s/it][A
                                               [A 35%|â–ˆâ–ˆâ–ˆâ–      | 1601/4620 [6:07:48<30:25:05, 36.27s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1602/4620 [6:08:00<24:13:21, 28.89s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1603/4620 [6:08:13<20:24:39, 24.36s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1604/4620 [6:08:27<17:34:01, 20.97s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1605/4620 [6:08:39<15:21:25, 18.34s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1606/4620 [6:08:51<13:48:28, 16.49s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1607/4620 [6:09:04<12:51:04, 15.36s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1608/4620 [6:09:18<12:42:36, 15.19s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1609/4620 [6:09:30<11:53:13, 14.21s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1610/4620 [6:09:44<11:46:24, 14.08s/it]                                                        {'loss': 0.0869, 'grad_norm': 0.28515625, 'learning_rate': 0.0003329646017699115, 'epoch': 1.74}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1610/4620 [6:09:44<11:46:24, 14.08s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1611/4620 [6:09:56<11:17:49, 13.52s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1612/4620 [6:10:09<11:03:03, 13.23s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1613/4620 [6:10:24<11:34:27, 13.86s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1614/4620 [6:10:36<11:03:10, 13.24s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1615/4620 [6:10:48<10:48:25, 12.95s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1616/4620 [6:11:01<10:42:58, 12.84s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1617/4620 [6:11:15<11:03:08, 13.25s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1618/4620 [6:11:27<10:41:26, 12.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1619/4620 [6:11:39<10:33:04, 12.66s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1620/4620 [6:11:53<10:55:51, 13.12s/it]                                                        {'loss': 0.0702, 'grad_norm': 0.37109375, 'learning_rate': 0.00033185840707964607, 'epoch': 1.75}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1620/4620 [6:11:53<10:55:51, 13.12s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1621/4620 [6:12:05<10:40:36, 12.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1622/4620 [6:12:19<10:53:03, 13.07s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1623/4620 [6:12:32<10:47:49, 12.97s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1624/4620 [6:12:44<10:36:15, 12.74s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1625/4620 [6:12:56<10:26:28, 12.55s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1626/4620 [6:13:09<10:29:40, 12.62s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1627/4620 [6:13:23<10:51:49, 13.07s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1628/4620 [6:13:35<10:39:28, 12.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1629/4620 [6:13:48<10:33:07, 12.70s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1630/4620 [6:14:02<10:59:23, 13.23s/it]                                                        {'loss': 0.0795, 'grad_norm': 0.24609375, 'learning_rate': 0.0003307522123893805, 'epoch': 1.76}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1630/4620 [6:14:02<10:59:23, 13.23s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1631/4620 [6:14:16<11:07:53, 13.41s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1632/4620 [6:14:29<11:02:28, 13.30s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1633/4620 [6:14:41<10:41:29, 12.89s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1634/4620 [6:14:53<10:32:07, 12.70s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1635/4620 [6:15:06<10:34:46, 12.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1636/4620 [6:15:20<10:49:49, 13.07s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1637/4620 [6:15:32<10:33:58, 12.75s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1638/4620 [6:15:45<10:30:31, 12.69s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1639/4620 [6:15:57<10:27:20, 12.63s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1640/4620 [6:16:11<10:47:10, 13.03s/it]                                                        {'loss': 0.0749, 'grad_norm': 0.263671875, 'learning_rate': 0.0003296460176991151, 'epoch': 1.77}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1640/4620 [6:16:11<10:47:10, 13.03s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1641/4620 [6:16:26<11:18:00, 13.66s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1642/4620 [6:16:37<10:43:52, 12.97s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1643/4620 [6:16:49<10:27:34, 12.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1644/4620 [6:17:02<10:20:12, 12.50s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1645/4620 [6:17:14<10:24:46, 12.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1646/4620 [6:17:27<10:25:27, 12.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1647/4620 [6:17:41<10:45:51, 13.03s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1648/4620 [6:17:53<10:31:03, 12.74s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1649/4620 [6:18:05<10:25:43, 12.64s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1650/4620 [6:18:20<10:50:47, 13.15s/it]                                                        {'loss': 0.0852, 'grad_norm': 0.16796875, 'learning_rate': 0.0003285398230088495, 'epoch': 1.79}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1650/4620 [6:18:20<10:50:47, 13.15s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1651/4620 [6:18:34<10:58:38, 13.31s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1652/4620 [6:18:46<10:42:23, 12.99s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1653/4620 [6:18:58<10:33:13, 12.81s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1654/4620 [6:19:11<10:28:41, 12.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1655/4620 [6:19:23<10:23:43, 12.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1656/4620 [6:19:36<10:28:29, 12.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1657/4620 [6:19:50<10:41:41, 12.99s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1658/4620 [6:20:02<10:27:01, 12.70s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1659/4620 [6:20:14<10:22:40, 12.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1660/4620 [6:20:30<11:09:47, 13.58s/it]                                                        {'loss': 0.0792, 'grad_norm': 0.2041015625, 'learning_rate': 0.0003274336283185841, 'epoch': 1.8}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1660/4620 [6:20:30<11:09:47, 13.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1661/4620 [6:20:41<10:36:29, 12.91s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1662/4620 [6:20:53<10:27:00, 12.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1663/4620 [6:21:06<10:20:13, 12.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1664/4620 [6:21:19<10:22:59, 12.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1665/4620 [6:21:31<10:23:19, 12.66s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1666/4620 [6:21:44<10:28:07, 12.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1667/4620 [6:21:58<10:45:16, 13.11s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1668/4620 [6:22:10<10:30:01, 12.81s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1669/4620 [6:22:24<10:42:42, 13.07s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1670/4620 [6:22:37<10:45:02, 13.12s/it]                                                        {'loss': 0.0841, 'grad_norm': 0.3515625, 'learning_rate': 0.0003263274336283186, 'epoch': 1.81}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1670/4620 [6:22:37<10:45:02, 13.12s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1671/4620 [6:22:49<10:27:44, 12.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1672/4620 [6:23:02<10:26:22, 12.75s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1673/4620 [6:23:14<10:24:49, 12.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1674/4620 [6:23:27<10:19:18, 12.61s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1675/4620 [6:23:39<10:18:37, 12.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1676/4620 [6:23:52<10:10:57, 12.45s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1677/4620 [6:24:04<10:15:56, 12.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1678/4620 [6:24:18<10:36:12, 12.98s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1679/4620 [6:24:33<11:01:07, 13.49s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1680/4620 [6:24:46<10:51:46, 13.30s/it]                                                        {'loss': 0.08, 'grad_norm': 0.4921875, 'learning_rate': 0.0003252212389380531, 'epoch': 1.82}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1680/4620 [6:24:46<10:51:46, 13.30s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1681/4620 [6:24:58<10:29:44, 12.86s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1682/4620 [6:25:10<10:21:58, 12.70s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1683/4620 [6:25:22<10:17:35, 12.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1684/4620 [6:25:35<10:17:55, 12.63s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1685/4620 [6:25:47<10:14:04, 12.55s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1686/4620 [6:26:00<10:13:39, 12.55s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1687/4620 [6:26:13<10:16:36, 12.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1688/4620 [6:26:28<11:00:18, 13.51s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1689/4620 [6:26:41<10:48:43, 13.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1690/4620 [6:26:54<10:45:54, 13.23s/it]                                                        {'loss': 0.0676, 'grad_norm': 0.294921875, 'learning_rate': 0.0003241150442477876, 'epoch': 1.83}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1690/4620 [6:26:54<10:45:54, 13.23s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1691/4620 [6:27:06<10:29:28, 12.89s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1692/4620 [6:27:19<10:21:09, 12.73s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1693/4620 [6:27:31<10:14:39, 12.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1694/4620 [6:27:43<10:11:48, 12.55s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1695/4620 [6:27:56<10:14:54, 12.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1696/4620 [6:28:09<10:14:05, 12.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1697/4620 [6:28:21<10:08:56, 12.50s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1698/4620 [6:28:37<10:55:41, 13.46s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1699/4620 [6:28:50<10:50:20, 13.36s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1700/4620 [6:29:01<10:17:46, 12.69s/it]                                                        {'loss': 0.0671, 'grad_norm': 0.373046875, 'learning_rate': 0.00032300884955752216, 'epoch': 1.84}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1700/4620 [6:29:01<10:17:46, 12.69s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:10<01:14,  5.31s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:12<00:49,  3.83s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:13<00:35,  2.96s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:15<00:27,  2.46s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:21,  2.15s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:18<00:17,  1.95s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:14,  1.81s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:21<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:23<00:10,  1.78s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.71s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.69s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:28<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:31<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.92s/it][A                                                        
                                               [A{'eval_loss': 0.06547119468450546, 'eval_runtime': 84.0238, 'eval_samples_per_second': 5.951, 'eval_steps_per_second': 0.19, 'epoch': 1.84}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1700/4620 [6:30:25<10:17:46, 12.69s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:34<00:00,  1.92s/it][A
                                               [A 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1701/4620 [6:30:35<30:11:33, 37.24s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1702/4620 [6:30:47<23:55:19, 29.51s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1703/4620 [6:30:59<19:44:53, 24.37s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1704/4620 [6:31:12<16:53:11, 20.85s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1705/4620 [6:31:25<14:52:41, 18.37s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1706/4620 [6:31:37<13:32:53, 16.74s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1707/4620 [6:31:50<12:34:05, 15.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1708/4620 [6:32:06<12:39:17, 15.64s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1709/4620 [6:32:18<11:41:42, 14.46s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1710/4620 [6:32:32<11:33:27, 14.30s/it]                                                        {'loss': 0.0757, 'grad_norm': 0.1748046875, 'learning_rate': 0.0003219026548672566, 'epoch': 1.85}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1710/4620 [6:32:32<11:33:27, 14.30s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1711/4620 [6:32:44<11:06:54, 13.76s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1712/4620 [6:32:57<10:51:42, 13.45s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1713/4620 [6:33:10<10:40:07, 13.21s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1714/4620 [6:33:22<10:34:16, 13.10s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1715/4620 [6:33:35<10:33:36, 13.09s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1716/4620 [6:33:48<10:25:59, 12.93s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1717/4620 [6:34:01<10:22:34, 12.87s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1718/4620 [6:34:17<11:07:50, 13.81s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1719/4620 [6:34:28<10:32:48, 13.09s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1720/4620 [6:34:42<10:36:05, 13.16s/it]                                                        {'loss': 0.1007, 'grad_norm': 0.330078125, 'learning_rate': 0.00032079646017699117, 'epoch': 1.86}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1720/4620 [6:34:42<10:36:05, 13.16s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1721/4620 [6:34:54<10:23:20, 12.90s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1722/4620 [6:35:06<10:17:52, 12.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1723/4620 [6:35:19<10:15:41, 12.75s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1724/4620 [6:35:31<10:10:59, 12.66s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1725/4620 [6:35:44<10:12:43, 12.70s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1726/4620 [6:35:57<10:07:52, 12.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1727/4620 [6:36:10<10:19:22, 12.85s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1728/4620 [6:36:23<10:22:47, 12.92s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1729/4620 [6:36:35<10:12:46, 12.72s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1730/4620 [6:36:49<10:25:19, 12.98s/it]                                                        {'loss': 0.0719, 'grad_norm': 0.2060546875, 'learning_rate': 0.00031969026548672567, 'epoch': 1.87}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1730/4620 [6:36:49<10:25:19, 12.98s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1731/4620 [6:37:01<10:11:24, 12.70s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1732/4620 [6:37:13<10:07:56, 12.63s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1733/4620 [6:37:26<10:07:30, 12.63s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1734/4620 [6:37:39<10:05:51, 12.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1735/4620 [6:37:51<10:01:19, 12.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1736/4620 [6:38:04<10:06:14, 12.61s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1737/4620 [6:38:18<10:26:29, 13.04s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1738/4620 [6:38:31<10:28:15, 13.08s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1739/4620 [6:38:44<10:31:58, 13.16s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1740/4620 [6:38:57<10:28:53, 13.10s/it]                                                        {'loss': 0.0929, 'grad_norm': 0.314453125, 'learning_rate': 0.0003185840707964602, 'epoch': 1.88}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1740/4620 [6:38:57<10:28:53, 13.10s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1741/4620 [6:39:09<10:12:08, 12.76s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1742/4620 [6:39:22<10:07:26, 12.66s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1743/4620 [6:39:34<10:06:58, 12.66s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1744/4620 [6:39:47<10:07:07, 12.67s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1745/4620 [6:39:59<10:03:48, 12.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1746/4620 [6:40:13<10:21:26, 12.97s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1747/4620 [6:40:26<10:15:24, 12.85s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1748/4620 [6:40:39<10:21:10, 12.98s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1749/4620 [6:40:52<10:24:55, 13.06s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1750/4620 [6:41:04<10:08:36, 12.72s/it]                                                        {'loss': 0.086, 'grad_norm': 0.25390625, 'learning_rate': 0.00031747787610619474, 'epoch': 1.89}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1750/4620 [6:41:04<10:08:36, 12.72s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1751/4620 [6:41:18<10:17:08, 12.91s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1752/4620 [6:41:30<10:06:31, 12.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1753/4620 [6:41:42<9:53:31, 12.42s/it]  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1754/4620 [6:41:54<9:55:04, 12.46s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1755/4620 [6:42:07<9:58:44, 12.54s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1756/4620 [6:42:19<9:55:04, 12.47s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1757/4620 [6:42:33<10:12:06, 12.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1758/4620 [6:42:46<10:19:25, 12.99s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1759/4620 [6:43:00<10:27:55, 13.17s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1760/4620 [6:43:12<10:09:49, 12.79s/it]                                                        {'loss': 0.0746, 'grad_norm': 0.365234375, 'learning_rate': 0.0003163716814159292, 'epoch': 1.9}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1760/4620 [6:43:12<10:09:49, 12.79s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1761/4620 [6:43:25<10:20:42, 13.03s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1762/4620 [6:43:38<10:10:08, 12.81s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1763/4620 [6:43:50<10:00:51, 12.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1764/4620 [6:44:02<9:55:38, 12.51s/it]  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1765/4620 [6:44:15<9:55:47, 12.52s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1766/4620 [6:44:27<9:52:49, 12.46s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1767/4620 [6:44:41<10:11:28, 12.86s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1768/4620 [6:44:54<10:23:01, 13.11s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1769/4620 [6:45:08<10:22:33, 13.10s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1770/4620 [6:45:19<9:57:35, 12.58s/it]                                                        {'loss': 0.0851, 'grad_norm': 0.21875, 'learning_rate': 0.00031526548672566374, 'epoch': 1.92}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1770/4620 [6:45:19<9:57:35, 12.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1771/4620 [6:45:32<10:09:31, 12.84s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1772/4620 [6:45:45<10:02:39, 12.70s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1773/4620 [6:45:57<9:57:49, 12.60s/it]  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1774/4620 [6:46:10<9:55:42, 12.56s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1775/4620 [6:46:22<9:55:10, 12.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1776/4620 [6:46:35<10:00:21, 12.67s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1777/4620 [6:46:49<10:14:28, 12.97s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1778/4620 [6:47:01<10:11:12, 12.90s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1779/4620 [6:47:14<10:03:32, 12.75s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1780/4620 [6:47:28<10:18:11, 13.06s/it]                                                        {'loss': 0.0854, 'grad_norm': 0.234375, 'learning_rate': 0.00031415929203539825, 'epoch': 1.93}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1780/4620 [6:47:28<10:18:11, 13.06s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1781/4620 [6:47:41<10:22:11, 13.15s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1782/4620 [6:47:53<10:04:25, 12.78s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1783/4620 [6:48:06<10:02:47, 12.75s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1784/4620 [6:48:18<10:02:19, 12.74s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1785/4620 [6:48:31<9:56:17, 12.62s/it]  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1786/4620 [6:48:43<9:55:12, 12.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1787/4620 [6:48:57<10:12:17, 12.97s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1788/4620 [6:49:10<10:14:25, 13.02s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1789/4620 [6:49:22<9:53:27, 12.58s/it]  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1790/4620 [6:49:36<10:13:11, 13.00s/it]                                                        {'loss': 0.079, 'grad_norm': 0.1904296875, 'learning_rate': 0.00031305309734513275, 'epoch': 1.94}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1790/4620 [6:49:36<10:13:11, 13.00s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1791/4620 [6:49:49<10:16:30, 13.08s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1792/4620 [6:50:01<10:00:11, 12.73s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1793/4620 [6:50:13<9:54:44, 12.62s/it]  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1794/4620 [6:50:26<9:55:59, 12.65s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1795/4620 [6:50:38<9:53:55, 12.61s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1796/4620 [6:50:51<9:49:31, 12.53s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1797/4620 [6:51:05<10:14:07, 13.05s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1798/4620 [6:51:18<10:17:51, 13.14s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1799/4620 [6:51:30<9:52:25, 12.60s/it]  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1800/4620 [6:51:43<10:05:28, 12.88s/it]                                                        {'loss': 0.0814, 'grad_norm': 0.4453125, 'learning_rate': 0.00031194690265486726, 'epoch': 1.95}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1800/4620 [6:51:43<10:05:28, 12.88s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:40,  2.91s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:31,  2.44s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:26,  2.22s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.98s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.85s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:14<00:15,  1.76s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.68s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:11,  1.68s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:19<00:10,  1.70s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.71s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.63s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.61s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:27<00:01,  1.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.88s/it][A                                                        
                                               [A{'eval_loss': 0.05797308310866356, 'eval_runtime': 71.9832, 'eval_samples_per_second': 6.946, 'eval_steps_per_second': 0.222, 'epoch': 1.95}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1800/4620 [6:52:55<10:05:28, 12.88s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.88s/it][A
                                               [A 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1801/4620 [6:53:07<26:42:55, 34.12s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1802/4620 [6:53:18<21:24:17, 27.34s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1803/4620 [6:53:31<17:52:01, 22.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1804/4620 [6:53:43<15:27:48, 19.77s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1805/4620 [6:53:56<13:51:47, 17.73s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1806/4620 [6:54:09<12:41:03, 16.23s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1807/4620 [6:54:24<12:15:19, 15.68s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1808/4620 [6:54:36<11:28:34, 14.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1809/4620 [6:54:48<10:54:46, 13.98s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1810/4620 [6:55:02<10:53:56, 13.96s/it]                                                        {'loss': 0.0819, 'grad_norm': 0.1845703125, 'learning_rate': 0.00031084070796460176, 'epoch': 1.96}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1810/4620 [6:55:02<10:53:56, 13.96s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1811/4620 [6:55:16<10:46:51, 13.82s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1812/4620 [6:55:28<10:21:22, 13.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1813/4620 [6:55:40<10:07:29, 12.99s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1814/4620 [6:55:52<9:58:11, 12.79s/it]  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1815/4620 [6:56:05<9:56:31, 12.76s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1816/4620 [6:56:18<9:54:31, 12.72s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1817/4620 [6:56:31<10:08:24, 13.02s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1818/4620 [6:56:44<10:08:01, 13.02s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1819/4620 [6:56:57<9:57:06, 12.79s/it]  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1820/4620 [6:57:10<10:10:04, 13.07s/it]                                                        {'loss': 0.0774, 'grad_norm': 0.283203125, 'learning_rate': 0.00030973451327433627, 'epoch': 1.97}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1820/4620 [6:57:10<10:10:04, 13.07s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1821/4620 [6:57:24<10:11:55, 13.12s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1822/4620 [6:57:35<9:52:35, 12.71s/it]  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1823/4620 [6:57:48<9:47:57, 12.61s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1824/4620 [6:58:00<9:47:29, 12.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1825/4620 [6:58:12<9:40:00, 12.45s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1826/4620 [6:58:26<10:00:38, 12.90s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1827/4620 [6:58:38<9:50:15, 12.68s/it]  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1828/4620 [6:58:52<9:57:56, 12.85s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1829/4620 [6:59:04<9:53:03, 12.75s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1830/4620 [6:59:18<10:13:01, 13.18s/it]                                                        {'loss': 0.0809, 'grad_norm': 0.353515625, 'learning_rate': 0.0003086283185840708, 'epoch': 1.98}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1830/4620 [6:59:18<10:13:01, 13.18s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1831/4620 [6:59:31<10:09:21, 13.11s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1832/4620 [6:59:41<9:21:07, 12.08s/it]  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1833/4620 [6:59:50<8:33:39, 11.06s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1834/4620 [6:59:58<7:59:44, 10.33s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1835/4620 [7:00:07<7:41:55,  9.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1836/4620 [7:00:16<7:23:08,  9.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1837/4620 [7:00:25<7:11:40,  9.31s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1838/4620 [7:00:34<7:07:59,  9.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1839/4620 [7:00:43<7:02:05,  9.11s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1840/4620 [7:00:52<7:03:59,  9.15s/it]                                                       {'loss': 0.0804, 'grad_norm': 0.61328125, 'learning_rate': 0.0003075221238938053, 'epoch': 1.99}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1840/4620 [7:00:52<7:03:59,  9.15s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1841/4620 [7:01:01<7:04:39,  9.17s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1842/4620 [7:01:10<6:57:37,  9.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1843/4620 [7:01:18<6:52:31,  8.91s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1844/4620 [7:01:27<6:49:03,  8.84s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1845/4620 [7:01:36<6:46:18,  8.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1846/4620 [7:01:45<6:51:14,  8.89s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1847/4620 [7:01:54<6:50:10,  8.88s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1848/4620 [7:02:05<7:19:04,  9.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1849/4620 [7:04:07<33:16:39, 43.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1850/4620 [7:04:46<32:25:53, 42.15s/it]                                                        {'loss': 0.0691, 'grad_norm': 0.2412109375, 'learning_rate': 0.00030641592920353984, 'epoch': 2.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1850/4620 [7:04:46<32:25:53, 42.15s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1851/4620 [7:05:09<27:58:05, 36.36s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1852/4620 [7:05:29<24:06:33, 31.36s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1853/4620 [7:05:42<19:59:16, 26.01s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1854/4620 [7:05:56<17:05:01, 22.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1855/4620 [7:06:11<15:26:57, 20.11s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1856/4620 [7:06:23<13:35:37, 17.71s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1857/4620 [7:06:37<12:41:03, 16.53s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1858/4620 [7:06:49<11:44:31, 15.30s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1859/4620 [7:07:01<10:59:08, 14.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1860/4620 [7:07:16<11:02:13, 14.40s/it]                                                        {'loss': 0.0674, 'grad_norm': 0.318359375, 'learning_rate': 0.00030530973451327434, 'epoch': 2.01}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1860/4620 [7:07:16<11:02:13, 14.40s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1861/4620 [7:07:29<10:51:09, 14.16s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1862/4620 [7:07:42<10:24:34, 13.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1863/4620 [7:07:55<10:14:39, 13.38s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1864/4620 [7:08:08<10:10:30, 13.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1865/4620 [7:08:22<10:19:23, 13.49s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1866/4620 [7:08:34<10:03:49, 13.16s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1867/4620 [7:08:48<10:11:16, 13.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1868/4620 [7:09:00<9:57:19, 13.02s/it]  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1869/4620 [7:09:13<9:50:20, 12.88s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1870/4620 [7:09:26<10:03:19, 13.16s/it]                                                        {'loss': 0.0668, 'grad_norm': 0.0869140625, 'learning_rate': 0.00030420353982300885, 'epoch': 2.02}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1870/4620 [7:09:27<10:03:19, 13.16s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1871/4620 [7:09:40<10:07:27, 13.26s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1872/4620 [7:09:52<9:50:05, 12.88s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1873/4620 [7:10:04<9:41:34, 12.70s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1874/4620 [7:10:19<10:03:50, 13.19s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1875/4620 [7:10:31<9:49:19, 12.88s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1876/4620 [7:10:43<9:42:49, 12.74s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1877/4620 [7:10:57<10:05:10, 13.24s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1878/4620 [7:11:09<9:46:40, 12.84s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1879/4620 [7:11:22<9:39:29, 12.69s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1880/4620 [7:11:36<9:55:53, 13.05s/it]                                                       {'loss': 0.0774, 'grad_norm': 0.1796875, 'learning_rate': 0.0003030973451327434, 'epoch': 2.03}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1880/4620 [7:11:36<9:55:53, 13.05s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1881/4620 [7:11:49<10:01:55, 13.19s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1882/4620 [7:12:01<9:44:43, 12.81s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1883/4620 [7:12:13<9:37:12, 12.65s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1884/4620 [7:12:26<9:37:51, 12.67s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1885/4620 [7:12:40<9:56:32, 13.09s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1886/4620 [7:12:53<10:00:17, 13.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1887/4620 [7:13:05<9:42:37, 12.79s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1888/4620 [7:13:17<9:31:21, 12.55s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1889/4620 [7:13:30<9:29:37, 12.51s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1890/4620 [7:13:44<9:48:14, 12.93s/it]                                                       {'loss': 0.0888, 'grad_norm': 0.1787109375, 'learning_rate': 0.00030199115044247785, 'epoch': 2.05}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1890/4620 [7:13:44<9:48:14, 12.93s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1891/4620 [7:13:57<9:58:39, 13.16s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1892/4620 [7:14:09<9:38:40, 12.73s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1893/4620 [7:14:21<9:27:03, 12.48s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1894/4620 [7:14:34<9:27:37, 12.49s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1895/4620 [7:14:47<9:46:16, 12.91s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1896/4620 [7:15:00<9:46:50, 12.93s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1897/4620 [7:15:12<9:35:00, 12.67s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1898/4620 [7:15:25<9:31:29, 12.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1899/4620 [7:15:37<9:29:49, 12.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1900/4620 [7:15:52<9:51:14, 13.04s/it]                                                       {'loss': 0.0667, 'grad_norm': 0.287109375, 'learning_rate': 0.0003008849557522124, 'epoch': 2.06}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1900/4620 [7:15:52<9:51:14, 13.04s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:54,  3.86s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:38,  3.00s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:29,  2.45s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.14s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  1.94s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.82s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.82s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.77s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.67s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.74s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.90s/it][A                                                       
                                               [A{'eval_loss': 0.05549822375178337, 'eval_runtime': 66.4086, 'eval_samples_per_second': 7.529, 'eval_steps_per_second': 0.241, 'epoch': 2.06}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1900/4620 [7:16:58<9:51:14, 13.04s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.90s/it][A
                                               [A 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1901/4620 [7:17:08<24:16:16, 32.14s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1902/4620 [7:17:21<19:58:04, 26.45s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1903/4620 [7:17:33<16:37:29, 22.03s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1904/4620 [7:17:45<14:25:14, 19.11s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1905/4620 [7:17:59<13:14:56, 17.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1906/4620 [7:18:11<11:58:46, 15.89s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1907/4620 [7:18:24<11:16:35, 14.96s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1908/4620 [7:18:37<10:48:41, 14.35s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1909/4620 [7:18:51<10:43:05, 14.23s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1910/4620 [7:19:03<10:10:59, 13.53s/it]                                                        {'loss': 0.0706, 'grad_norm': 0.2734375, 'learning_rate': 0.0002997787610619469, 'epoch': 2.07}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1910/4620 [7:19:03<10:10:59, 13.53s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1911/4620 [7:19:15<9:56:38, 13.21s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1912/4620 [7:19:30<10:14:44, 13.62s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1913/4620 [7:19:42<9:51:41, 13.11s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1914/4620 [7:19:54<9:41:10, 12.89s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1915/4620 [7:20:09<10:03:10, 13.38s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1916/4620 [7:20:21<9:41:12, 12.90s/it]  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1917/4620 [7:20:33<9:34:15, 12.75s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1918/4620 [7:20:46<9:31:47, 12.70s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1919/4620 [7:21:00<9:52:26, 13.16s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1920/4620 [7:21:12<9:37:03, 12.82s/it]                                                       {'loss': 0.0793, 'grad_norm': 0.359375, 'learning_rate': 0.0002986725663716814, 'epoch': 2.08}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1920/4620 [7:21:12<9:37:03, 12.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1921/4620 [7:21:24<9:30:07, 12.67s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1922/4620 [7:21:39<9:52:43, 13.18s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1923/4620 [7:21:51<9:36:59, 12.84s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1924/4620 [7:22:05<9:54:51, 13.24s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1925/4620 [7:22:17<9:38:26, 12.88s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1926/4620 [7:22:29<9:31:17, 12.72s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1927/4620 [7:22:41<9:22:10, 12.53s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1928/4620 [7:22:55<9:39:46, 12.92s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1929/4620 [7:23:07<9:31:02, 12.73s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1930/4620 [7:23:20<9:25:16, 12.61s/it]                                                       {'loss': 0.0645, 'grad_norm': 0.2451171875, 'learning_rate': 0.00029756637168141593, 'epoch': 2.09}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1930/4620 [7:23:20<9:25:16, 12.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1931/4620 [7:23:32<9:22:27, 12.55s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1932/4620 [7:23:46<9:37:45, 12.90s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1933/4620 [7:23:59<9:41:37, 12.99s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1934/4620 [7:24:12<9:39:51, 12.95s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1935/4620 [7:24:24<9:30:11, 12.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1936/4620 [7:24:37<9:28:40, 12.71s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1937/4620 [7:24:50<9:31:23, 12.78s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1938/4620 [7:25:02<9:25:56, 12.66s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1939/4620 [7:25:16<9:49:05, 13.18s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1940/4620 [7:25:29<9:34:28, 12.86s/it]                                                       {'loss': 0.0722, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002964601769911505, 'epoch': 2.1}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1940/4620 [7:25:29<9:34:28, 12.86s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1941/4620 [7:25:41<9:27:46, 12.72s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1942/4620 [7:25:55<9:47:30, 13.16s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1943/4620 [7:26:08<9:49:38, 13.22s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1944/4620 [7:26:22<9:50:22, 13.24s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1945/4620 [7:26:34<9:35:27, 12.91s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1946/4620 [7:26:47<9:31:17, 12.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1947/4620 [7:26:59<9:25:04, 12.68s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1948/4620 [7:27:11<9:22:15, 12.63s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1949/4620 [7:27:26<9:50:27, 13.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1950/4620 [7:27:39<9:39:13, 13.02s/it]                                                       {'loss': 0.0758, 'grad_norm': 0.34375, 'learning_rate': 0.00029535398230088494, 'epoch': 2.11}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1950/4620 [7:27:39<9:39:13, 13.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1951/4620 [7:27:51<9:27:20, 12.75s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1952/4620 [7:28:06<10:05:49, 13.62s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1953/4620 [7:28:18<9:35:17, 12.94s/it]  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1954/4620 [7:28:31<9:38:22, 13.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1955/4620 [7:28:43<9:26:28, 12.75s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1956/4620 [7:28:55<9:20:10, 12.62s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1957/4620 [7:29:08<9:17:37, 12.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1958/4620 [7:29:20<9:12:24, 12.45s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1959/4620 [7:29:34<9:33:24, 12.93s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1960/4620 [7:29:46<9:19:00, 12.61s/it]                                                       {'loss': 0.0752, 'grad_norm': 0.2578125, 'learning_rate': 0.0002942477876106195, 'epoch': 2.12}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1960/4620 [7:29:46<9:19:00, 12.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1961/4620 [7:29:59<9:20:02, 12.64s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1962/4620 [7:30:14<9:57:17, 13.48s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1963/4620 [7:30:25<9:29:51, 12.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1964/4620 [7:30:39<9:37:03, 13.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1965/4620 [7:30:51<9:20:32, 12.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1966/4620 [7:31:03<9:14:29, 12.54s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1967/4620 [7:31:16<9:16:44, 12.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1968/4620 [7:31:28<9:19:12, 12.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1969/4620 [7:31:42<9:36:57, 13.06s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1970/4620 [7:31:54<9:19:11, 12.66s/it]                                                       {'loss': 0.0689, 'grad_norm': 0.1611328125, 'learning_rate': 0.00029314159292035395, 'epoch': 2.13}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1970/4620 [7:31:54<9:19:11, 12.66s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1971/4620 [7:32:08<9:27:37, 12.86s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1972/4620 [7:32:20<9:17:23, 12.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1973/4620 [7:32:32<9:10:56, 12.49s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1974/4620 [7:32:44<9:13:56, 12.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1975/4620 [7:32:58<9:28:52, 12.90s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1976/4620 [7:33:10<9:13:17, 12.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1977/4620 [7:33:22<9:09:13, 12.47s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1978/4620 [7:33:35<9:14:41, 12.60s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1979/4620 [7:33:50<9:43:20, 13.25s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1980/4620 [7:34:02<9:23:54, 12.82s/it]                                                       {'loss': 0.0698, 'grad_norm': 0.111328125, 'learning_rate': 0.0002920353982300885, 'epoch': 2.14}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1980/4620 [7:34:02<9:23:54, 12.82s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1981/4620 [7:34:16<9:38:47, 13.16s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1982/4620 [7:34:27<9:19:07, 12.72s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1983/4620 [7:34:40<9:16:06, 12.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1984/4620 [7:34:52<9:12:54, 12.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1985/4620 [7:35:06<9:31:08, 13.01s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1986/4620 [7:35:19<9:21:56, 12.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1987/4620 [7:35:31<9:12:22, 12.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1988/4620 [7:35:43<9:08:38, 12.51s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1989/4620 [7:35:58<9:36:01, 13.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1990/4620 [7:36:11<9:36:36, 13.15s/it]                                                       {'loss': 0.0664, 'grad_norm': 0.2021484375, 'learning_rate': 0.000290929203539823, 'epoch': 2.15}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1990/4620 [7:36:11<9:36:36, 13.15s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1991/4620 [7:36:24<9:34:41, 13.12s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1992/4620 [7:36:36<9:23:30, 12.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1993/4620 [7:36:49<9:18:19, 12.75s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1994/4620 [7:37:01<9:14:35, 12.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1995/4620 [7:37:15<9:37:06, 13.19s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1996/4620 [7:37:28<9:24:07, 12.90s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1997/4620 [7:37:40<9:15:48, 12.71s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1998/4620 [7:37:52<9:12:09, 12.64s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1999/4620 [7:38:06<9:28:22, 13.01s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2000/4620 [7:38:20<9:31:34, 13.09s/it]                                                       {'loss': 0.0757, 'grad_norm': 0.2197265625, 'learning_rate': 0.0002898230088495575, 'epoch': 2.16}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2000/4620 [7:38:20<9:31:34, 13.09s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:46,  3.35s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:35,  2.73s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.37s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:22,  2.08s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.93s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.81s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:16<00:13,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.69s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.64s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:24<00:04,  1.61s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.60s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.84s/it][A                                                       
                                               [A{'eval_loss': 0.0537186898291111, 'eval_runtime': 65.9525, 'eval_samples_per_second': 7.581, 'eval_steps_per_second': 0.243, 'epoch': 2.16}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2000/4620 [7:39:26<9:31:34, 13.09s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.84s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2001/4620 [7:39:38<23:52:33, 32.82s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2002/4620 [7:39:50<19:11:37, 26.39s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2003/4620 [7:40:02<16:04:23, 22.11s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2004/4620 [7:40:16<14:17:45, 19.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2005/4620 [7:40:28<12:39:01, 17.42s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2006/4620 [7:40:40<11:30:10, 15.84s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2007/4620 [7:40:53<10:44:49, 14.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2008/4620 [7:41:07<10:38:08, 14.66s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2009/4620 [7:41:19<10:00:21, 13.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2010/4620 [7:41:32<9:54:01, 13.66s/it]                                                        {'loss': 0.0638, 'grad_norm': 0.181640625, 'learning_rate': 0.000288716814159292, 'epoch': 2.18}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2010/4620 [7:41:32<9:54:01, 13.66s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2011/4620 [7:41:46<9:53:19, 13.65s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2012/4620 [7:41:57<9:26:28, 13.03s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2013/4620 [7:42:10<9:17:55, 12.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2014/4620 [7:42:24<9:32:24, 13.18s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2015/4620 [7:42:36<9:22:52, 12.96s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2016/4620 [7:42:49<9:18:19, 12.86s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2017/4620 [7:43:01<9:11:42, 12.72s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2018/4620 [7:43:15<9:32:08, 13.19s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2019/4620 [7:43:28<9:21:50, 12.96s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2020/4620 [7:43:42<9:33:49, 13.24s/it]                                                       {'loss': 0.0543, 'grad_norm': 0.158203125, 'learning_rate': 0.0002876106194690266, 'epoch': 2.19}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2020/4620 [7:43:42<9:33:49, 13.24s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2021/4620 [7:43:55<9:38:07, 13.35s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2022/4620 [7:44:07<9:20:38, 12.95s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2023/4620 [7:44:19<9:09:12, 12.69s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2024/4620 [7:44:33<9:20:39, 12.96s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2025/4620 [7:44:45<9:08:02, 12.67s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2026/4620 [7:44:57<9:05:09, 12.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2027/4620 [7:45:10<9:02:22, 12.55s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2028/4620 [7:45:24<9:17:29, 12.90s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2029/4620 [7:45:36<9:09:09, 12.72s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2030/4620 [7:45:50<9:21:33, 13.01s/it]                                                       {'loss': 0.0602, 'grad_norm': 0.27734375, 'learning_rate': 0.00028650442477876103, 'epoch': 2.2}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2030/4620 [7:45:50<9:21:33, 13.01s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2031/4620 [7:46:03<9:21:21, 13.01s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2032/4620 [7:46:15<9:11:06, 12.78s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2033/4620 [7:46:28<9:17:17, 12.93s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2034/4620 [7:46:40<9:04:04, 12.62s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2035/4620 [7:46:53<9:01:54, 12.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2036/4620 [7:47:05<9:06:06, 12.68s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2037/4620 [7:47:18<9:01:29, 12.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2038/4620 [7:47:32<9:19:30, 13.00s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2039/4620 [7:47:44<9:04:36, 12.66s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2040/4620 [7:47:57<9:17:30, 12.97s/it]                                                       {'loss': 0.06, 'grad_norm': 0.208984375, 'learning_rate': 0.0002853982300884956, 'epoch': 2.21}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2040/4620 [7:47:57<9:17:30, 12.97s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2041/4620 [7:48:10<9:19:47, 13.02s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2042/4620 [7:48:22<9:01:54, 12.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2043/4620 [7:48:36<9:18:48, 13.01s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2044/4620 [7:48:48<9:06:45, 12.74s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2045/4620 [7:49:00<8:59:33, 12.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2046/4620 [7:49:12<8:53:54, 12.45s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2047/4620 [7:49:25<8:53:47, 12.45s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2048/4620 [7:49:39<9:17:50, 13.01s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2049/4620 [7:49:51<9:02:37, 12.66s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2050/4620 [7:50:05<9:15:39, 12.97s/it]                                                       {'loss': 0.0692, 'grad_norm': 0.166015625, 'learning_rate': 0.00028429203539823015, 'epoch': 2.22}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2050/4620 [7:50:05<9:15:39, 12.97s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2051/4620 [7:50:18<9:19:56, 13.08s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2052/4620 [7:50:31<9:20:05, 13.09s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2053/4620 [7:50:43<9:08:37, 12.82s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2054/4620 [7:50:56<8:58:58, 12.60s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2055/4620 [7:51:08<8:57:28, 12.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2056/4620 [7:51:20<8:54:46, 12.51s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2057/4620 [7:51:34<9:13:10, 12.95s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2058/4620 [7:51:47<9:06:47, 12.81s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2059/4620 [7:51:59<9:03:43, 12.74s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2060/4620 [7:52:13<9:13:51, 12.98s/it]                                                       {'loss': 0.0779, 'grad_norm': 0.1103515625, 'learning_rate': 0.0002831858407079646, 'epoch': 2.23}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2060/4620 [7:52:13<9:13:51, 12.98s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2061/4620 [7:52:27<9:23:16, 13.21s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2062/4620 [7:52:40<9:24:36, 13.24s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2063/4620 [7:52:52<9:09:50, 12.90s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2064/4620 [7:53:05<9:06:31, 12.83s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2065/4620 [7:53:17<8:58:45, 12.65s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2066/4620 [7:53:29<8:55:35, 12.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2067/4620 [7:53:43<9:11:45, 12.97s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2068/4620 [7:53:55<8:57:56, 12.65s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2069/4620 [7:54:08<8:54:53, 12.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2070/4620 [7:54:21<9:06:40, 12.86s/it]                                                       {'loss': 0.0672, 'grad_norm': 0.1650390625, 'learning_rate': 0.00028207964601769915, 'epoch': 2.24}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2070/4620 [7:54:21<9:06:40, 12.86s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2071/4620 [7:54:34<9:11:17, 12.98s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2072/4620 [7:54:48<9:15:29, 13.08s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2073/4620 [7:55:00<9:01:09, 12.75s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2074/4620 [7:55:12<8:50:39, 12.51s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2075/4620 [7:55:24<8:51:05, 12.52s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2076/4620 [7:55:37<8:56:00, 12.64s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2077/4620 [7:55:51<9:14:08, 13.07s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2078/4620 [7:56:03<8:57:54, 12.70s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2079/4620 [7:56:15<8:54:18, 12.62s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2080/4620 [7:56:29<9:08:32, 12.96s/it]                                                       {'loss': 0.0569, 'grad_norm': 0.267578125, 'learning_rate': 0.0002809734513274336, 'epoch': 2.25}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2080/4620 [7:56:29<9:08:32, 12.96s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2081/4620 [7:56:41<8:54:12, 12.62s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2082/4620 [7:56:55<9:06:57, 12.93s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2083/4620 [7:57:07<8:56:31, 12.69s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2084/4620 [7:57:19<8:51:11, 12.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2085/4620 [7:57:32<8:49:53, 12.54s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2086/4620 [7:57:46<9:11:46, 13.06s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2087/4620 [7:57:58<9:02:09, 12.84s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2088/4620 [7:58:11<8:55:39, 12.69s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2089/4620 [7:58:23<8:56:05, 12.71s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2090/4620 [7:58:37<9:14:13, 13.14s/it]                                                       {'loss': 0.0576, 'grad_norm': 0.123046875, 'learning_rate': 0.00027986725663716816, 'epoch': 2.26}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2090/4620 [7:58:38<9:14:13, 13.14s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2091/4620 [7:58:49<8:57:29, 12.75s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2092/4620 [7:59:03<9:11:16, 13.08s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2093/4620 [7:59:15<8:57:13, 12.76s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2094/4620 [7:59:28<8:53:18, 12.67s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2095/4620 [7:59:42<9:11:24, 13.10s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2096/4620 [7:59:54<8:56:59, 12.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2097/4620 [8:00:06<8:52:52, 12.67s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2098/4620 [8:00:18<8:47:27, 12.55s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2099/4620 [8:00:32<9:04:48, 12.97s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2100/4620 [8:00:44<8:51:45, 12.66s/it]                                                       {'loss': 0.0722, 'grad_norm': 0.328125, 'learning_rate': 0.00027876106194690267, 'epoch': 2.27}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2100/4620 [8:00:44<8:51:45, 12.66s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:04<00:30,  2.14s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:05<00:25,  1.94s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:07<00:21,  1.77s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:09<00:19,  1.79s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:17,  1.78s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:12<00:15,  1.71s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:14<00:13,  1.68s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:15<00:11,  1.63s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:17<00:09,  1.61s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:18<00:08,  1.61s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:20<00:06,  1.70s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:22<00:04,  1.65s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:23<00:03,  1.64s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:25<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:27<00:00,  1.87s/it][A                                                       
                                               [A{'eval_loss': 0.05796298012137413, 'eval_runtime': 67.8912, 'eval_samples_per_second': 7.365, 'eval_steps_per_second': 0.236, 'epoch': 2.27}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2100/4620 [8:01:52<8:51:45, 12.66s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:27<00:00,  1.87s/it][A
                                               [A 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2101/4620 [8:02:04<22:58:50, 32.84s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2102/4620 [8:02:16<18:27:17, 26.39s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2103/4620 [8:02:28<15:27:45, 22.12s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2104/4620 [8:02:40<13:27:18, 19.25s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2105/4620 [8:02:55<12:30:43, 17.91s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2106/4620 [8:03:07<11:12:43, 16.06s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2107/4620 [8:03:19<10:27:21, 14.98s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2108/4620 [8:03:32<9:54:24, 14.20s/it]  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2109/4620 [8:03:46<9:52:53, 14.17s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2110/4620 [8:03:59<9:41:58, 13.91s/it]                                                       {'loss': 0.0658, 'grad_norm': 0.1630859375, 'learning_rate': 0.0002776548672566372, 'epoch': 2.28}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2110/4620 [8:03:59<9:41:58, 13.91s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2111/4620 [8:04:12<9:29:27, 13.62s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2112/4620 [8:04:24<9:11:28, 13.19s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2113/4620 [8:04:37<9:03:48, 13.01s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2114/4620 [8:04:51<9:14:14, 13.27s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2115/4620 [8:05:03<8:57:06, 12.86s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2116/4620 [8:05:15<8:52:47, 12.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2117/4620 [8:05:28<8:51:33, 12.74s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2118/4620 [8:05:40<8:49:28, 12.70s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2119/4620 [8:05:53<8:47:14, 12.65s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2120/4620 [8:06:07<9:07:15, 13.13s/it]                                                       {'loss': 0.0661, 'grad_norm': 0.232421875, 'learning_rate': 0.0002765486725663717, 'epoch': 2.29}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2120/4620 [8:06:07<9:07:15, 13.13s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2121/4620 [8:06:20<9:06:02, 13.11s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2122/4620 [8:06:32<8:55:17, 12.86s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2123/4620 [8:06:45<8:50:31, 12.75s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2124/4620 [8:06:59<9:05:20, 13.11s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2125/4620 [8:07:11<8:54:18, 12.85s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2126/4620 [8:07:23<8:46:57, 12.68s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2127/4620 [8:07:36<8:48:00, 12.71s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2128/4620 [8:07:49<8:42:50, 12.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2129/4620 [8:08:01<8:39:56, 12.52s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2130/4620 [8:08:15<8:53:32, 12.86s/it]                                                       {'loss': 0.0693, 'grad_norm': 0.1943359375, 'learning_rate': 0.00027544247787610624, 'epoch': 2.31}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2130/4620 [8:08:15<8:53:32, 12.86s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2131/4620 [8:08:28<8:57:49, 12.96s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2132/4620 [8:08:40<8:50:42, 12.80s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2133/4620 [8:08:54<9:04:41, 13.14s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2134/4620 [8:09:06<8:48:23, 12.75s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2135/4620 [8:09:18<8:40:59, 12.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2136/4620 [8:09:31<8:38:38, 12.53s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2137/4620 [8:09:43<8:38:17, 12.52s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2138/4620 [8:09:56<8:42:41, 12.64s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2139/4620 [8:10:09<8:46:35, 12.74s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2140/4620 [8:10:23<9:03:40, 13.15s/it]                                                       {'loss': 0.074, 'grad_norm': 0.2353515625, 'learning_rate': 0.0002743362831858407, 'epoch': 2.32}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2140/4620 [8:10:23<9:03:40, 13.15s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2141/4620 [8:10:37<9:10:37, 13.33s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2142/4620 [8:10:49<8:55:38, 12.97s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2143/4620 [8:11:03<9:04:14, 13.18s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2144/4620 [8:11:15<8:48:55, 12.82s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2145/4620 [8:11:27<8:39:38, 12.60s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2146/4620 [8:11:39<8:33:56, 12.46s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2147/4620 [8:11:51<8:33:39, 12.46s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2148/4620 [8:12:04<8:38:00, 12.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2149/4620 [8:12:17<8:39:27, 12.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2150/4620 [8:12:32<9:13:12, 13.44s/it]                                                       {'loss': 0.0749, 'grad_norm': 0.62890625, 'learning_rate': 0.00027323008849557525, 'epoch': 2.33}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2150/4620 [8:12:32<9:13:12, 13.44s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2151/4620 [8:12:44<8:50:15, 12.89s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2152/4620 [8:12:57<8:58:28, 13.09s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2153/4620 [8:13:09<8:42:44, 12.71s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2154/4620 [8:13:21<8:35:27, 12.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2155/4620 [8:13:34<8:36:23, 12.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2156/4620 [8:13:47<8:39:40, 12.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2157/4620 [8:14:00<8:40:50, 12.69s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2158/4620 [8:14:12<8:39:14, 12.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2159/4620 [8:14:25<8:40:32, 12.69s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2160/4620 [8:14:41<9:19:23, 13.64s/it]                                                       {'loss': 0.0657, 'grad_norm': 0.2001953125, 'learning_rate': 0.0002721238938053097, 'epoch': 2.34}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2160/4620 [8:14:41<9:19:23, 13.64s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2161/4620 [8:14:52<8:53:05, 13.01s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2162/4620 [8:15:06<9:00:50, 13.20s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2163/4620 [8:15:18<8:42:24, 12.76s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2164/4620 [8:15:30<8:37:59, 12.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2165/4620 [8:15:43<8:37:42, 12.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2166/4620 [8:15:55<8:37:09, 12.64s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2167/4620 [8:16:08<8:38:02, 12.67s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2168/4620 [8:16:20<8:34:24, 12.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2169/4620 [8:16:33<8:35:53, 12.63s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2170/4620 [8:16:49<9:15:43, 13.61s/it]                                                       {'loss': 0.0679, 'grad_norm': 0.28515625, 'learning_rate': 0.00027101769911504425, 'epoch': 2.35}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2170/4620 [8:16:49<9:15:43, 13.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2171/4620 [8:17:02<9:12:27, 13.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2172/4620 [8:17:14<8:49:47, 12.99s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2173/4620 [8:17:26<8:41:01, 12.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2174/4620 [8:17:39<8:41:04, 12.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2175/4620 [8:17:52<8:34:15, 12.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2176/4620 [8:18:04<8:34:00, 12.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2177/4620 [8:18:17<8:30:58, 12.55s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2178/4620 [8:18:29<8:28:30, 12.49s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2179/4620 [8:18:41<8:27:47, 12.48s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2180/4620 [8:18:55<8:47:13, 12.96s/it]                                                       {'loss': 0.0604, 'grad_norm': 0.279296875, 'learning_rate': 0.00026991150442477876, 'epoch': 2.36}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2180/4620 [8:18:55<8:47:13, 12.96s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2181/4620 [8:19:09<8:52:57, 13.11s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2182/4620 [8:19:21<8:35:21, 12.68s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2183/4620 [8:19:33<8:29:37, 12.55s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2184/4620 [8:19:46<8:33:12, 12.64s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2185/4620 [8:19:58<8:30:12, 12.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2186/4620 [8:20:11<8:28:52, 12.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2187/4620 [8:20:23<8:28:17, 12.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2188/4620 [8:20:36<8:29:51, 12.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2189/4620 [8:20:50<8:44:58, 12.96s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2190/4620 [8:21:03<8:50:56, 13.11s/it]                                                       {'loss': 0.0643, 'grad_norm': 0.2275390625, 'learning_rate': 0.00026880530973451326, 'epoch': 2.37}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2190/4620 [8:21:03<8:50:56, 13.11s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2191/4620 [8:21:15<8:39:42, 12.84s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2192/4620 [8:21:27<8:24:32, 12.47s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2193/4620 [8:21:39<8:23:26, 12.45s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2194/4620 [8:21:52<8:22:32, 12.43s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2195/4620 [8:22:04<8:25:02, 12.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2196/4620 [8:22:17<8:27:26, 12.56s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2197/4620 [8:22:29<8:25:25, 12.52s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2198/4620 [8:22:42<8:24:16, 12.49s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2199/4620 [8:22:56<8:42:30, 12.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2200/4620 [8:23:09<8:42:56, 12.97s/it]                                                       {'loss': 0.058, 'grad_norm': 0.32421875, 'learning_rate': 0.0002676991150442478, 'epoch': 2.38}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2200/4620 [8:23:09<8:42:56, 12.97s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:46,  3.34s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:35,  2.74s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.35s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:23,  2.09s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.99s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:17,  1.93s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.83s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.68s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.63s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.68s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:05,  1.71s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.68s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.85s/it][A                                                       
                                               [A{'eval_loss': 0.05750085040926933, 'eval_runtime': 71.7353, 'eval_samples_per_second': 6.97, 'eval_steps_per_second': 0.223, 'epoch': 2.38}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2200/4620 [8:24:21<8:42:56, 12.97s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.85s/it][A
                                               [A 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2201/4620 [8:24:31<22:39:29, 33.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2202/4620 [8:24:43<18:14:04, 27.15s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2203/4620 [8:24:55<15:17:27, 22.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2204/4620 [8:25:08<13:16:08, 19.77s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2205/4620 [8:25:21<11:50:58, 17.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2206/4620 [8:25:34<10:53:40, 16.25s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2207/4620 [8:25:47<10:14:13, 15.27s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2208/4620 [8:26:00<9:46:12, 14.58s/it]  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2209/4620 [8:26:15<9:48:10, 14.64s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2210/4620 [8:26:27<9:17:23, 13.88s/it]                                                       {'loss': 0.0599, 'grad_norm': 0.26171875, 'learning_rate': 0.00026659292035398233, 'epoch': 2.39}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2210/4620 [8:26:27<9:17:23, 13.88s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2211/4620 [8:26:39<9:00:07, 13.45s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2212/4620 [8:26:52<8:47:05, 13.13s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2213/4620 [8:27:04<8:43:25, 13.05s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2214/4620 [8:27:17<8:38:01, 12.92s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2215/4620 [8:27:30<8:35:20, 12.86s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2216/4620 [8:27:42<8:33:27, 12.82s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2217/4620 [8:27:55<8:32:39, 12.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2218/4620 [8:28:08<8:34:06, 12.84s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2219/4620 [8:28:22<8:49:24, 13.23s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2220/4620 [8:28:36<8:55:51, 13.40s/it]                                                       {'loss': 0.0699, 'grad_norm': 0.228515625, 'learning_rate': 0.00026548672566371683, 'epoch': 2.4}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2220/4620 [8:28:36<8:55:51, 13.40s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2221/4620 [8:28:48<8:37:40, 12.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2222/4620 [8:29:00<8:31:27, 12.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2223/4620 [8:29:13<8:32:16, 12.82s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2224/4620 [8:29:26<8:36:40, 12.94s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2225/4620 [8:29:40<8:38:07, 12.98s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2226/4620 [8:29:52<8:30:55, 12.81s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2227/4620 [8:30:05<8:29:28, 12.77s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2228/4620 [8:30:19<8:48:56, 13.27s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2229/4620 [8:30:33<8:51:37, 13.34s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2230/4620 [8:30:46<8:49:59, 13.31s/it]                                                       {'loss': 0.0786, 'grad_norm': 0.412109375, 'learning_rate': 0.00026438053097345134, 'epoch': 2.41}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2230/4620 [8:30:46<8:49:59, 13.31s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2231/4620 [8:30:58<8:32:43, 12.88s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2232/4620 [8:31:10<8:26:31, 12.73s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2233/4620 [8:31:23<8:23:13, 12.65s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2234/4620 [8:31:35<8:22:54, 12.65s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2235/4620 [8:31:48<8:25:33, 12.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2236/4620 [8:32:01<8:23:25, 12.67s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2237/4620 [8:32:13<8:21:37, 12.63s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2238/4620 [8:32:26<8:19:30, 12.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2239/4620 [8:32:40<8:37:51, 13.05s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2240/4620 [8:32:53<8:38:00, 13.06s/it]                                                       {'loss': 0.0674, 'grad_norm': 0.1787109375, 'learning_rate': 0.00026327433628318584, 'epoch': 2.42}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2240/4620 [8:32:53<8:38:00, 13.06s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2241/4620 [8:33:05<8:25:16, 12.74s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2242/4620 [8:33:18<8:24:31, 12.73s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2243/4620 [8:33:30<8:19:31, 12.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2244/4620 [8:33:42<8:15:15, 12.51s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2245/4620 [8:33:55<8:20:44, 12.65s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2246/4620 [8:34:08<8:22:15, 12.69s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2247/4620 [8:34:21<8:22:17, 12.70s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2248/4620 [8:34:35<8:37:32, 13.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2249/4620 [8:34:48<8:43:49, 13.26s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2250/4620 [8:35:02<8:44:05, 13.27s/it]                                                       {'loss': 0.0656, 'grad_norm': 0.412109375, 'learning_rate': 0.00026216814159292035, 'epoch': 2.44}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2250/4620 [8:35:02<8:44:05, 13.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2251/4620 [8:35:13<8:27:31, 12.85s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2252/4620 [8:35:26<8:20:41, 12.69s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2253/4620 [8:35:39<8:23:23, 12.76s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2254/4620 [8:35:51<8:20:34, 12.69s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2255/4620 [8:36:04<8:20:54, 12.71s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2256/4620 [8:36:17<8:24:27, 12.80s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2257/4620 [8:36:30<8:23:11, 12.78s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2258/4620 [8:36:43<8:33:44, 13.05s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2259/4620 [8:36:57<8:37:46, 13.16s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2260/4620 [8:37:10<8:41:04, 13.25s/it]                                                       {'loss': 0.0634, 'grad_norm': 0.1904296875, 'learning_rate': 0.0002610619469026549, 'epoch': 2.45}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2260/4620 [8:37:10<8:41:04, 13.25s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2261/4620 [8:37:22<8:24:18, 12.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2262/4620 [8:37:35<8:20:10, 12.73s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2263/4620 [8:37:48<8:22:35, 12.79s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2264/4620 [8:38:00<8:17:43, 12.68s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2265/4620 [8:38:12<8:13:56, 12.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2266/4620 [8:38:25<8:14:41, 12.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2267/4620 [8:38:39<8:27:24, 12.94s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2268/4620 [8:38:51<8:14:59, 12.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2269/4620 [8:39:05<8:35:06, 13.15s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2270/4620 [8:39:16<8:15:46, 12.66s/it]                                                       {'loss': 0.0734, 'grad_norm': 0.2373046875, 'learning_rate': 0.00025995575221238936, 'epoch': 2.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2270/4620 [8:39:17<8:15:46, 12.66s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2271/4620 [8:39:30<8:27:02, 12.95s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2272/4620 [8:39:42<8:14:15, 12.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2273/4620 [8:39:54<8:11:55, 12.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2274/4620 [8:40:07<8:12:44, 12.60s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2275/4620 [8:40:20<8:14:03, 12.64s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2276/4620 [8:40:32<8:07:41, 12.48s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2277/4620 [8:40:45<8:11:22, 12.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2278/4620 [8:40:59<8:36:01, 13.22s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2279/4620 [8:41:13<8:37:40, 13.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2280/4620 [8:41:25<8:19:36, 12.81s/it]                                                       {'loss': 0.0735, 'grad_norm': 0.294921875, 'learning_rate': 0.0002588495575221239, 'epoch': 2.47}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2280/4620 [8:41:25<8:19:36, 12.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2281/4620 [8:41:38<8:26:19, 12.99s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2282/4620 [8:41:50<8:16:39, 12.75s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2283/4620 [8:42:02<8:10:09, 12.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2284/4620 [8:42:15<8:13:54, 12.69s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2285/4620 [8:42:28<8:12:20, 12.65s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2286/4620 [8:42:40<8:09:40, 12.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2287/4620 [8:42:53<8:10:25, 12.61s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2288/4620 [8:43:07<8:23:37, 12.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2289/4620 [8:43:20<8:25:43, 13.02s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2290/4620 [8:43:33<8:26:45, 13.05s/it]                                                       {'loss': 0.0657, 'grad_norm': 0.291015625, 'learning_rate': 0.0002577433628318584, 'epoch': 2.48}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2290/4620 [8:43:33<8:26:45, 13.05s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2291/4620 [8:43:47<8:32:31, 13.20s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2292/4620 [8:43:59<8:21:17, 12.92s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2293/4620 [8:44:11<8:10:16, 12.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2294/4620 [8:44:24<8:10:27, 12.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2295/4620 [8:44:36<8:09:02, 12.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2296/4620 [8:44:49<8:06:35, 12.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2297/4620 [8:45:01<8:07:03, 12.58s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2298/4620 [8:45:16<8:27:33, 13.12s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2299/4620 [8:45:29<8:34:19, 13.30s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2300/4620 [8:45:43<8:35:50, 13.34s/it]                                                       {'loss': 0.0742, 'grad_norm': 0.181640625, 'learning_rate': 0.0002566371681415929, 'epoch': 2.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2300/4620 [8:45:43<8:35:50, 13.34s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:04<00:30,  2.17s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:30,  2.38s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:25,  2.13s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:22,  2.02s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.89s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:16,  1.79s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.69s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:16<00:11,  1.66s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.63s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.70s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.65s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A                                                       
                                               [A{'eval_loss': 0.06738967448472977, 'eval_runtime': 70.4369, 'eval_samples_per_second': 7.099, 'eval_steps_per_second': 0.227, 'epoch': 2.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2300/4620 [8:46:53<8:35:50, 13.34s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A
                                               [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2301/4620 [8:47:04<21:41:16, 33.67s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2302/4620 [8:47:15<17:26:06, 27.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2303/4620 [8:47:28<14:33:52, 22.63s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2304/4620 [8:47:40<12:38:47, 19.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2305/4620 [8:47:53<11:15:53, 17.52s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2306/4620 [8:48:06<10:20:14, 16.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2307/4620 [8:48:19<9:42:35, 15.11s/it]  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2308/4620 [8:48:33<9:36:27, 14.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2309/4620 [8:48:47<9:22:26, 14.60s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2310/4620 [8:49:02<9:30:06, 14.81s/it]                                                       {'loss': 0.0755, 'grad_norm': 0.37890625, 'learning_rate': 0.00025553097345132743, 'epoch': 2.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2310/4620 [8:49:02<9:30:06, 14.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2311/4620 [8:49:14<8:50:44, 13.79s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2312/4620 [8:49:26<8:33:32, 13.35s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2313/4620 [8:49:38<8:23:25, 13.09s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2314/4620 [8:49:51<8:14:53, 12.88s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2315/4620 [8:50:04<8:15:14, 12.89s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2316/4620 [8:50:17<8:16:33, 12.93s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2317/4620 [8:50:30<8:15:56, 12.92s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2318/4620 [8:50:44<8:29:00, 13.27s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2319/4620 [8:50:57<8:29:10, 13.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2320/4620 [8:51:12<8:49:17, 13.81s/it]                                                       {'loss': 0.0618, 'grad_norm': 0.1162109375, 'learning_rate': 0.00025442477876106193, 'epoch': 2.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2320/4620 [8:51:12<8:49:17, 13.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2321/4620 [8:51:24<8:24:25, 13.16s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2322/4620 [8:51:36<8:14:39, 12.92s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2323/4620 [8:51:49<8:09:28, 12.79s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2324/4620 [8:52:01<8:08:46, 12.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2325/4620 [8:52:14<8:04:45, 12.67s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2326/4620 [8:52:27<8:08:45, 12.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2327/4620 [8:52:41<8:21:29, 13.12s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2328/4620 [8:52:53<8:09:41, 12.82s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2329/4620 [8:53:07<8:21:46, 13.14s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2330/4620 [8:53:20<8:21:05, 13.13s/it]                                                       {'loss': 0.0585, 'grad_norm': 0.369140625, 'learning_rate': 0.00025331858407079644, 'epoch': 2.52}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2330/4620 [8:53:20<8:21:05, 13.13s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2331/4620 [8:53:31<8:04:17, 12.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2332/4620 [8:53:44<8:01:38, 12.63s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2333/4620 [8:53:57<8:05:05, 12.73s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2334/4620 [8:54:10<8:03:56, 12.70s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2335/4620 [8:54:22<8:05:41, 12.75s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2336/4620 [8:54:35<8:06:51, 12.79s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2337/4620 [8:54:49<8:20:19, 13.15s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2338/4620 [8:55:01<8:05:53, 12.78s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2339/4620 [8:55:15<8:17:48, 13.09s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2340/4620 [8:55:29<8:22:13, 13.22s/it]                                                       {'loss': 0.071, 'grad_norm': 0.26171875, 'learning_rate': 0.000252212389380531, 'epoch': 2.53}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2340/4620 [8:55:29<8:22:13, 13.22s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2341/4620 [8:55:40<8:02:08, 12.69s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2342/4620 [8:55:52<7:57:19, 12.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2343/4620 [8:56:05<7:55:57, 12.54s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2344/4620 [8:56:17<7:56:49, 12.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2345/4620 [8:56:30<7:54:53, 12.52s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2346/4620 [8:56:44<8:15:38, 13.08s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2347/4620 [8:56:57<8:07:28, 12.87s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2348/4620 [8:57:09<8:02:34, 12.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2349/4620 [8:57:23<8:18:52, 13.18s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2350/4620 [8:57:37<8:25:30, 13.36s/it]                                                       {'loss': 0.0573, 'grad_norm': 0.2041015625, 'learning_rate': 0.00025110619469026545, 'epoch': 2.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2350/4620 [8:57:37<8:25:30, 13.36s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2351/4620 [8:57:49<8:07:35, 12.89s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2352/4620 [8:58:01<7:59:32, 12.69s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2353/4620 [8:58:14<8:01:12, 12.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2354/4620 [8:58:26<7:58:21, 12.67s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2355/4620 [8:58:39<7:55:22, 12.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2356/4620 [8:58:52<8:03:12, 12.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2357/4620 [8:59:04<7:55:05, 12.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2358/4620 [8:59:18<8:08:48, 12.97s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2359/4620 [8:59:31<8:05:31, 12.88s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2360/4620 [8:59:44<8:13:38, 13.11s/it]                                                       {'loss': 0.0629, 'grad_norm': 0.279296875, 'learning_rate': 0.00025, 'epoch': 2.55}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2360/4620 [8:59:44<8:13:38, 13.11s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2361/4620 [8:59:56<8:01:34, 12.79s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2362/4620 [9:00:09<7:55:08, 12.63s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2363/4620 [9:00:21<7:51:31, 12.54s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2364/4620 [9:00:33<7:50:03, 12.50s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2365/4620 [9:00:47<8:06:47, 12.95s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2366/4620 [9:00:59<7:55:16, 12.65s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2367/4620 [9:01:11<7:46:42, 12.43s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2368/4620 [9:01:27<8:23:48, 13.42s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2369/4620 [9:01:38<8:00:45, 12.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2370/4620 [9:01:52<8:07:58, 13.01s/it]                                                       {'loss': 0.058, 'grad_norm': 0.1689453125, 'learning_rate': 0.0002488938053097345, 'epoch': 2.56}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2370/4620 [9:01:52<8:07:58, 13.01s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2371/4620 [9:02:04<7:57:44, 12.75s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2372/4620 [9:02:16<7:51:32, 12.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2373/4620 [9:02:29<7:53:17, 12.64s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2374/4620 [9:02:41<7:51:12, 12.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2375/4620 [9:02:56<8:09:14, 13.08s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2376/4620 [9:03:08<7:56:17, 12.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2377/4620 [9:03:21<8:05:55, 13.00s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2378/4620 [9:03:35<8:11:32, 13.15s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2379/4620 [9:03:47<7:59:03, 12.83s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2380/4620 [9:04:00<8:04:38, 12.98s/it]                                                       {'loss': 0.0687, 'grad_norm': 0.333984375, 'learning_rate': 0.000247787610619469, 'epoch': 2.58}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2380/4620 [9:04:00<8:04:38, 12.98s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2381/4620 [9:04:12<7:53:21, 12.69s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2382/4620 [9:04:25<7:50:45, 12.62s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2383/4620 [9:04:37<7:52:00, 12.66s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2384/4620 [9:04:51<8:05:58, 13.04s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2385/4620 [9:05:04<7:58:41, 12.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2386/4620 [9:05:16<7:54:09, 12.73s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2387/4620 [9:05:30<8:07:48, 13.11s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2388/4620 [9:05:43<8:09:08, 13.15s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2389/4620 [9:05:55<7:55:59, 12.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2390/4620 [9:06:09<8:07:05, 13.11s/it]                                                       {'loss': 0.0722, 'grad_norm': 0.203125, 'learning_rate': 0.0002466814159292035, 'epoch': 2.59}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2390/4620 [9:06:09<8:07:05, 13.11s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2391/4620 [9:06:21<7:51:43, 12.70s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2392/4620 [9:06:33<7:46:32, 12.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2393/4620 [9:06:46<7:48:10, 12.61s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2394/4620 [9:07:00<8:06:01, 13.10s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2395/4620 [9:07:12<7:51:01, 12.70s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2396/4620 [9:07:26<8:03:46, 13.05s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2397/4620 [9:07:38<7:55:53, 12.84s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2398/4620 [9:07:51<7:59:26, 12.95s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2399/4620 [9:08:04<7:53:54, 12.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2400/4620 [9:08:18<8:05:44, 13.13s/it]                                                       {'loss': 0.0843, 'grad_norm': 0.28125, 'learning_rate': 0.0002455752212389381, 'epoch': 2.6}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2400/4620 [9:08:18<8:05:44, 13.13s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:41,  2.96s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:31,  2.45s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:25,  2.15s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.95s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.81s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.71s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.67s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:11,  1.63s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:10,  1.68s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.69s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.65s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.89s/it][A                                                       
                                               [A{'eval_loss': 0.06381771713495255, 'eval_runtime': 86.3457, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 0.185, 'epoch': 2.6}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2400/4620 [9:09:44<8:05:44, 13.13s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.89s/it][A
                                               [A 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2401/4620 [9:09:55<23:35:34, 38.28s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2402/4620 [9:10:06<18:42:00, 30.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2403/4620 [9:10:20<15:35:47, 25.33s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2404/4620 [9:10:32<13:08:43, 21.36s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2405/4620 [9:10:45<11:36:58, 18.88s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2406/4620 [9:10:59<10:36:45, 17.26s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2407/4620 [9:11:11<9:40:01, 15.73s/it]  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2408/4620 [9:11:25<9:20:00, 15.19s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2409/4620 [9:11:38<9:02:22, 14.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2410/4620 [9:11:50<8:27:59, 13.79s/it]                                                       {'loss': 0.0667, 'grad_norm': 0.302734375, 'learning_rate': 0.0002444690265486726, 'epoch': 2.61}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2410/4620 [9:11:50<8:27:59, 13.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2411/4620 [9:12:02<8:10:44, 13.33s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2412/4620 [9:12:17<8:23:46, 13.69s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2413/4620 [9:12:29<8:11:24, 13.36s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2414/4620 [9:12:42<7:58:40, 13.02s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2415/4620 [9:12:56<8:10:46, 13.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2416/4620 [9:13:08<8:01:32, 13.11s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2417/4620 [9:13:20<7:50:23, 12.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2418/4620 [9:13:34<8:01:47, 13.13s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2419/4620 [9:13:47<8:01:40, 13.13s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2420/4620 [9:14:00<7:51:01, 12.85s/it]                                                       {'loss': 0.0595, 'grad_norm': 0.1435546875, 'learning_rate': 0.0002433628318584071, 'epoch': 2.62}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2420/4620 [9:14:00<7:51:01, 12.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2421/4620 [9:14:12<7:46:22, 12.73s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2422/4620 [9:14:27<8:07:12, 13.30s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2423/4620 [9:14:39<7:55:46, 12.99s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2424/4620 [9:14:51<7:45:18, 12.71s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2425/4620 [9:15:05<7:58:37, 13.08s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2426/4620 [9:15:17<7:45:43, 12.74s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2427/4620 [9:15:29<7:39:10, 12.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2428/4620 [9:15:43<7:50:50, 12.89s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2429/4620 [9:15:56<7:57:53, 13.09s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2430/4620 [9:16:08<7:40:57, 12.63s/it]                                                       {'loss': 0.0623, 'grad_norm': 0.23828125, 'learning_rate': 0.0002422566371681416, 'epoch': 2.63}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2430/4620 [9:16:08<7:40:57, 12.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2431/4620 [9:16:22<7:52:09, 12.94s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2432/4620 [9:16:34<7:43:03, 12.70s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2433/4620 [9:16:46<7:35:31, 12.50s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2434/4620 [9:17:00<7:52:32, 12.97s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2435/4620 [9:17:12<7:39:11, 12.61s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2436/4620 [9:17:24<7:35:10, 12.50s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2437/4620 [9:17:37<7:38:07, 12.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2438/4620 [9:17:51<7:52:39, 13.00s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2439/4620 [9:18:03<7:42:46, 12.73s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2440/4620 [9:18:15<7:40:10, 12.67s/it]                                                       {'loss': 0.0658, 'grad_norm': 0.12255859375, 'learning_rate': 0.0002411504424778761, 'epoch': 2.64}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2440/4620 [9:18:15<7:40:10, 12.67s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2441/4620 [9:18:29<7:56:19, 13.12s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2442/4620 [9:18:41<7:45:35, 12.83s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2443/4620 [9:18:54<7:40:32, 12.69s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2444/4620 [9:19:06<7:39:09, 12.66s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2445/4620 [9:19:20<7:53:15, 13.06s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2446/4620 [9:19:32<7:37:21, 12.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2447/4620 [9:19:46<7:51:54, 13.03s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2448/4620 [9:19:59<7:53:18, 13.07s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2449/4620 [9:20:11<7:40:46, 12.73s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2450/4620 [9:20:25<7:49:58, 12.99s/it]                                                       {'loss': 0.0645, 'grad_norm': 0.255859375, 'learning_rate': 0.00024004424778761063, 'epoch': 2.65}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2450/4620 [9:20:25<7:49:58, 12.99s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2451/4620 [9:20:37<7:42:32, 12.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2452/4620 [9:20:50<7:38:57, 12.70s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2453/4620 [9:21:02<7:35:12, 12.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2454/4620 [9:21:14<7:33:54, 12.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2455/4620 [9:21:29<7:52:16, 13.09s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2456/4620 [9:21:41<7:41:05, 12.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2457/4620 [9:21:55<7:51:06, 13.07s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2458/4620 [9:22:08<7:51:49, 13.09s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2459/4620 [9:22:21<7:50:19, 13.06s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2460/4620 [9:22:33<7:39:06, 12.75s/it]                                                       {'loss': 0.0618, 'grad_norm': 0.396484375, 'learning_rate': 0.00023893805309734516, 'epoch': 2.66}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2460/4620 [9:22:33<7:39:06, 12.75s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2461/4620 [9:22:45<7:37:46, 12.72s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2462/4620 [9:22:58<7:36:05, 12.68s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2463/4620 [9:23:10<7:34:13, 12.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2464/4620 [9:23:23<7:33:30, 12.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2465/4620 [9:23:38<7:53:25, 13.18s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2466/4620 [9:23:51<7:53:53, 13.20s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2467/4620 [9:24:03<7:39:30, 12.81s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2468/4620 [9:24:17<7:51:53, 13.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2469/4620 [9:24:29<7:45:11, 12.98s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2470/4620 [9:24:43<7:50:59, 13.14s/it]                                                       {'loss': 0.0615, 'grad_norm': 0.2060546875, 'learning_rate': 0.00023783185840707966, 'epoch': 2.67}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2470/4620 [9:24:43<7:50:59, 13.14s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2471/4620 [9:24:55<7:40:31, 12.86s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2472/4620 [9:25:07<7:35:02, 12.71s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2473/4620 [9:25:20<7:34:18, 12.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2474/4620 [9:25:32<7:32:14, 12.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2475/4620 [9:25:45<7:33:57, 12.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2476/4620 [9:25:59<7:47:00, 13.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2477/4620 [9:26:11<7:33:41, 12.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2478/4620 [9:26:25<7:43:42, 12.99s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2479/4620 [9:26:37<7:39:09, 12.87s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2480/4620 [9:26:51<7:46:47, 13.09s/it]                                                       {'loss': 0.062, 'grad_norm': 0.2001953125, 'learning_rate': 0.00023672566371681417, 'epoch': 2.68}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2480/4620 [9:26:51<7:46:47, 13.09s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2481/4620 [9:27:03<7:35:19, 12.77s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2482/4620 [9:27:15<7:32:38, 12.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2483/4620 [9:27:28<7:32:15, 12.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2484/4620 [9:27:41<7:33:56, 12.75s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2485/4620 [9:27:55<7:45:09, 13.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2486/4620 [9:28:08<7:45:45, 13.10s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2487/4620 [9:28:21<7:43:43, 13.04s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2488/4620 [9:28:33<7:32:01, 12.72s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2489/4620 [9:28:45<7:29:44, 12.66s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2490/4620 [9:29:00<7:46:49, 13.15s/it]                                                       {'loss': 0.0611, 'grad_norm': 0.267578125, 'learning_rate': 0.00023561946902654867, 'epoch': 2.69}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2490/4620 [9:29:00<7:46:49, 13.15s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2491/4620 [9:29:12<7:34:17, 12.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2492/4620 [9:29:24<7:29:35, 12.68s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2493/4620 [9:29:37<7:28:54, 12.66s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2494/4620 [9:29:49<7:28:53, 12.67s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2495/4620 [9:30:04<7:48:55, 13.24s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2496/4620 [9:30:18<7:53:52, 13.39s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2497/4620 [9:30:31<7:53:43, 13.39s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2498/4620 [9:30:43<7:36:51, 12.92s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2499/4620 [9:30:55<7:32:24, 12.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2500/4620 [9:31:08<7:29:53, 12.73s/it]                                                       {'loss': 0.0732, 'grad_norm': 0.158203125, 'learning_rate': 0.0002345132743362832, 'epoch': 2.71}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2500/4620 [9:31:08<7:29:53, 12.73s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:47,  3.39s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:35,  2.76s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:27,  2.32s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:23,  2.09s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.96s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:17,  1.93s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.82s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:11,  1.70s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.69s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.65s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:05,  1.68s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.70s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.87s/it][A                                                       
                                               [A{'eval_loss': 0.06508247554302216, 'eval_runtime': 82.8267, 'eval_samples_per_second': 6.037, 'eval_steps_per_second': 0.193, 'epoch': 2.71}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2500/4620 [9:32:31<7:29:53, 12.73s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.87s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2501/4620 [9:32:45<22:27:22, 38.15s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2502/4620 [9:32:57<17:41:20, 30.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2503/4620 [9:33:09<14:30:51, 24.68s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2504/4620 [9:33:22<12:33:18, 21.36s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2505/4620 [9:33:35<10:55:39, 18.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2506/4620 [9:33:48<10:02:52, 17.11s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2507/4620 [9:34:01<9:20:36, 15.92s/it]  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2508/4620 [9:34:14<8:41:52, 14.83s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2509/4620 [9:34:26<8:14:31, 14.06s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2510/4620 [9:34:40<8:13:08, 14.02s/it]                                                       {'loss': 0.0646, 'grad_norm': 0.2197265625, 'learning_rate': 0.0002334070796460177, 'epoch': 2.72}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2510/4620 [9:34:40<8:13:08, 14.02s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2511/4620 [9:34:52<7:56:24, 13.55s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2512/4620 [9:35:05<7:45:17, 13.24s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2513/4620 [9:35:18<7:41:11, 13.13s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2514/4620 [9:35:31<7:47:38, 13.32s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2515/4620 [9:35:44<7:36:46, 13.02s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2516/4620 [9:35:59<8:01:17, 13.72s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2517/4620 [9:36:11<7:37:18, 13.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2518/4620 [9:36:23<7:27:36, 12.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2519/4620 [9:36:36<7:30:00, 12.85s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2520/4620 [9:36:49<7:37:01, 13.06s/it]                                                       {'loss': 0.0709, 'grad_norm': 0.10888671875, 'learning_rate': 0.00023230088495575221, 'epoch': 2.73}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2520/4620 [9:36:49<7:37:01, 13.06s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2521/4620 [9:37:01<7:25:06, 12.72s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2522/4620 [9:37:14<7:22:00, 12.64s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2523/4620 [9:37:27<7:25:18, 12.74s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2524/4620 [9:37:41<7:38:06, 13.11s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2525/4620 [9:37:54<7:41:57, 13.23s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2526/4620 [9:38:08<7:43:01, 13.27s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2527/4620 [9:38:19<7:27:48, 12.84s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2528/4620 [9:38:31<7:19:23, 12.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2529/4620 [9:38:45<7:26:19, 12.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2530/4620 [9:38:57<7:17:57, 12.57s/it]                                                       {'loss': 0.0615, 'grad_norm': 0.1435546875, 'learning_rate': 0.00023119469026548672, 'epoch': 2.74}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2530/4620 [9:38:57<7:17:57, 12.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2531/4620 [9:39:09<7:16:07, 12.53s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2532/4620 [9:39:22<7:16:15, 12.54s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2533/4620 [9:39:34<7:18:37, 12.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2534/4620 [9:39:49<7:36:20, 13.13s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2535/4620 [9:40:02<7:35:28, 13.11s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2536/4620 [9:40:15<7:36:11, 13.13s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2537/4620 [9:40:27<7:23:12, 12.77s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2538/4620 [9:40:39<7:16:37, 12.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2539/4620 [9:40:51<7:12:53, 12.48s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2540/4620 [9:41:05<7:28:57, 12.95s/it]                                                       {'loss': 0.0514, 'grad_norm': 0.23046875, 'learning_rate': 0.00023008849557522125, 'epoch': 2.75}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2540/4620 [9:41:05<7:28:57, 12.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2541/4620 [9:41:17<7:19:34, 12.69s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2542/4620 [9:41:30<7:15:44, 12.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2543/4620 [9:41:44<7:30:45, 13.02s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2544/4620 [9:41:57<7:34:45, 13.14s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2545/4620 [9:42:09<7:24:37, 12.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2546/4620 [9:42:23<7:32:51, 13.10s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2547/4620 [9:42:35<7:22:39, 12.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2548/4620 [9:42:48<7:21:03, 12.77s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2549/4620 [9:43:00<7:14:09, 12.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2550/4620 [9:43:14<7:28:47, 13.01s/it]                                                       {'loss': 0.0632, 'grad_norm': 0.359375, 'learning_rate': 0.00022898230088495576, 'epoch': 2.76}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2550/4620 [9:43:14<7:28:47, 13.01s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2551/4620 [9:43:26<7:21:25, 12.80s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2552/4620 [9:43:39<7:18:31, 12.72s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2553/4620 [9:43:53<7:31:31, 13.11s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2554/4620 [9:44:07<7:37:14, 13.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2555/4620 [9:44:20<7:35:12, 13.23s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2556/4620 [9:44:32<7:24:04, 12.91s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2557/4620 [9:44:45<7:21:37, 12.84s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2558/4620 [9:44:57<7:21:02, 12.83s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2559/4620 [9:45:10<7:18:07, 12.75s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2560/4620 [9:45:24<7:26:07, 12.99s/it]                                                       {'loss': 0.0698, 'grad_norm': 0.2373046875, 'learning_rate': 0.00022787610619469026, 'epoch': 2.77}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2560/4620 [9:45:24<7:26:07, 12.99s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2561/4620 [9:45:36<7:18:53, 12.79s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2562/4620 [9:45:50<7:31:01, 13.15s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2563/4620 [9:46:03<7:31:37, 13.17s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2564/4620 [9:46:15<7:20:32, 12.86s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2565/4620 [9:46:29<7:31:12, 13.17s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2566/4620 [9:46:41<7:18:37, 12.81s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2567/4620 [9:46:54<7:15:08, 12.72s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2568/4620 [9:47:06<7:16:11, 12.75s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2569/4620 [9:47:19<7:11:32, 12.62s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2570/4620 [9:47:33<7:25:14, 13.03s/it]                                                       {'loss': 0.0673, 'grad_norm': 0.478515625, 'learning_rate': 0.00022676991150442476, 'epoch': 2.78}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2570/4620 [9:47:33<7:25:14, 13.03s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2571/4620 [9:47:46<7:28:11, 13.12s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2572/4620 [9:47:58<7:16:34, 12.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2573/4620 [9:48:11<7:21:12, 12.93s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2574/4620 [9:48:25<7:31:08, 13.23s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2575/4620 [9:48:37<7:18:39, 12.87s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2576/4620 [9:48:50<7:14:43, 12.76s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2577/4620 [9:49:02<7:12:05, 12.69s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2578/4620 [9:49:15<7:10:24, 12.65s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2579/4620 [9:49:28<7:10:02, 12.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2580/4620 [9:49:42<7:24:30, 13.07s/it]                                                       {'loss': 0.0705, 'grad_norm': 0.2373046875, 'learning_rate': 0.0002256637168141593, 'epoch': 2.79}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2580/4620 [9:49:42<7:24:30, 13.07s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2581/4620 [9:49:55<7:32:35, 13.32s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2582/4620 [9:50:09<7:35:12, 13.40s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2583/4620 [9:50:21<7:20:54, 12.99s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2584/4620 [9:50:34<7:14:50, 12.81s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2585/4620 [9:50:47<7:25:29, 13.14s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2586/4620 [9:50:59<7:13:42, 12.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2587/4620 [9:51:12<7:07:37, 12.62s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2588/4620 [9:51:24<7:09:52, 12.69s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2589/4620 [9:51:37<7:06:28, 12.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2590/4620 [9:51:50<7:15:14, 12.86s/it]                                                       {'loss': 0.0659, 'grad_norm': 0.234375, 'learning_rate': 0.0002245575221238938, 'epoch': 2.8}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2590/4620 [9:51:50<7:15:14, 12.86s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2591/4620 [9:52:04<7:20:45, 13.03s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2592/4620 [9:52:17<7:23:31, 13.12s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2593/4620 [9:52:29<7:07:43, 12.66s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2594/4620 [9:52:41<7:03:04, 12.53s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2595/4620 [9:52:55<7:18:48, 13.00s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2596/4620 [9:53:07<7:09:46, 12.74s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2597/4620 [9:53:20<7:07:54, 12.69s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2598/4620 [9:53:32<7:07:54, 12.70s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2599/4620 [9:53:45<7:07:21, 12.69s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2600/4620 [9:53:59<7:17:24, 12.99s/it]                                                       {'loss': 0.0583, 'grad_norm': 0.095703125, 'learning_rate': 0.0002234513274336283, 'epoch': 2.81}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2600/4620 [9:53:59<7:17:24, 12.99s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:42,  3.04s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:32,  2.50s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:26,  2.24s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:22,  2.01s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.86s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:14<00:15,  1.75s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.70s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:12,  1.74s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:19<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.69s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.64s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:24<00:04,  1.61s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.62s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:27<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.90s/it][A                                                       
                                               [A{'eval_loss': 0.059196822345256805, 'eval_runtime': 77.2155, 'eval_samples_per_second': 6.475, 'eval_steps_per_second': 0.207, 'epoch': 2.81}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2600/4620 [9:55:16<7:17:24, 12.99s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.90s/it][A
                                               [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2601/4620 [9:55:29<20:16:19, 36.15s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2602/4620 [9:55:40<16:03:28, 28.65s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2603/4620 [9:55:52<13:14:44, 23.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2604/4620 [9:56:05<11:21:40, 20.29s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2605/4620 [9:56:20<10:28:30, 18.71s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2606/4620 [9:56:31<9:16:40, 16.58s/it]  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2607/4620 [9:56:43<8:33:05, 15.29s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2608/4620 [9:56:56<8:07:46, 14.55s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2609/4620 [9:57:09<7:49:23, 14.00s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2610/4620 [9:57:24<7:54:23, 14.16s/it]                                                       {'loss': 0.0697, 'grad_norm': 0.328125, 'learning_rate': 0.00022234513274336284, 'epoch': 2.82}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2610/4620 [9:57:24<7:54:23, 14.16s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2611/4620 [9:57:37<7:46:40, 13.94s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2612/4620 [9:57:49<7:23:37, 13.26s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2613/4620 [9:58:01<7:13:40, 12.96s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2614/4620 [9:58:14<7:11:55, 12.92s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2615/4620 [9:58:28<7:24:29, 13.30s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2616/4620 [9:58:40<7:11:53, 12.93s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2617/4620 [9:58:52<7:05:02, 12.73s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2618/4620 [9:59:05<7:01:29, 12.63s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2619/4620 [9:59:17<7:00:26, 12.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2620/4620 [9:59:31<7:12:30, 12.98s/it]                                                       {'loss': 0.0607, 'grad_norm': 0.189453125, 'learning_rate': 0.00022123893805309737, 'epoch': 2.84}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2620/4620 [9:59:31<7:12:30, 12.98s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2621/4620 [9:59:44<7:13:45, 13.02s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2622/4620 [9:59:56<7:05:18, 12.77s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2623/4620 [10:00:09<7:00:03, 12.62s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2624/4620 [10:00:21<6:59:35, 12.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2625/4620 [10:00:34<7:00:34, 12.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2626/4620 [10:00:48<7:18:20, 13.19s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2627/4620 [10:01:00<7:06:32, 12.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2628/4620 [10:01:13<6:59:42, 12.64s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2629/4620 [10:01:26<7:02:57, 12.75s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2630/4620 [10:01:40<7:14:43, 13.11s/it]                                                        {'loss': 0.0631, 'grad_norm': 0.306640625, 'learning_rate': 0.00022013274336283187, 'epoch': 2.85}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2630/4620 [10:01:40<7:14:43, 13.11s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2631/4620 [10:01:53<7:18:24, 13.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2632/4620 [10:02:06<7:20:31, 13.30s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2633/4620 [10:02:18<7:06:14, 12.87s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2634/4620 [10:02:30<6:56:58, 12.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2635/4620 [10:02:43<6:54:46, 12.54s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2636/4620 [10:02:57<7:10:52, 13.03s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2637/4620 [10:03:09<7:04:31, 12.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2638/4620 [10:03:21<6:57:35, 12.64s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2639/4620 [10:03:34<6:57:57, 12.66s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2640/4620 [10:03:48<7:08:50, 13.00s/it]                                                        {'loss': 0.0558, 'grad_norm': 0.2392578125, 'learning_rate': 0.00021902654867256638, 'epoch': 2.86}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2640/4620 [10:03:48<7:08:50, 13.00s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2641/4620 [10:04:01<7:12:41, 13.12s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2642/4620 [10:04:15<7:18:11, 13.29s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2643/4620 [10:04:27<7:08:08, 12.99s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2644/4620 [10:04:40<7:01:41, 12.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2645/4620 [10:04:52<6:57:23, 12.68s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2646/4620 [10:05:06<7:13:28, 13.18s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2647/4620 [10:05:19<7:03:15, 12.87s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2648/4620 [10:05:31<6:54:52, 12.62s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2649/4620 [10:05:43<6:51:36, 12.53s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2650/4620 [10:05:57<7:09:34, 13.08s/it]                                                        {'loss': 0.066, 'grad_norm': 0.138671875, 'learning_rate': 0.00021792035398230088, 'epoch': 2.87}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2650/4620 [10:05:57<7:09:34, 13.08s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2651/4620 [10:06:10<7:08:00, 13.04s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2652/4620 [10:06:24<7:10:44, 13.13s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2653/4620 [10:06:35<6:57:10, 12.73s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2654/4620 [10:06:48<6:53:08, 12.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2655/4620 [10:07:00<6:48:58, 12.49s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2656/4620 [10:07:14<7:06:17, 13.02s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2657/4620 [10:07:27<6:58:50, 12.80s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2658/4620 [10:07:39<6:54:16, 12.67s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2659/4620 [10:07:52<6:53:43, 12.66s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2660/4620 [10:08:06<7:09:36, 13.15s/it]                                                        {'loss': 0.0703, 'grad_norm': 0.3828125, 'learning_rate': 0.00021681415929203541, 'epoch': 2.88}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2660/4620 [10:08:06<7:09:36, 13.15s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2661/4620 [10:08:19<7:10:30, 13.19s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2662/4620 [10:08:32<7:11:08, 13.21s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2663/4620 [10:08:44<6:56:53, 12.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2664/4620 [10:08:56<6:51:30, 12.62s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2665/4620 [10:09:09<6:47:59, 12.52s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2666/4620 [10:09:23<7:01:31, 12.94s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2667/4620 [10:09:34<6:47:11, 12.51s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2668/4620 [10:09:47<6:46:12, 12.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2669/4620 [10:09:59<6:47:47, 12.54s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2670/4620 [10:10:15<7:20:23, 13.55s/it]                                                        {'loss': 0.0684, 'grad_norm': 0.3125, 'learning_rate': 0.00021570796460176992, 'epoch': 2.89}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2670/4620 [10:10:15<7:20:23, 13.55s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2671/4620 [10:10:27<7:00:54, 12.96s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2672/4620 [10:10:40<7:04:19, 13.07s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2673/4620 [10:10:52<6:54:01, 12.76s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2674/4620 [10:11:04<6:49:46, 12.63s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2675/4620 [10:11:17<6:48:44, 12.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2676/4620 [10:11:29<6:47:33, 12.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2677/4620 [10:11:43<6:59:13, 12.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2678/4620 [10:11:55<6:50:21, 12.68s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2679/4620 [10:12:09<7:00:40, 13.00s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2680/4620 [10:12:22<7:01:20, 13.03s/it]                                                        {'loss': 0.0708, 'grad_norm': 0.1484375, 'learning_rate': 0.00021460176991150442, 'epoch': 2.9}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2680/4620 [10:12:22<7:01:20, 13.03s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2681/4620 [10:12:34<6:51:51, 12.74s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2682/4620 [10:12:48<6:57:55, 12.94s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2683/4620 [10:13:00<6:47:30, 12.62s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2684/4620 [10:13:12<6:41:21, 12.44s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2685/4620 [10:13:24<6:43:09, 12.50s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2686/4620 [10:13:37<6:44:16, 12.54s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2687/4620 [10:13:50<6:52:45, 12.81s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2688/4620 [10:14:02<6:43:30, 12.53s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2689/4620 [10:14:18<7:12:10, 13.43s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2690/4620 [10:14:29<6:49:30, 12.73s/it]                                                        {'loss': 0.0826, 'grad_norm': 0.2392578125, 'learning_rate': 0.00021349557522123893, 'epoch': 2.91}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2690/4620 [10:14:29<6:49:30, 12.73s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2691/4620 [10:14:41<6:42:28, 12.52s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2692/4620 [10:14:55<6:55:24, 12.93s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2693/4620 [10:15:07<6:45:28, 12.62s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2694/4620 [10:15:19<6:42:15, 12.53s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2695/4620 [10:15:31<6:39:52, 12.46s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2696/4620 [10:15:44<6:40:37, 12.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2697/4620 [10:15:58<6:54:20, 12.93s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2698/4620 [10:16:11<6:55:55, 12.98s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2699/4620 [10:16:24<6:58:21, 13.07s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2700/4620 [10:16:36<6:45:52, 12.68s/it]                                                        {'loss': 0.0657, 'grad_norm': 0.546875, 'learning_rate': 0.00021238938053097346, 'epoch': 2.92}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2700/4620 [10:16:36<6:45:52, 12.68s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:09<01:05,  4.71s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:11<00:46,  3.56s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:12<00:33,  2.83s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:14<00:26,  2.40s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:21,  2.18s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:18<00:18,  2.05s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:15,  1.90s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:21<00:12,  1.79s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.73s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.67s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.68s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:27<00:05,  1.72s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.68s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.87s/it][A                                                        
                                               [A{'eval_loss': 0.06735344976186752, 'eval_runtime': 94.3616, 'eval_samples_per_second': 5.299, 'eval_steps_per_second': 0.17, 'epoch': 2.92}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2700/4620 [10:18:10<6:45:52, 12.68s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.87s/it][A
                                               [A 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2701/4620 [10:18:21<21:28:23, 40.28s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2702/4620 [10:18:34<17:11:30, 32.27s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2703/4620 [10:18:46<13:54:57, 26.13s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2704/4620 [10:18:58<11:42:20, 21.99s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2705/4620 [10:19:11<10:09:18, 19.09s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2706/4620 [10:19:23<9:07:20, 17.16s/it]  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2707/4620 [10:19:37<8:39:14, 16.29s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2708/4620 [10:19:51<8:11:15, 15.42s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2709/4620 [10:20:03<7:37:27, 14.36s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2710/4620 [10:20:15<7:18:01, 13.76s/it]                                                        {'loss': 0.0685, 'grad_norm': 0.6953125, 'learning_rate': 0.00021128318584070796, 'epoch': 2.93}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2710/4620 [10:20:15<7:18:01, 13.76s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2711/4620 [10:20:28<7:06:39, 13.41s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2712/4620 [10:20:42<7:13:34, 13.63s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2713/4620 [10:20:54<7:01:46, 13.27s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2714/4620 [10:21:07<6:54:08, 13.04s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2715/4620 [10:21:20<6:52:55, 13.01s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2716/4620 [10:21:32<6:46:03, 12.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2717/4620 [10:21:47<7:05:12, 13.41s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2718/4620 [10:22:00<7:05:35, 13.43s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2719/4620 [10:22:12<6:48:46, 12.90s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2720/4620 [10:22:24<6:43:54, 12.76s/it]                                                        {'loss': 0.0794, 'grad_norm': 0.390625, 'learning_rate': 0.00021017699115044247, 'epoch': 2.94}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2720/4620 [10:22:24<6:43:54, 12.76s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2721/4620 [10:22:37<6:42:24, 12.71s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2722/4620 [10:22:51<6:55:06, 13.12s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2723/4620 [10:23:03<6:45:49, 12.84s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2724/4620 [10:23:16<6:45:06, 12.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2725/4620 [10:23:29<6:47:14, 12.89s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2726/4620 [10:23:42<6:44:35, 12.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2727/4620 [10:23:56<6:56:59, 13.22s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2728/4620 [10:24:10<7:03:00, 13.41s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2729/4620 [10:24:21<6:45:41, 12.87s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2730/4620 [10:24:34<6:39:56, 12.70s/it]                                                        {'loss': 0.066, 'grad_norm': 0.2451171875, 'learning_rate': 0.00020907079646017697, 'epoch': 2.95}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2730/4620 [10:24:34<6:39:56, 12.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2731/4620 [10:24:47<6:43:16, 12.81s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2732/4620 [10:25:01<6:55:12, 13.20s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2733/4620 [10:25:13<6:45:48, 12.90s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2734/4620 [10:25:25<6:39:06, 12.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2735/4620 [10:25:38<6:40:31, 12.75s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2736/4620 [10:25:51<6:39:39, 12.73s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2737/4620 [10:26:05<6:57:33, 13.31s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2738/4620 [10:26:19<7:00:09, 13.39s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2739/4620 [10:26:31<6:42:27, 12.84s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2740/4620 [10:26:43<6:37:11, 12.68s/it]                                                        {'loss': 0.0687, 'grad_norm': 0.138671875, 'learning_rate': 0.0002079646017699115, 'epoch': 2.97}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2740/4620 [10:26:43<6:37:11, 12.68s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2741/4620 [10:26:56<6:40:12, 12.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2742/4620 [10:27:10<6:52:29, 13.18s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2743/4620 [10:27:22<6:40:20, 12.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2744/4620 [10:27:34<6:36:54, 12.69s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2745/4620 [10:27:47<6:39:18, 12.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2746/4620 [10:28:00<6:35:54, 12.68s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2747/4620 [10:28:14<6:47:11, 13.04s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2748/4620 [10:28:27<6:51:46, 13.20s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2749/4620 [10:28:39<6:40:29, 12.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2750/4620 [10:28:51<6:32:39, 12.60s/it]                                                        {'loss': 0.0695, 'grad_norm': 0.1865234375, 'learning_rate': 0.000206858407079646, 'epoch': 2.98}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2750/4620 [10:28:51<6:32:39, 12.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2751/4620 [10:29:05<6:38:36, 12.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2752/4620 [10:29:18<6:47:33, 13.09s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2753/4620 [10:29:30<6:38:17, 12.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2754/4620 [10:29:43<6:35:00, 12.70s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2755/4620 [10:29:56<6:37:34, 12.79s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2756/4620 [10:30:06<6:14:09, 12.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2757/4620 [10:30:15<5:45:20, 11.12s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2758/4620 [10:30:24<5:26:34, 10.52s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2759/4620 [10:30:33<5:10:18, 10.00s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2760/4620 [10:30:42<4:57:40,  9.60s/it]                                                        {'loss': 0.073, 'grad_norm': 0.259765625, 'learning_rate': 0.00020575221238938052, 'epoch': 2.99}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2760/4620 [10:30:42<4:57:40,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2761/4620 [10:30:51<4:50:33,  9.38s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2762/4620 [10:30:59<4:44:25,  9.18s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2763/4620 [10:31:09<4:44:03,  9.18s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2764/4620 [10:31:17<4:39:48,  9.05s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2765/4620 [10:31:26<4:36:35,  8.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2766/4620 [10:31:35<4:35:21,  8.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2767/4620 [10:31:44<4:38:50,  9.03s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2768/4620 [10:31:54<4:42:26,  9.15s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2769/4620 [10:32:02<4:39:51,  9.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2770/4620 [10:32:11<4:37:46,  9.01s/it]                                                        {'loss': 0.0684, 'grad_norm': 0.45703125, 'learning_rate': 0.00020464601769911507, 'epoch': 3.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2770/4620 [10:32:11<4:37:46,  9.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2771/4620 [10:32:20<4:35:19,  8.93s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2772/4620 [10:32:31<4:51:48,  9.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2773/4620 [10:34:56<25:48:03, 50.29s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2774/4620 [10:35:20<21:38:07, 42.19s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2775/4620 [10:35:38<17:52:54, 34.89s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2776/4620 [10:35:56<15:21:56, 30.00s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2777/4620 [10:36:13<13:17:25, 25.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2778/4620 [10:36:25<11:15:14, 21.99s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2779/4620 [10:36:38<9:46:36, 19.12s/it]  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2780/4620 [10:36:50<8:44:38, 17.11s/it]                                                        {'loss': 0.0456, 'grad_norm': 0.1572265625, 'learning_rate': 0.00020353982300884958, 'epoch': 3.01}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2780/4620 [10:36:50<8:44:38, 17.11s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2781/4620 [10:37:03<8:01:12, 15.70s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2782/4620 [10:37:17<7:48:42, 15.30s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2783/4620 [10:37:29<7:18:07, 14.31s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2784/4620 [10:37:41<6:57:51, 13.66s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2785/4620 [10:37:54<6:49:51, 13.40s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2786/4620 [10:38:07<6:46:08, 13.29s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2787/4620 [10:38:21<6:52:12, 13.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2788/4620 [10:38:33<6:41:26, 13.15s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2789/4620 [10:38:46<6:35:43, 12.97s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2790/4620 [10:38:59<6:36:24, 13.00s/it]                                                        {'loss': 0.062, 'grad_norm': 0.515625, 'learning_rate': 0.00020243362831858408, 'epoch': 3.02}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2790/4620 [10:38:59<6:36:24, 13.00s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2791/4620 [10:39:11<6:32:29, 12.88s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2792/4620 [10:39:26<6:45:53, 13.32s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2793/4620 [10:39:38<6:35:49, 13.00s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2794/4620 [10:39:50<6:29:55, 12.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2795/4620 [10:40:03<6:24:46, 12.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2796/4620 [10:40:17<6:37:51, 13.09s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2797/4620 [10:40:30<6:40:49, 13.19s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2798/4620 [10:40:42<6:28:44, 12.80s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2799/4620 [10:40:54<6:24:06, 12.66s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2800/4620 [10:41:07<6:26:59, 12.76s/it]                                                        {'loss': 0.0492, 'grad_norm': 0.1357421875, 'learning_rate': 0.0002013274336283186, 'epoch': 3.03}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2800/4620 [10:41:08<6:26:59, 12.76s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:51,  3.66s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:37,  2.87s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.41s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:24,  2.22s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  2.00s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.84s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:13,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:11,  1.69s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.62s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.62s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.91s/it][A                                                        
                                               [A{'eval_loss': 0.06996223330497742, 'eval_runtime': 66.8825, 'eval_samples_per_second': 7.476, 'eval_steps_per_second': 0.239, 'epoch': 3.03}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2800/4620 [10:42:14<6:26:59, 12.76s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.91s/it][A
                                               [A 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2801/4620 [10:42:26<16:25:37, 32.51s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2802/4620 [10:42:37<13:12:17, 26.15s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2803/4620 [10:42:50<11:05:35, 21.98s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2804/4620 [10:43:02<9:36:36, 19.05s/it]  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2805/4620 [10:43:15<8:39:33, 17.18s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2806/4620 [10:43:29<8:09:57, 16.21s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2807/4620 [10:43:42<7:44:19, 15.37s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2808/4620 [10:43:55<7:23:52, 14.70s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2809/4620 [10:44:07<7:02:05, 13.98s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2810/4620 [10:44:21<6:59:20, 13.90s/it]                                                        {'loss': 0.0622, 'grad_norm': 0.296875, 'learning_rate': 0.00020022123893805312, 'epoch': 3.04}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2810/4620 [10:44:21<6:59:20, 13.90s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2811/4620 [10:44:33<6:45:02, 13.43s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2812/4620 [10:44:46<6:37:28, 13.19s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2813/4620 [10:44:59<6:33:08, 13.05s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2814/4620 [10:45:11<6:29:04, 12.93s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2815/4620 [10:45:25<6:38:07, 13.23s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2816/4620 [10:45:39<6:40:47, 13.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2817/4620 [10:45:51<6:27:55, 12.91s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2818/4620 [10:46:05<6:35:10, 13.16s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2819/4620 [10:46:17<6:26:39, 12.88s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2820/4620 [10:46:29<6:23:53, 12.80s/it]                                                        {'loss': 0.0552, 'grad_norm': 0.326171875, 'learning_rate': 0.00019911504424778762, 'epoch': 3.05}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2820/4620 [10:46:30<6:23:53, 12.80s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2821/4620 [10:46:44<6:39:30, 13.32s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2822/4620 [10:46:56<6:28:50, 12.98s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2823/4620 [10:47:08<6:22:02, 12.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2824/4620 [10:47:21<6:23:51, 12.82s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2825/4620 [10:47:35<6:32:28, 13.12s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2826/4620 [10:47:49<6:37:09, 13.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2827/4620 [10:48:01<6:23:38, 12.84s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2828/4620 [10:48:14<6:31:21, 13.10s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2829/4620 [10:48:27<6:25:27, 12.91s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2830/4620 [10:48:40<6:23:58, 12.87s/it]                                                        {'loss': 0.0511, 'grad_norm': 0.291015625, 'learning_rate': 0.00019800884955752213, 'epoch': 3.06}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2830/4620 [10:48:40<6:23:58, 12.87s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2831/4620 [10:48:53<6:31:28, 13.13s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2832/4620 [10:49:06<6:22:49, 12.85s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2833/4620 [10:49:18<6:19:41, 12.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2834/4620 [10:49:32<6:30:28, 13.12s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2835/4620 [10:49:46<6:34:15, 13.25s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2836/4620 [10:49:57<6:19:45, 12.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2837/4620 [10:50:09<6:14:01, 12.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2838/4620 [10:50:23<6:26:26, 13.01s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2839/4620 [10:50:35<6:15:18, 12.64s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2840/4620 [10:50:48<6:15:10, 12.65s/it]                                                        {'loss': 0.0576, 'grad_norm': 0.12890625, 'learning_rate': 0.00019690265486725663, 'epoch': 3.07}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2840/4620 [10:50:48<6:15:10, 12.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2841/4620 [10:51:02<6:24:21, 12.96s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2842/4620 [10:51:14<6:18:32, 12.77s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2843/4620 [10:51:26<6:16:05, 12.70s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2844/4620 [10:51:39<6:16:38, 12.72s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2845/4620 [10:51:53<6:26:29, 13.06s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2846/4620 [10:52:05<6:17:47, 12.78s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2847/4620 [10:52:18<6:15:04, 12.69s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2848/4620 [10:52:31<6:22:29, 12.95s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2849/4620 [10:52:43<6:14:13, 12.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2850/4620 [10:52:56<6:11:59, 12.61s/it]                                                        {'loss': 0.0577, 'grad_norm': 0.21875, 'learning_rate': 0.00019579646017699117, 'epoch': 3.08}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2850/4620 [10:52:56<6:11:59, 12.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2851/4620 [10:53:08<6:11:22, 12.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2852/4620 [10:53:22<6:24:37, 13.05s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2853/4620 [10:53:35<6:16:43, 12.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2854/4620 [10:53:49<6:26:19, 13.13s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2855/4620 [10:54:02<6:27:45, 13.18s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2856/4620 [10:54:14<6:14:46, 12.75s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2857/4620 [10:54:26<6:12:27, 12.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2858/4620 [10:54:39<6:10:43, 12.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2859/4620 [10:54:53<6:25:04, 13.12s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2860/4620 [10:55:05<6:17:00, 12.85s/it]                                                        {'loss': 0.0508, 'grad_norm': 0.220703125, 'learning_rate': 0.00019469026548672567, 'epoch': 3.1}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2860/4620 [10:55:05<6:17:00, 12.85s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2861/4620 [10:55:17<6:12:34, 12.71s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2862/4620 [10:55:32<6:24:28, 13.12s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2863/4620 [10:55:44<6:16:42, 12.86s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2864/4620 [10:55:58<6:26:53, 13.22s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2865/4620 [10:56:11<6:23:06, 13.10s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2866/4620 [10:56:23<6:15:23, 12.84s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2867/4620 [10:56:36<6:16:25, 12.88s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2868/4620 [10:56:49<6:16:12, 12.88s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2869/4620 [10:57:03<6:28:55, 13.33s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2870/4620 [10:57:15<6:16:31, 12.91s/it]                                                        {'loss': 0.0557, 'grad_norm': 0.1376953125, 'learning_rate': 0.00019358407079646017, 'epoch': 3.11}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2870/4620 [10:57:15<6:16:31, 12.91s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2871/4620 [10:57:28<6:13:01, 12.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2872/4620 [10:57:41<6:22:04, 13.11s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2873/4620 [10:57:55<6:28:16, 13.34s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2874/4620 [10:58:07<6:15:07, 12.89s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2875/4620 [10:58:19<6:08:44, 12.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2876/4620 [10:58:34<6:22:23, 13.16s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2877/4620 [10:58:45<6:10:57, 12.77s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2878/4620 [10:58:58<6:07:40, 12.66s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2879/4620 [10:59:11<6:13:26, 12.87s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2880/4620 [10:59:23<6:06:47, 12.65s/it]                                                        {'loss': 0.0702, 'grad_norm': 0.4609375, 'learning_rate': 0.00019247787610619468, 'epoch': 3.12}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2880/4620 [10:59:23<6:06:47, 12.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2881/4620 [10:59:36<6:07:00, 12.66s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2882/4620 [10:59:50<6:20:21, 13.13s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2883/4620 [11:00:02<6:10:23, 12.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2884/4620 [11:00:15<6:07:23, 12.70s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2885/4620 [11:00:27<6:06:01, 12.66s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2886/4620 [11:00:41<6:15:23, 12.99s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2887/4620 [11:00:54<6:10:43, 12.84s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2888/4620 [11:01:06<6:07:47, 12.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2889/4620 [11:01:20<6:16:10, 13.04s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2890/4620 [11:01:32<6:08:16, 12.77s/it]                                                        {'loss': 0.0502, 'grad_norm': 0.13671875, 'learning_rate': 0.0001913716814159292, 'epoch': 3.13}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2890/4620 [11:01:32<6:08:16, 12.77s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2891/4620 [11:01:45<6:12:39, 12.93s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2892/4620 [11:01:59<6:19:33, 13.18s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2893/4620 [11:02:11<6:08:28, 12.80s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2894/4620 [11:02:24<6:06:06, 12.73s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2895/4620 [11:02:36<6:06:13, 12.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2896/4620 [11:02:50<6:16:20, 13.10s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2897/4620 [11:03:02<6:06:58, 12.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2898/4620 [11:03:15<6:08:45, 12.85s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2899/4620 [11:03:28<6:10:47, 12.93s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2900/4620 [11:03:40<6:02:20, 12.64s/it]                                                        {'loss': 0.0525, 'grad_norm': 0.4140625, 'learning_rate': 0.00019026548672566372, 'epoch': 3.14}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2900/4620 [11:03:40<6:02:20, 12.64s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:48,  3.48s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:35,  2.76s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.34s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:23,  2.09s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.92s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.89s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.82s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.77s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.65s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.62s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.63s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.64s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.07169615477323532, 'eval_runtime': 68.002, 'eval_samples_per_second': 7.353, 'eval_steps_per_second': 0.235, 'epoch': 3.14}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2900/4620 [11:04:48<6:02:20, 12.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.86s/it][A
                                               [A 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2901/4620 [11:05:00<15:39:50, 32.80s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2902/4620 [11:05:14<12:52:17, 26.97s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2903/4620 [11:05:25<10:39:25, 22.34s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2904/4620 [11:05:37<9:12:32, 19.32s/it]  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2905/4620 [11:05:51<8:21:58, 17.56s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2906/4620 [11:06:03<7:35:37, 15.95s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2907/4620 [11:06:15<7:04:48, 14.88s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2908/4620 [11:06:28<6:46:39, 14.25s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2909/4620 [11:06:42<6:40:45, 14.05s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2910/4620 [11:06:54<6:27:30, 13.60s/it]                                                        {'loss': 0.0557, 'grad_norm': 0.25390625, 'learning_rate': 0.00018915929203539822, 'epoch': 3.15}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2910/4620 [11:06:54<6:27:30, 13.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2911/4620 [11:07:08<6:29:23, 13.67s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2912/4620 [11:07:22<6:28:59, 13.66s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2913/4620 [11:07:34<6:19:08, 13.33s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2914/4620 [11:07:47<6:12:07, 13.09s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2915/4620 [11:08:00<6:16:12, 13.24s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2916/4620 [11:08:12<6:05:24, 12.87s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2917/4620 [11:08:25<6:02:49, 12.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2918/4620 [11:08:39<6:14:26, 13.20s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2919/4620 [11:08:51<6:03:22, 12.82s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2920/4620 [11:09:04<5:59:44, 12.70s/it]                                                        {'loss': 0.0499, 'grad_norm': 0.1669921875, 'learning_rate': 0.00018805309734513272, 'epoch': 3.16}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2920/4620 [11:09:04<5:59:44, 12.70s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2921/4620 [11:09:18<6:12:26, 13.15s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2922/4620 [11:09:31<6:16:05, 13.29s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2923/4620 [11:09:43<6:04:03, 12.87s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2924/4620 [11:09:56<5:59:32, 12.72s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2925/4620 [11:10:10<6:12:05, 13.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2926/4620 [11:10:21<5:57:26, 12.66s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2927/4620 [11:10:35<6:09:03, 13.08s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2928/4620 [11:10:47<5:55:10, 12.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2929/4620 [11:10:59<5:53:06, 12.53s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2930/4620 [11:11:13<6:05:27, 12.98s/it]                                                        {'loss': 0.0591, 'grad_norm': 0.23828125, 'learning_rate': 0.00018694690265486728, 'epoch': 3.17}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2930/4620 [11:11:13<6:05:27, 12.98s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2931/4620 [11:11:25<5:58:30, 12.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2932/4620 [11:11:39<6:06:19, 13.02s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2933/4620 [11:11:51<5:57:33, 12.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2934/4620 [11:12:04<5:55:41, 12.66s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2935/4620 [11:12:17<6:04:45, 12.99s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2936/4620 [11:12:30<6:04:25, 12.98s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2937/4620 [11:12:42<5:52:38, 12.57s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2938/4620 [11:12:54<5:49:04, 12.45s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2939/4620 [11:13:07<5:48:41, 12.45s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2940/4620 [11:13:19<5:50:30, 12.52s/it]                                                        {'loss': 0.068, 'grad_norm': 0.142578125, 'learning_rate': 0.0001858407079646018, 'epoch': 3.18}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2940/4620 [11:13:19<5:50:30, 12.52s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2941/4620 [11:13:34<6:05:26, 13.06s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2942/4620 [11:13:46<5:58:02, 12.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2943/4620 [11:14:00<6:07:57, 13.16s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2944/4620 [11:14:11<5:54:40, 12.70s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2945/4620 [11:14:25<6:03:04, 13.01s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2946/4620 [11:14:37<5:52:08, 12.62s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2947/4620 [11:14:51<6:01:22, 12.96s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2948/4620 [11:15:03<5:55:19, 12.75s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2949/4620 [11:15:15<5:51:48, 12.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2950/4620 [11:15:28<5:51:52, 12.64s/it]                                                        {'loss': 0.0599, 'grad_norm': 0.1396484375, 'learning_rate': 0.0001847345132743363, 'epoch': 3.19}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2950/4620 [11:15:28<5:51:52, 12.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2951/4620 [11:15:42<6:01:03, 12.98s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2952/4620 [11:15:54<5:53:31, 12.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2953/4620 [11:16:08<6:03:45, 13.09s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2954/4620 [11:16:21<6:02:23, 13.05s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2955/4620 [11:16:32<5:50:43, 12.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2956/4620 [11:16:45<5:50:43, 12.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2957/4620 [11:16:59<6:00:55, 13.02s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2958/4620 [11:17:11<5:52:36, 12.73s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2959/4620 [11:17:24<5:51:03, 12.68s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2960/4620 [11:17:36<5:50:28, 12.67s/it]                                                        {'loss': 0.06, 'grad_norm': 0.439453125, 'learning_rate': 0.0001836283185840708, 'epoch': 3.2}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2960/4620 [11:17:36<5:50:28, 12.67s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2961/4620 [11:17:50<6:02:12, 13.10s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2962/4620 [11:18:03<5:55:31, 12.87s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2963/4620 [11:18:17<6:06:59, 13.29s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2964/4620 [11:18:30<6:06:29, 13.28s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2965/4620 [11:18:42<5:51:30, 12.74s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2966/4620 [11:18:54<5:48:06, 12.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2967/4620 [11:19:08<5:59:56, 13.06s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2968/4620 [11:19:20<5:47:57, 12.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2969/4620 [11:19:32<5:44:27, 12.52s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2970/4620 [11:19:45<5:48:16, 12.66s/it]                                                        {'loss': 0.0619, 'grad_norm': 0.34765625, 'learning_rate': 0.00018252212389380533, 'epoch': 3.21}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2970/4620 [11:19:45<5:48:16, 12.66s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2971/4620 [11:19:58<5:49:19, 12.71s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2972/4620 [11:20:12<5:57:15, 13.01s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2973/4620 [11:20:25<6:01:09, 13.16s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2974/4620 [11:20:38<6:01:27, 13.18s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2975/4620 [11:20:50<5:50:08, 12.77s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2976/4620 [11:21:03<5:47:15, 12.67s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2977/4620 [11:21:15<5:46:32, 12.66s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2978/4620 [11:21:29<5:59:25, 13.13s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2979/4620 [11:21:41<5:49:22, 12.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2980/4620 [11:21:54<5:47:15, 12.70s/it]                                                        {'loss': 0.0676, 'grad_norm': 0.1669921875, 'learning_rate': 0.00018141592920353983, 'epoch': 3.23}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2980/4620 [11:21:54<5:47:15, 12.70s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2981/4620 [11:22:06<5:44:54, 12.63s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2982/4620 [11:22:20<5:55:20, 13.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2983/4620 [11:22:34<5:59:43, 13.18s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2984/4620 [11:22:47<6:00:48, 13.23s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2985/4620 [11:22:59<5:52:02, 12.92s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2986/4620 [11:23:12<5:46:27, 12.72s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2987/4620 [11:23:25<5:48:37, 12.81s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2988/4620 [11:23:39<5:57:22, 13.14s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2989/4620 [11:23:50<5:47:20, 12.78s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2990/4620 [11:24:03<5:43:37, 12.65s/it]                                                        {'loss': 0.059, 'grad_norm': 0.265625, 'learning_rate': 0.00018030973451327434, 'epoch': 3.24}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2990/4620 [11:24:03<5:43:37, 12.65s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2991/4620 [11:24:16<5:47:05, 12.78s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2992/4620 [11:24:30<5:55:53, 13.12s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2993/4620 [11:24:45<6:14:02, 13.79s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2994/4620 [11:24:57<5:53:57, 13.06s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2995/4620 [11:25:09<5:46:25, 12.79s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2996/4620 [11:25:21<5:44:35, 12.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2997/4620 [11:25:34<5:44:20, 12.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2998/4620 [11:25:48<5:54:24, 13.11s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2999/4620 [11:26:00<5:47:53, 12.88s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3000/4620 [11:26:13<5:42:39, 12.69s/it]                                                        {'loss': 0.0648, 'grad_norm': 0.158203125, 'learning_rate': 0.00017920353982300884, 'epoch': 3.25}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3000/4620 [11:26:13<5:42:39, 12.69s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:54,  3.92s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:10<00:41,  3.19s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:31,  2.59s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:24,  2.24s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:20,  2.04s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:17,  1.91s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:14,  1.87s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.77s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.70s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.66s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:05,  1.71s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:28<00:03,  1.75s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06604351103305817, 'eval_runtime': 67.1924, 'eval_samples_per_second': 7.441, 'eval_steps_per_second': 0.238, 'epoch': 3.25}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3000/4620 [11:27:20<5:42:39, 12.69s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.86s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3001/4620 [11:27:33<14:49:57, 32.98s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3002/4620 [11:27:46<12:08:33, 27.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3003/4620 [11:27:59<10:13:00, 22.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3004/4620 [11:28:11<8:47:26, 19.58s/it]  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3005/4620 [11:28:24<7:50:05, 17.46s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3006/4620 [11:28:36<7:13:13, 16.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3007/4620 [11:28:51<6:58:54, 15.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3008/4620 [11:29:03<6:28:06, 14.45s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3009/4620 [11:29:15<6:10:13, 13.79s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3010/4620 [11:29:28<6:02:35, 13.51s/it]                                                        {'loss': 0.0483, 'grad_norm': 0.146484375, 'learning_rate': 0.00017809734513274337, 'epoch': 3.26}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3010/4620 [11:29:28<6:02:35, 13.51s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3011/4620 [11:29:40<5:56:27, 13.29s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3012/4620 [11:29:54<6:01:33, 13.49s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3013/4620 [11:30:08<6:03:26, 13.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3014/4620 [11:30:20<5:48:08, 13.01s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3015/4620 [11:30:32<5:42:26, 12.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3016/4620 [11:30:45<5:42:34, 12.81s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3017/4620 [11:30:59<5:52:18, 13.19s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3018/4620 [11:31:11<5:40:45, 12.76s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3019/4620 [11:31:23<5:37:05, 12.63s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3020/4620 [11:31:36<5:37:37, 12.66s/it]                                                        {'loss': 0.0552, 'grad_norm': 0.11572265625, 'learning_rate': 0.00017699115044247788, 'epoch': 3.27}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3020/4620 [11:31:36<5:37:37, 12.66s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3021/4620 [11:31:49<5:39:22, 12.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3022/4620 [11:32:03<5:49:25, 13.12s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3023/4620 [11:32:15<5:38:56, 12.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3024/4620 [11:32:27<5:33:39, 12.54s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3025/4620 [11:32:40<5:35:25, 12.62s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3026/4620 [11:32:52<5:36:08, 12.65s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3027/4620 [11:33:07<5:52:30, 13.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3028/4620 [11:33:19<5:43:50, 12.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3029/4620 [11:33:31<5:37:07, 12.71s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3030/4620 [11:33:44<5:36:21, 12.69s/it]                                                        {'loss': 0.0565, 'grad_norm': 0.1806640625, 'learning_rate': 0.00017588495575221238, 'epoch': 3.28}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3030/4620 [11:33:44<5:36:21, 12.69s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3031/4620 [11:33:58<5:49:01, 13.18s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3032/4620 [11:34:12<5:49:49, 13.22s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3033/4620 [11:34:25<5:51:45, 13.30s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3034/4620 [11:34:37<5:41:42, 12.93s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3035/4620 [11:34:49<5:36:01, 12.72s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3036/4620 [11:35:02<5:32:28, 12.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3037/4620 [11:35:16<5:44:09, 13.04s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3038/4620 [11:35:28<5:36:08, 12.75s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3039/4620 [11:35:40<5:30:59, 12.56s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3040/4620 [11:35:53<5:30:55, 12.57s/it]                                                        {'loss': 0.0563, 'grad_norm': 0.31640625, 'learning_rate': 0.0001747787610619469, 'epoch': 3.29}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3040/4620 [11:35:53<5:30:55, 12.57s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3041/4620 [11:36:07<5:42:32, 13.02s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3042/4620 [11:36:19<5:40:11, 12.93s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3043/4620 [11:36:33<5:44:25, 13.10s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3044/4620 [11:36:45<5:35:12, 12.76s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3045/4620 [11:36:57<5:31:44, 12.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3046/4620 [11:37:10<5:29:42, 12.57s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3047/4620 [11:37:24<5:42:19, 13.06s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3048/4620 [11:37:36<5:33:46, 12.74s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3049/4620 [11:37:48<5:30:45, 12.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3050/4620 [11:38:00<5:26:58, 12.50s/it]                                                        {'loss': 0.056, 'grad_norm': 0.1435546875, 'learning_rate': 0.00017367256637168142, 'epoch': 3.3}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3050/4620 [11:38:00<5:26:58, 12.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3051/4620 [11:38:14<5:38:52, 12.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3052/4620 [11:38:27<5:39:15, 12.98s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3053/4620 [11:38:41<5:41:51, 13.09s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3054/4620 [11:38:52<5:29:38, 12.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3055/4620 [11:39:05<5:27:58, 12.57s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3056/4620 [11:39:17<5:26:04, 12.51s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3057/4620 [11:39:31<5:36:32, 12.92s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3058/4620 [11:39:43<5:30:16, 12.69s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3059/4620 [11:39:56<5:28:48, 12.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3060/4620 [11:40:10<5:39:12, 13.05s/it]                                                        {'loss': 0.0617, 'grad_norm': 0.189453125, 'learning_rate': 0.00017256637168141592, 'epoch': 3.31}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3060/4620 [11:40:10<5:39:12, 13.05s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3061/4620 [11:40:22<5:31:16, 12.75s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3062/4620 [11:40:36<5:41:02, 13.13s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3063/4620 [11:40:50<5:47:40, 13.40s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3064/4620 [11:41:01<5:33:14, 12.85s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3065/4620 [11:41:14<5:28:04, 12.66s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3066/4620 [11:41:26<5:27:17, 12.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3067/4620 [11:41:41<5:41:03, 13.18s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3068/4620 [11:41:52<5:30:15, 12.77s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3069/4620 [11:42:05<5:28:24, 12.70s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3070/4620 [11:42:20<5:43:00, 13.28s/it]                                                        {'loss': 0.0533, 'grad_norm': 0.3828125, 'learning_rate': 0.00017146017699115043, 'epoch': 3.32}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3070/4620 [11:42:20<5:43:00, 13.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3071/4620 [11:42:32<5:32:27, 12.88s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3072/4620 [11:42:44<5:27:24, 12.69s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3073/4620 [11:42:58<5:40:15, 13.20s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3074/4620 [11:43:10<5:31:21, 12.86s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3075/4620 [11:43:23<5:26:53, 12.69s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3076/4620 [11:43:35<5:26:17, 12.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3077/4620 [11:43:49<5:34:42, 13.02s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3078/4620 [11:44:01<5:26:16, 12.70s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3079/4620 [11:44:15<5:35:22, 13.06s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3080/4620 [11:44:27<5:25:22, 12.68s/it]                                                        {'loss': 0.0563, 'grad_norm': 0.3515625, 'learning_rate': 0.00017035398230088493, 'epoch': 3.33}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3080/4620 [11:44:27<5:25:22, 12.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3081/4620 [11:44:39<5:22:39, 12.58s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3082/4620 [11:44:51<5:19:50, 12.48s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3083/4620 [11:45:06<5:33:24, 13.02s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3084/4620 [11:45:17<5:24:16, 12.67s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3085/4620 [11:45:30<5:22:14, 12.60s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3086/4620 [11:45:44<5:31:22, 12.96s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3087/4620 [11:45:56<5:27:30, 12.82s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3088/4620 [11:46:08<5:23:51, 12.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3089/4620 [11:46:22<5:33:47, 13.08s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3090/4620 [11:46:35<5:26:28, 12.80s/it]                                                        {'loss': 0.062, 'grad_norm': 0.2099609375, 'learning_rate': 0.0001692477876106195, 'epoch': 3.34}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3090/4620 [11:46:35<5:26:28, 12.80s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3091/4620 [11:46:47<5:23:13, 12.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3092/4620 [11:47:00<5:22:58, 12.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3093/4620 [11:47:14<5:32:16, 13.06s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3094/4620 [11:47:26<5:24:08, 12.74s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3095/4620 [11:47:38<5:22:51, 12.70s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3096/4620 [11:47:52<5:28:17, 12.92s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3097/4620 [11:48:04<5:23:55, 12.76s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3098/4620 [11:48:18<5:28:46, 12.96s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3099/4620 [11:48:30<5:23:04, 12.74s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3100/4620 [11:48:42<5:18:55, 12.59s/it]                                                        {'loss': 0.052, 'grad_norm': 0.232421875, 'learning_rate': 0.000168141592920354, 'epoch': 3.35}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3100/4620 [11:48:42<5:18:55, 12.59s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:04<00:34,  2.44s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:06<00:28,  2.22s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:23,  1.94s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:09<00:20,  1.85s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:18,  1.83s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.76s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:14<00:13,  1.65s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:16<00:11,  1.61s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:17<00:09,  1.59s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:19<00:08,  1.67s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.69s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:05,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:24<00:03,  1.64s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.88s/it][A                                                        
                                               [A{'eval_loss': 0.0671394020318985, 'eval_runtime': 69.7751, 'eval_samples_per_second': 7.166, 'eval_steps_per_second': 0.229, 'epoch': 3.35}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3100/4620 [11:49:52<5:18:55, 12.59s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.88s/it][A
                                               [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3101/4620 [11:50:02<13:53:39, 32.93s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3102/4620 [11:50:16<11:24:56, 27.07s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3103/4620 [11:50:27<9:27:44, 22.45s/it]  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3104/4620 [11:50:40<8:08:39, 19.34s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3105/4620 [11:50:52<7:17:21, 17.32s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3106/4620 [11:51:07<6:57:42, 16.55s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3107/4620 [11:51:19<6:25:29, 15.29s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3108/4620 [11:51:33<6:17:11, 14.97s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3109/4620 [11:51:45<5:51:59, 13.98s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3110/4620 [11:51:57<5:39:06, 13.47s/it]                                                        {'loss': 0.0647, 'grad_norm': 0.201171875, 'learning_rate': 0.0001670353982300885, 'epoch': 3.37}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3110/4620 [11:51:57<5:39:06, 13.47s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3111/4620 [11:52:10<5:32:41, 13.23s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3112/4620 [11:52:24<5:37:46, 13.44s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3113/4620 [11:52:36<5:29:53, 13.13s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3114/4620 [11:52:49<5:24:48, 12.94s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3115/4620 [11:53:02<5:25:06, 12.96s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3116/4620 [11:53:16<5:33:20, 13.30s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3117/4620 [11:53:28<5:26:20, 13.03s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3118/4620 [11:53:42<5:29:20, 13.16s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3119/4620 [11:53:55<5:25:05, 12.99s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3120/4620 [11:54:07<5:19:42, 12.79s/it]                                                        {'loss': 0.0586, 'grad_norm': 0.2236328125, 'learning_rate': 0.00016592920353982303, 'epoch': 3.38}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3120/4620 [11:54:07<5:19:42, 12.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3121/4620 [11:54:19<5:18:16, 12.74s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3122/4620 [11:54:33<5:24:48, 13.01s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3123/4620 [11:54:45<5:18:57, 12.78s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3124/4620 [11:54:58<5:17:04, 12.72s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3125/4620 [11:55:11<5:16:47, 12.71s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3126/4620 [11:55:25<5:29:17, 13.22s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3127/4620 [11:55:37<5:20:58, 12.90s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3128/4620 [11:55:51<5:25:05, 13.07s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3129/4620 [11:56:03<5:19:18, 12.85s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3130/4620 [11:56:16<5:18:41, 12.83s/it]                                                        {'loss': 0.0635, 'grad_norm': 0.2236328125, 'learning_rate': 0.00016482300884955754, 'epoch': 3.39}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3130/4620 [11:56:16<5:18:41, 12.83s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3131/4620 [11:56:30<5:27:20, 13.19s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3132/4620 [11:56:42<5:16:21, 12.76s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3133/4620 [11:56:54<5:13:17, 12.64s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3134/4620 [11:57:07<5:12:47, 12.63s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3135/4620 [11:57:19<5:12:24, 12.62s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3136/4620 [11:57:33<5:23:34, 13.08s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3137/4620 [11:57:46<5:18:13, 12.88s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3138/4620 [11:57:59<5:22:21, 13.05s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3139/4620 [11:58:11<5:14:04, 12.72s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3140/4620 [11:58:24<5:11:39, 12.63s/it]                                                        {'loss': 0.0615, 'grad_norm': 0.380859375, 'learning_rate': 0.00016371681415929204, 'epoch': 3.4}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3140/4620 [11:58:24<5:11:39, 12.63s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3141/4620 [11:58:38<5:24:02, 13.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3142/4620 [11:58:50<5:14:01, 12.75s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3143/4620 [11:59:02<5:08:48, 12.54s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3144/4620 [11:59:14<5:09:57, 12.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3145/4620 [11:59:28<5:20:04, 13.02s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3146/4620 [11:59:41<5:14:18, 12.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3147/4620 [11:59:53<5:13:02, 12.75s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3148/4620 [12:00:07<5:20:15, 13.05s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3149/4620 [12:00:20<5:15:10, 12.86s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3150/4620 [12:00:33<5:22:42, 13.17s/it]                                                        {'loss': 0.0552, 'grad_norm': 0.2177734375, 'learning_rate': 0.00016261061946902655, 'epoch': 3.41}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3150/4620 [12:00:33<5:22:42, 13.17s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3151/4620 [12:00:45<5:14:07, 12.83s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3152/4620 [12:00:58<5:11:59, 12.75s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3153/4620 [12:01:10<5:09:13, 12.65s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3154/4620 [12:01:23<5:06:00, 12.52s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3155/4620 [12:01:37<5:15:48, 12.93s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3156/4620 [12:01:48<5:07:51, 12.62s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3157/4620 [12:02:01<5:04:13, 12.48s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3158/4620 [12:02:14<5:13:55, 12.88s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3159/4620 [12:02:27<5:08:21, 12.66s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3160/4620 [12:02:40<5:16:04, 12.99s/it]                                                        {'loss': 0.0657, 'grad_norm': 0.2421875, 'learning_rate': 0.00016150442477876108, 'epoch': 3.42}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3160/4620 [12:02:40<5:16:04, 12.99s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3161/4620 [12:02:52<5:08:50, 12.70s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3162/4620 [12:03:05<5:08:01, 12.68s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3163/4620 [12:03:18<5:08:45, 12.71s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3164/4620 [12:03:31<5:12:01, 12.86s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3165/4620 [12:03:45<5:20:22, 13.21s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3166/4620 [12:03:57<5:14:43, 12.99s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3167/4620 [12:04:10<5:08:47, 12.75s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3168/4620 [12:04:24<5:17:00, 13.10s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3169/4620 [12:04:37<5:18:20, 13.16s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3170/4620 [12:04:49<5:09:57, 12.83s/it]                                                        {'loss': 0.0469, 'grad_norm': 0.3359375, 'learning_rate': 0.00016039823008849558, 'epoch': 3.43}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3170/4620 [12:04:49<5:09:57, 12.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3171/4620 [12:05:01<5:04:16, 12.60s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3172/4620 [12:05:14<5:03:32, 12.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3173/4620 [12:05:26<5:04:18, 12.62s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3174/4620 [12:05:39<5:05:07, 12.66s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3175/4620 [12:05:53<5:14:50, 13.07s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3176/4620 [12:06:05<5:07:25, 12.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3177/4620 [12:06:18<5:05:19, 12.70s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3178/4620 [12:06:32<5:14:11, 13.07s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3179/4620 [12:06:46<5:20:37, 13.35s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3180/4620 [12:06:57<5:08:05, 12.84s/it]                                                        {'loss': 0.0609, 'grad_norm': 0.173828125, 'learning_rate': 0.0001592920353982301, 'epoch': 3.44}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3180/4620 [12:06:57<5:08:05, 12.84s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3181/4620 [12:07:10<5:03:49, 12.67s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3182/4620 [12:07:22<5:04:27, 12.70s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3183/4620 [12:07:35<5:05:26, 12.75s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3184/4620 [12:07:50<5:17:57, 13.28s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3185/4620 [12:08:02<5:07:30, 12.86s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3186/4620 [12:08:14<5:02:48, 12.67s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3187/4620 [12:08:27<5:03:47, 12.72s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3188/4620 [12:08:41<5:13:18, 13.13s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3189/4620 [12:08:54<5:14:15, 13.18s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3190/4620 [12:09:06<5:05:16, 12.81s/it]                                                        {'loss': 0.0584, 'grad_norm': 0.357421875, 'learning_rate': 0.0001581858407079646, 'epoch': 3.45}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3190/4620 [12:09:06<5:05:16, 12.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3191/4620 [12:09:18<5:02:57, 12.72s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3192/4620 [12:09:31<5:00:18, 12.62s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3193/4620 [12:09:43<5:00:12, 12.62s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3194/4620 [12:09:57<5:10:00, 13.04s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3195/4620 [12:10:09<5:01:25, 12.69s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3196/4620 [12:10:22<4:57:25, 12.53s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3197/4620 [12:10:36<5:08:54, 13.03s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3198/4620 [12:10:49<5:13:40, 13.24s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3199/4620 [12:11:03<5:14:53, 13.30s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3200/4620 [12:11:15<5:07:17, 12.98s/it]                                                        {'loss': 0.0512, 'grad_norm': 0.2138671875, 'learning_rate': 0.00015707964601769912, 'epoch': 3.46}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3200/4620 [12:11:15<5:07:17, 12.98s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:48,  3.49s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:42,  3.28s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:32,  2.69s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:26,  2.38s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:15<00:21,  2.13s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:16<00:17,  1.95s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:14,  1.82s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:23<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.70s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:04,  1.66s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.92s/it][A                                                        
                                               [A{'eval_loss': 0.06991074979305267, 'eval_runtime': 79.0536, 'eval_samples_per_second': 6.325, 'eval_steps_per_second': 0.202, 'epoch': 3.46}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3200/4620 [12:12:34<5:07:17, 12.98s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.92s/it][A
                                               [A 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3201/4620 [12:12:45<14:15:21, 36.17s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3202/4620 [12:12:57<11:18:46, 28.72s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3203/4620 [12:13:09<9:21:06, 23.76s/it]  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3204/4620 [12:13:23<8:10:57, 20.80s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3205/4620 [12:13:35<7:12:47, 18.35s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3206/4620 [12:13:48<6:31:22, 16.61s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3207/4620 [12:14:02<6:11:38, 15.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3208/4620 [12:14:16<5:56:56, 15.17s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3209/4620 [12:14:28<5:35:56, 14.29s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3210/4620 [12:14:40<5:21:38, 13.69s/it]                                                        {'loss': 0.0441, 'grad_norm': 0.1279296875, 'learning_rate': 0.00015597345132743363, 'epoch': 3.47}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3210/4620 [12:14:40<5:21:38, 13.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3211/4620 [12:14:53<5:13:16, 13.34s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3212/4620 [12:15:05<5:08:53, 13.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3213/4620 [12:15:19<5:13:52, 13.39s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3214/4620 [12:15:31<5:02:41, 12.92s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3215/4620 [12:15:44<4:59:52, 12.81s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3216/4620 [12:15:57<5:00:11, 12.83s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3217/4620 [12:16:09<4:58:28, 12.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3218/4620 [12:16:25<5:20:58, 13.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3219/4620 [12:16:37<5:08:27, 13.21s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3220/4620 [12:16:50<5:02:51, 12.98s/it]                                                        {'loss': 0.0531, 'grad_norm': 0.466796875, 'learning_rate': 0.00015486725663716813, 'epoch': 3.48}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3220/4620 [12:16:50<5:02:51, 12.98s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3221/4620 [12:17:02<5:00:53, 12.90s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3222/4620 [12:17:17<5:11:49, 13.38s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3223/4620 [12:17:29<5:06:25, 13.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3224/4620 [12:17:41<4:58:05, 12.81s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3225/4620 [12:17:54<4:56:37, 12.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3226/4620 [12:18:07<4:56:45, 12.77s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3227/4620 [12:18:21<5:04:29, 13.11s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3228/4620 [12:18:36<5:19:11, 13.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3229/4620 [12:18:47<5:02:00, 13.03s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3230/4620 [12:19:00<4:56:25, 12.80s/it]                                                        {'loss': 0.0583, 'grad_norm': 0.306640625, 'learning_rate': 0.00015376106194690264, 'epoch': 3.5}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3230/4620 [12:19:00<4:56:25, 12.80s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3231/4620 [12:19:14<5:04:52, 13.17s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3232/4620 [12:19:26<4:58:57, 12.92s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3233/4620 [12:19:38<4:55:08, 12.77s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3234/4620 [12:19:51<4:53:41, 12.71s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3235/4620 [12:20:04<4:52:56, 12.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3236/4620 [12:20:16<4:53:08, 12.71s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3237/4620 [12:20:30<5:00:05, 13.02s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3238/4620 [12:20:45<5:14:44, 13.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3239/4620 [12:20:57<4:58:52, 12.99s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3240/4620 [12:21:09<4:51:54, 12.69s/it]                                                        {'loss': 0.055, 'grad_norm': 0.1298828125, 'learning_rate': 0.00015265486725663717, 'epoch': 3.51}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3240/4620 [12:21:09<4:51:54, 12.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3241/4620 [12:21:21<4:48:20, 12.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3242/4620 [12:21:35<5:00:30, 13.08s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3243/4620 [12:21:47<4:53:21, 12.78s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3244/4620 [12:22:00<4:49:44, 12.63s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3245/4620 [12:22:12<4:48:59, 12.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3246/4620 [12:22:26<4:59:14, 13.07s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3247/4620 [12:22:40<5:04:37, 13.31s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3248/4620 [12:22:54<5:05:44, 13.37s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3249/4620 [12:23:06<4:58:34, 13.07s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3250/4620 [12:23:19<4:56:37, 12.99s/it]                                                        {'loss': 0.0562, 'grad_norm': 0.158203125, 'learning_rate': 0.0001515486725663717, 'epoch': 3.52}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3250/4620 [12:23:19<4:56:37, 12.99s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3251/4620 [12:23:31<4:50:43, 12.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3252/4620 [12:23:45<4:59:44, 13.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3253/4620 [12:23:57<4:49:58, 12.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3254/4620 [12:24:09<4:46:38, 12.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3255/4620 [12:24:22<4:45:28, 12.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3256/4620 [12:24:35<4:54:25, 12.95s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3257/4620 [12:24:49<4:56:44, 13.06s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3258/4620 [12:25:02<4:56:55, 13.08s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3259/4620 [12:25:14<4:51:08, 12.84s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3260/4620 [12:25:27<4:48:32, 12.73s/it]                                                        {'loss': 0.0613, 'grad_norm': 0.2392578125, 'learning_rate': 0.0001504424778761062, 'epoch': 3.53}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3260/4620 [12:25:27<4:48:32, 12.73s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3261/4620 [12:25:39<4:48:50, 12.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3262/4620 [12:25:54<4:57:24, 13.14s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3263/4620 [12:26:06<4:49:24, 12.80s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3264/4620 [12:26:18<4:45:13, 12.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3265/4620 [12:26:31<4:50:49, 12.88s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3266/4620 [12:26:45<4:57:20, 13.18s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3267/4620 [12:26:58<4:52:01, 12.95s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3268/4620 [12:27:11<4:55:55, 13.13s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3269/4620 [12:27:23<4:46:05, 12.71s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3270/4620 [12:27:35<4:43:05, 12.58s/it]                                                        {'loss': 0.058, 'grad_norm': 0.171875, 'learning_rate': 0.0001493362831858407, 'epoch': 3.54}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3270/4620 [12:27:35<4:43:05, 12.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3271/4620 [12:27:48<4:42:29, 12.56s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3272/4620 [12:28:00<4:42:12, 12.56s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3273/4620 [12:28:14<4:52:14, 13.02s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3274/4620 [12:28:26<4:45:41, 12.74s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3275/4620 [12:28:40<4:48:58, 12.89s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3276/4620 [12:28:53<4:50:09, 12.95s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3277/4620 [12:29:05<4:44:11, 12.70s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3278/4620 [12:29:19<4:51:19, 13.02s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3279/4620 [12:29:30<4:43:08, 12.67s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3280/4620 [12:29:43<4:40:16, 12.55s/it]                                                        {'loss': 0.0635, 'grad_norm': 0.2119140625, 'learning_rate': 0.00014823008849557524, 'epoch': 3.55}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3280/4620 [12:29:43<4:40:16, 12.55s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3281/4620 [12:29:55<4:41:32, 12.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3282/4620 [12:30:08<4:42:02, 12.65s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3283/4620 [12:30:21<4:45:27, 12.81s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3284/4620 [12:30:35<4:50:07, 13.03s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3285/4620 [12:30:48<4:50:07, 13.04s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3286/4620 [12:31:00<4:44:06, 12.78s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3287/4620 [12:31:12<4:41:15, 12.66s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3288/4620 [12:31:27<4:55:33, 13.31s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3289/4620 [12:31:39<4:46:58, 12.94s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3290/4620 [12:31:52<4:42:22, 12.74s/it]                                                        {'loss': 0.0564, 'grad_norm': 0.1845703125, 'learning_rate': 0.00014712389380530975, 'epoch': 3.56}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3290/4620 [12:31:52<4:42:22, 12.74s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3291/4620 [12:32:04<4:41:42, 12.72s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3292/4620 [12:32:17<4:43:53, 12.83s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3293/4620 [12:32:32<4:52:54, 13.24s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3294/4620 [12:32:45<4:54:07, 13.31s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3295/4620 [12:32:57<4:44:45, 12.89s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3296/4620 [12:33:10<4:42:02, 12.78s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3297/4620 [12:33:22<4:39:14, 12.66s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3298/4620 [12:33:36<4:48:37, 13.10s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3299/4620 [12:33:48<4:42:13, 12.82s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3300/4620 [12:34:01<4:39:18, 12.70s/it]                                                        {'loss': 0.0423, 'grad_norm': 0.361328125, 'learning_rate': 0.00014601769911504425, 'epoch': 3.57}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3300/4620 [12:34:01<4:39:18, 12.70s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:08<01:00,  4.34s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:10<00:42,  3.31s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:12<00:31,  2.66s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:24,  2.26s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:15<00:20,  2.10s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:17<00:17,  1.98s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:15,  1.89s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:20<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:21<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:23<00:08,  1.67s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:25<00:06,  1.63s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:05,  1.68s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:28<00:03,  1.67s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.90s/it][A                                                        
                                               [A{'eval_loss': 0.06841136515140533, 'eval_runtime': 78.7446, 'eval_samples_per_second': 6.35, 'eval_steps_per_second': 0.203, 'epoch': 3.57}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3300/4620 [12:35:19<4:39:18, 12.70s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.90s/it][A
                                               [A 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3301/4620 [12:35:30<13:03:24, 35.64s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3302/4620 [12:35:43<10:35:22, 28.92s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3303/4620 [12:35:56<8:50:02, 24.15s/it]  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3304/4620 [12:36:09<7:36:11, 20.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3305/4620 [12:36:21<6:38:04, 18.16s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3306/4620 [12:36:34<6:02:00, 16.53s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3307/4620 [12:36:48<5:48:07, 15.91s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3308/4620 [12:37:00<5:22:33, 14.75s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3309/4620 [12:37:13<5:06:11, 14.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3310/4620 [12:37:26<4:59:18, 13.71s/it]                                                        {'loss': 0.0658, 'grad_norm': 0.310546875, 'learning_rate': 0.00014491150442477876, 'epoch': 3.58}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3310/4620 [12:37:26<4:59:18, 13.71s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3311/4620 [12:37:38<4:53:05, 13.43s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3312/4620 [12:37:52<4:57:12, 13.63s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3313/4620 [12:38:08<5:11:10, 14.28s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3314/4620 [12:38:20<4:51:13, 13.38s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3315/4620 [12:38:32<4:43:25, 13.03s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3316/4620 [12:38:45<4:41:32, 12.95s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3317/4620 [12:38:58<4:47:20, 13.23s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3318/4620 [12:39:11<4:40:29, 12.93s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3319/4620 [12:39:23<4:37:40, 12.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3320/4620 [12:39:36<4:35:02, 12.69s/it]                                                        {'loss': 0.0696, 'grad_norm': 0.306640625, 'learning_rate': 0.0001438053097345133, 'epoch': 3.59}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3320/4620 [12:39:36<4:35:02, 12.69s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3321/4620 [12:39:48<4:34:42, 12.69s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3322/4620 [12:40:02<4:43:35, 13.11s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3323/4620 [12:40:16<4:44:52, 13.18s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3324/4620 [12:40:28<4:38:42, 12.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3325/4620 [12:40:40<4:34:33, 12.72s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3326/4620 [12:40:54<4:43:28, 13.14s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3327/4620 [12:41:07<4:37:58, 12.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3328/4620 [12:41:19<4:34:48, 12.76s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3329/4620 [12:41:32<4:34:46, 12.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3330/4620 [12:41:45<4:35:52, 12.83s/it]                                                        {'loss': 0.0744, 'grad_norm': 0.470703125, 'learning_rate': 0.0001426991150442478, 'epoch': 3.6}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3330/4620 [12:41:45<4:35:52, 12.83s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3331/4620 [12:41:59<4:44:17, 13.23s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3332/4620 [12:42:13<4:47:35, 13.40s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3333/4620 [12:42:25<4:39:41, 13.04s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3334/4620 [12:42:38<4:36:21, 12.89s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3335/4620 [12:42:52<4:43:13, 13.22s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3336/4620 [12:43:04<4:37:52, 12.98s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3337/4620 [12:43:16<4:33:52, 12.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3338/4620 [12:43:29<4:33:13, 12.79s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3339/4620 [12:43:42<4:30:32, 12.67s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3340/4620 [12:43:55<4:32:11, 12.76s/it]                                                        {'loss': 0.0452, 'grad_norm': 0.11767578125, 'learning_rate': 0.0001415929203539823, 'epoch': 3.61}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3340/4620 [12:43:55<4:32:11, 12.76s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3341/4620 [12:44:09<4:42:08, 13.24s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3342/4620 [12:44:22<4:38:36, 13.08s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3343/4620 [12:44:34<4:31:19, 12.75s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3344/4620 [12:44:46<4:28:33, 12.63s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3345/4620 [12:45:00<4:36:01, 12.99s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3346/4620 [12:45:12<4:29:08, 12.68s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3347/4620 [12:45:24<4:27:24, 12.60s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3348/4620 [12:45:36<4:25:42, 12.53s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3349/4620 [12:45:49<4:24:24, 12.48s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3350/4620 [12:46:03<4:35:38, 13.02s/it]                                                        {'loss': 0.0493, 'grad_norm': 0.1650390625, 'learning_rate': 0.0001404867256637168, 'epoch': 3.63}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3350/4620 [12:46:03<4:35:38, 13.02s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3351/4620 [12:46:16<4:36:58, 13.10s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3352/4620 [12:46:30<4:37:19, 13.12s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3353/4620 [12:46:41<4:29:13, 12.75s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3354/4620 [12:46:56<4:40:16, 13.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3355/4620 [12:47:08<4:33:30, 12.97s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3356/4620 [12:47:21<4:28:54, 12.76s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3357/4620 [12:47:33<4:25:54, 12.63s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3358/4620 [12:47:46<4:26:31, 12.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3359/4620 [12:47:58<4:27:01, 12.71s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3360/4620 [12:48:11<4:26:34, 12.69s/it]                                                        {'loss': 0.0518, 'grad_norm': 0.337890625, 'learning_rate': 0.00013938053097345133, 'epoch': 3.64}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3360/4620 [12:48:11<4:26:34, 12.69s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3361/4620 [12:48:27<4:44:41, 13.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3362/4620 [12:48:39<4:38:57, 13.30s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3363/4620 [12:48:51<4:30:57, 12.93s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3364/4620 [12:49:06<4:39:04, 13.33s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3365/4620 [12:49:18<4:32:34, 13.03s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3366/4620 [12:49:30<4:27:20, 12.79s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3367/4620 [12:49:43<4:24:32, 12.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3368/4620 [12:49:55<4:24:08, 12.66s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3369/4620 [12:50:08<4:21:39, 12.55s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3370/4620 [12:50:20<4:20:18, 12.49s/it]                                                        {'loss': 0.0495, 'grad_norm': 0.1279296875, 'learning_rate': 0.00013827433628318584, 'epoch': 3.65}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3370/4620 [12:50:20<4:20:18, 12.49s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3371/4620 [12:50:34<4:30:36, 13.00s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3372/4620 [12:50:47<4:32:23, 13.10s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3373/4620 [12:51:00<4:31:30, 13.06s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3374/4620 [12:51:12<4:24:11, 12.72s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3375/4620 [12:51:25<4:21:12, 12.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3376/4620 [12:51:37<4:20:36, 12.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3377/4620 [12:51:50<4:20:52, 12.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3378/4620 [12:52:02<4:19:27, 12.53s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3379/4620 [12:52:15<4:20:33, 12.60s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3380/4620 [12:52:28<4:21:52, 12.67s/it]                                                        {'loss': 0.0609, 'grad_norm': 0.205078125, 'learning_rate': 0.00013716814159292034, 'epoch': 3.66}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3380/4620 [12:52:28<4:21:52, 12.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3381/4620 [12:52:42<4:28:39, 13.01s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3382/4620 [12:52:55<4:29:00, 13.04s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3383/4620 [12:53:08<4:30:02, 13.10s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3384/4620 [12:53:20<4:22:54, 12.76s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3385/4620 [12:53:32<4:18:41, 12.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3386/4620 [12:53:44<4:17:28, 12.52s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3387/4620 [12:53:57<4:19:14, 12.62s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3388/4620 [12:54:10<4:19:07, 12.62s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3389/4620 [12:54:22<4:18:17, 12.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3390/4620 [12:54:35<4:17:19, 12.55s/it]                                                        {'loss': 0.0741, 'grad_norm': 0.259765625, 'learning_rate': 0.00013606194690265485, 'epoch': 3.67}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3390/4620 [12:54:35<4:17:19, 12.55s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3391/4620 [12:54:49<4:27:20, 13.05s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3392/4620 [12:55:03<4:31:17, 13.26s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3393/4620 [12:55:15<4:26:19, 13.02s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3394/4620 [12:55:27<4:18:32, 12.65s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3395/4620 [12:55:40<4:16:55, 12.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3396/4620 [12:55:52<4:17:19, 12.61s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3397/4620 [12:56:05<4:17:39, 12.64s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3398/4620 [12:56:17<4:17:02, 12.62s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3399/4620 [12:56:30<4:15:13, 12.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3400/4620 [12:56:42<4:13:44, 12.48s/it]                                                        {'loss': 0.0625, 'grad_norm': 0.361328125, 'learning_rate': 0.00013495575221238938, 'epoch': 3.68}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3400/4620 [12:56:42<4:13:44, 12.48s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:09<01:09,  4.95s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:11<00:46,  3.59s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:13<00:34,  2.84s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:15<00:27,  2.49s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:16<00:22,  2.25s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:18<00:18,  2.01s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:19<00:14,  1.87s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:21<00:12,  1.75s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.70s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:24<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:26<00:06,  1.71s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:27<00:05,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:29<00:03,  1.65s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:31<00:01,  1.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06925498694181442, 'eval_runtime': 80.3387, 'eval_samples_per_second': 6.224, 'eval_steps_per_second': 0.199, 'epoch': 3.68}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3400/4620 [12:58:03<4:13:44, 12.48s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:33<00:00,  1.86s/it][A
                                               [A 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3401/4620 [12:58:16<12:30:50, 36.96s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3402/4620 [12:58:29<10:01:15, 29.62s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3403/4620 [12:58:40<8:08:25, 24.08s/it]  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3404/4620 [12:58:52<6:55:22, 20.50s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3405/4620 [12:59:05<6:08:47, 18.21s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3406/4620 [12:59:18<5:34:56, 16.55s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3407/4620 [12:59:30<5:12:08, 15.44s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3408/4620 [12:59:43<4:55:26, 14.63s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3409/4620 [12:59:56<4:46:38, 14.20s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3410/4620 [13:00:09<4:39:12, 13.84s/it]                                                        {'loss': 0.0502, 'grad_norm': 0.205078125, 'learning_rate': 0.0001338495575221239, 'epoch': 3.69}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3410/4620 [13:00:09<4:39:12, 13.84s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3411/4620 [13:00:25<4:48:37, 14.32s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3412/4620 [13:00:39<4:50:18, 14.42s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3413/4620 [13:00:51<4:31:15, 13.48s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3414/4620 [13:01:03<4:21:57, 13.03s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3415/4620 [13:01:15<4:19:58, 12.95s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3416/4620 [13:01:28<4:19:20, 12.92s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3417/4620 [13:01:41<4:18:12, 12.88s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3418/4620 [13:01:54<4:17:16, 12.84s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3419/4620 [13:02:07<4:16:04, 12.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3420/4620 [13:02:20<4:21:35, 13.08s/it]                                                        {'loss': 0.06, 'grad_norm': 0.240234375, 'learning_rate': 0.00013274336283185842, 'epoch': 3.7}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3420/4620 [13:02:20<4:21:35, 13.08s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3421/4620 [13:02:34<4:26:22, 13.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3422/4620 [13:02:48<4:28:13, 13.43s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3423/4620 [13:03:00<4:18:51, 12.98s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3424/4620 [13:03:12<4:14:02, 12.74s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3425/4620 [13:03:25<4:13:12, 12.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3426/4620 [13:03:37<4:10:42, 12.60s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3427/4620 [13:03:49<4:08:31, 12.50s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3428/4620 [13:04:02<4:08:02, 12.49s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3429/4620 [13:04:15<4:11:05, 12.65s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3430/4620 [13:04:29<4:22:51, 13.25s/it]                                                        {'loss': 0.0566, 'grad_norm': 0.1455078125, 'learning_rate': 0.00013163716814159292, 'epoch': 3.71}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3430/4620 [13:04:29<4:22:51, 13.25s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3431/4620 [13:04:43<4:23:12, 13.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3432/4620 [13:04:56<4:23:09, 13.29s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3433/4620 [13:05:08<4:13:56, 12.84s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3434/4620 [13:05:20<4:07:48, 12.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3435/4620 [13:05:32<4:05:39, 12.44s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3436/4620 [13:05:44<4:06:18, 12.48s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3437/4620 [13:05:57<4:04:54, 12.42s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3438/4620 [13:06:10<4:07:00, 12.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3439/4620 [13:06:24<4:15:53, 13.00s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3440/4620 [13:06:36<4:11:05, 12.77s/it]                                                        {'loss': 0.062, 'grad_norm': 0.2236328125, 'learning_rate': 0.00013053097345132745, 'epoch': 3.72}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3440/4620 [13:06:36<4:11:05, 12.77s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3441/4620 [13:06:51<4:21:56, 13.33s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3442/4620 [13:07:04<4:21:07, 13.30s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3443/4620 [13:07:16<4:13:31, 12.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3444/4620 [13:07:28<4:09:49, 12.75s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3445/4620 [13:07:41<4:08:08, 12.67s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3446/4620 [13:07:53<4:08:46, 12.71s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3447/4620 [13:08:06<4:10:16, 12.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3448/4620 [13:08:21<4:18:33, 13.24s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3449/4620 [13:08:33<4:12:06, 12.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3450/4620 [13:08:46<4:15:49, 13.12s/it]                                                        {'loss': 0.0667, 'grad_norm': 0.3125, 'learning_rate': 0.00012942477876106196, 'epoch': 3.73}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3450/4620 [13:08:47<4:15:49, 13.12s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3451/4620 [13:08:59<4:09:23, 12.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3452/4620 [13:09:12<4:12:36, 12.98s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3453/4620 [13:09:24<4:07:45, 12.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3454/4620 [13:09:36<4:05:29, 12.63s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3455/4620 [13:09:50<4:07:39, 12.75s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3456/4620 [13:10:02<4:07:28, 12.76s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3457/4620 [13:10:15<4:08:21, 12.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3458/4620 [13:10:30<4:17:30, 13.30s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3459/4620 [13:10:42<4:09:56, 12.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3460/4620 [13:10:56<4:15:25, 13.21s/it]                                                        {'loss': 0.0462, 'grad_norm': 0.1552734375, 'learning_rate': 0.00012831858407079646, 'epoch': 3.74}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3460/4620 [13:10:56<4:15:25, 13.21s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3461/4620 [13:11:08<4:09:58, 12.94s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3462/4620 [13:11:21<4:12:43, 13.09s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3463/4620 [13:11:34<4:07:32, 12.84s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3464/4620 [13:11:46<4:03:02, 12.61s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3465/4620 [13:11:58<4:02:40, 12.61s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3466/4620 [13:12:11<4:02:14, 12.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3467/4620 [13:12:25<4:10:29, 13.04s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3468/4620 [13:12:37<4:05:52, 12.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3469/4620 [13:12:51<4:12:20, 13.15s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3470/4620 [13:13:03<4:06:02, 12.84s/it]                                                        {'loss': 0.0605, 'grad_norm': 0.1845703125, 'learning_rate': 0.00012721238938053097, 'epoch': 3.76}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3470/4620 [13:13:03<4:06:02, 12.84s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3471/4620 [13:13:16<4:03:04, 12.69s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3472/4620 [13:13:29<4:09:19, 13.03s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3473/4620 [13:13:41<4:03:12, 12.72s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3474/4620 [13:13:54<4:00:18, 12.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3475/4620 [13:14:06<4:00:53, 12.62s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3476/4620 [13:14:19<4:01:57, 12.69s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3477/4620 [13:14:34<4:12:37, 13.26s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3478/4620 [13:14:46<4:04:26, 12.84s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3479/4620 [13:14:59<4:08:52, 13.09s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3480/4620 [13:15:11<4:02:12, 12.75s/it]                                                        {'loss': 0.0562, 'grad_norm': 0.349609375, 'learning_rate': 0.0001261061946902655, 'epoch': 3.77}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3480/4620 [13:15:11<4:02:12, 12.75s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3481/4620 [13:15:24<3:59:07, 12.60s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3482/4620 [13:15:37<4:06:04, 12.97s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3483/4620 [13:15:49<3:59:35, 12.64s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3484/4620 [13:16:01<3:55:34, 12.44s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3485/4620 [13:16:14<3:54:54, 12.42s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3486/4620 [13:16:27<4:02:14, 12.82s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3487/4620 [13:16:39<3:56:56, 12.55s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3488/4620 [13:16:53<4:05:21, 13.00s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3489/4620 [13:17:05<3:58:38, 12.66s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3490/4620 [13:17:18<3:57:31, 12.61s/it]                                                        {'loss': 0.059, 'grad_norm': 0.3359375, 'learning_rate': 0.000125, 'epoch': 3.78}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3490/4620 [13:17:18<3:57:31, 12.61s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3491/4620 [13:17:31<4:03:17, 12.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3492/4620 [13:17:44<3:58:38, 12.69s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3493/4620 [13:17:56<3:57:34, 12.65s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3494/4620 [13:18:09<3:57:08, 12.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3495/4620 [13:18:21<3:56:32, 12.62s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3496/4620 [13:18:36<4:08:28, 13.26s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3497/4620 [13:18:49<4:07:07, 13.20s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3498/4620 [13:19:02<4:06:39, 13.19s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3499/4620 [13:19:14<3:59:00, 12.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3500/4620 [13:19:26<3:55:53, 12.64s/it]                                                        {'loss': 0.0442, 'grad_norm': 0.1572265625, 'learning_rate': 0.0001238938053097345, 'epoch': 3.79}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3500/4620 [13:19:26<3:55:53, 12.64s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:49,  3.54s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:08<00:36,  2.81s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:28,  2.35s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:22,  2.05s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.93s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:17,  1.93s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.80s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.75s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.69s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.65s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.66s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:05,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.65s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.89s/it][A                                                        
                                               [A{'eval_loss': 0.06941039860248566, 'eval_runtime': 73.6851, 'eval_samples_per_second': 6.786, 'eval_steps_per_second': 0.217, 'epoch': 3.79}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3500/4620 [13:20:40<3:55:53, 12.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.89s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3501/4620 [13:20:54<10:58:04, 35.29s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3502/4620 [13:21:06<8:43:41, 28.11s/it]  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3503/4620 [13:21:18<7:13:04, 23.26s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3504/4620 [13:21:30<6:10:49, 19.94s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3505/4620 [13:21:44<5:37:42, 18.17s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3506/4620 [13:21:56<5:01:56, 16.26s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3507/4620 [13:22:10<4:47:30, 15.50s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3508/4620 [13:22:23<4:35:55, 14.89s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3509/4620 [13:22:35<4:19:30, 14.01s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3510/4620 [13:22:48<4:11:37, 13.60s/it]                                                        {'loss': 0.0532, 'grad_norm': 0.1708984375, 'learning_rate': 0.00012278761061946904, 'epoch': 3.8}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3510/4620 [13:22:48<4:11:37, 13.60s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3511/4620 [13:23:02<4:13:55, 13.74s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3512/4620 [13:23:14<4:05:02, 13.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3513/4620 [13:23:26<3:59:35, 12.99s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3514/4620 [13:23:39<3:56:14, 12.82s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3515/4620 [13:23:53<4:05:22, 13.32s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3516/4620 [13:24:05<3:56:55, 12.88s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3517/4620 [13:24:19<4:01:31, 13.14s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3518/4620 [13:24:32<4:01:56, 13.17s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3519/4620 [13:24:44<3:54:34, 12.78s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3520/4620 [13:24:57<3:58:31, 13.01s/it]                                                        {'loss': 0.0497, 'grad_norm': 0.1982421875, 'learning_rate': 0.00012168141592920354, 'epoch': 3.81}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3520/4620 [13:24:57<3:58:31, 13.01s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3521/4620 [13:25:09<3:53:11, 12.73s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3522/4620 [13:25:22<3:50:34, 12.60s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3523/4620 [13:25:34<3:50:41, 12.62s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3524/4620 [13:25:47<3:48:36, 12.51s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3525/4620 [13:26:01<3:55:58, 12.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3526/4620 [13:26:14<3:57:50, 13.04s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3527/4620 [13:26:26<3:53:21, 12.81s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3528/4620 [13:26:40<4:00:19, 13.20s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3529/4620 [13:26:52<3:54:15, 12.88s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3530/4620 [13:27:06<3:57:46, 13.09s/it]                                                        {'loss': 0.0647, 'grad_norm': 0.2177734375, 'learning_rate': 0.00012057522123893805, 'epoch': 3.82}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3530/4620 [13:27:06<3:57:46, 13.09s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3531/4620 [13:27:19<3:55:01, 12.95s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3532/4620 [13:27:31<3:51:58, 12.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3533/4620 [13:27:44<3:50:23, 12.72s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3534/4620 [13:27:56<3:50:36, 12.74s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3535/4620 [13:28:10<3:56:23, 13.07s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3536/4620 [13:28:22<3:49:20, 12.69s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3537/4620 [13:28:36<3:55:47, 13.06s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3538/4620 [13:28:49<3:55:20, 13.05s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3539/4620 [13:29:01<3:49:15, 12.72s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3540/4620 [13:29:15<3:53:48, 12.99s/it]                                                        {'loss': 0.0497, 'grad_norm': 0.1181640625, 'learning_rate': 0.00011946902654867258, 'epoch': 3.83}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3540/4620 [13:29:15<3:53:48, 12.99s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3541/4620 [13:29:27<3:48:27, 12.70s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3542/4620 [13:29:39<3:47:23, 12.66s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3543/4620 [13:29:52<3:46:52, 12.64s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3544/4620 [13:30:05<3:48:06, 12.72s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3545/4620 [13:30:19<3:55:35, 13.15s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3546/4620 [13:30:31<3:48:44, 12.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3547/4620 [13:30:45<3:54:59, 13.14s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3548/4620 [13:30:58<3:56:59, 13.26s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3549/4620 [13:31:10<3:49:56, 12.88s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3550/4620 [13:31:24<3:56:11, 13.24s/it]                                                        {'loss': 0.0569, 'grad_norm': 0.2099609375, 'learning_rate': 0.00011836283185840708, 'epoch': 3.84}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3550/4620 [13:31:24<3:56:11, 13.24s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3551/4620 [13:31:36<3:48:03, 12.80s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3552/4620 [13:31:48<3:45:05, 12.65s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3553/4620 [13:32:01<3:45:16, 12.67s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3554/4620 [13:32:15<3:51:21, 13.02s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3555/4620 [13:32:27<3:45:32, 12.71s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3556/4620 [13:32:39<3:42:55, 12.57s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3557/4620 [13:32:53<3:51:14, 13.05s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3558/4620 [13:33:07<3:53:50, 13.21s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3559/4620 [13:33:20<3:51:39, 13.10s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3560/4620 [13:33:32<3:46:53, 12.84s/it]                                                        {'loss': 0.0612, 'grad_norm': 0.2216796875, 'learning_rate': 0.0001172566371681416, 'epoch': 3.85}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3560/4620 [13:33:32<3:46:53, 12.84s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3561/4620 [13:33:45<3:49:23, 13.00s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3562/4620 [13:33:57<3:44:13, 12.72s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3563/4620 [13:34:10<3:42:13, 12.61s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3564/4620 [13:34:24<3:52:10, 13.19s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3565/4620 [13:34:36<3:44:36, 12.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3566/4620 [13:34:48<3:42:09, 12.65s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3567/4620 [13:35:01<3:40:20, 12.55s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3568/4620 [13:35:15<3:50:29, 13.15s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3569/4620 [13:35:29<3:54:35, 13.39s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3570/4620 [13:35:41<3:46:18, 12.93s/it]                                                        {'loss': 0.0673, 'grad_norm': 0.1796875, 'learning_rate': 0.00011615044247787611, 'epoch': 3.86}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3570/4620 [13:35:41<3:46:18, 12.93s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3571/4620 [13:35:54<3:43:53, 12.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3572/4620 [13:36:07<3:44:40, 12.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3573/4620 [13:36:21<3:51:45, 13.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3574/4620 [13:36:33<3:43:47, 12.84s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3575/4620 [13:36:45<3:41:13, 12.70s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3576/4620 [13:36:58<3:40:23, 12.67s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3577/4620 [13:37:10<3:39:44, 12.64s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3578/4620 [13:37:24<3:45:43, 13.00s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3579/4620 [13:37:36<3:40:34, 12.71s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3580/4620 [13:37:49<3:38:32, 12.61s/it]                                                        {'loss': 0.0596, 'grad_norm': 0.42578125, 'learning_rate': 0.00011504424778761063, 'epoch': 3.87}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3580/4620 [13:37:49<3:38:32, 12.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3581/4620 [13:38:02<3:40:28, 12.73s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3582/4620 [13:38:14<3:39:03, 12.66s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3583/4620 [13:38:28<3:45:31, 13.05s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3584/4620 [13:38:40<3:39:25, 12.71s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3585/4620 [13:38:52<3:37:51, 12.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3586/4620 [13:39:05<3:37:56, 12.65s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3587/4620 [13:39:18<3:37:04, 12.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3588/4620 [13:39:32<3:44:18, 13.04s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3589/4620 [13:39:44<3:39:09, 12.75s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3590/4620 [13:39:56<3:36:44, 12.63s/it]                                                        {'loss': 0.0538, 'grad_norm': 0.1767578125, 'learning_rate': 0.00011393805309734513, 'epoch': 3.89}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3590/4620 [13:39:56<3:36:44, 12.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3591/4620 [13:40:09<3:36:38, 12.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3592/4620 [13:40:23<3:45:15, 13.15s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3593/4620 [13:40:35<3:38:45, 12.78s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3594/4620 [13:40:47<3:35:16, 12.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3595/4620 [13:40:59<3:33:13, 12.48s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3596/4620 [13:41:12<3:32:53, 12.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3597/4620 [13:41:26<3:41:50, 13.01s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3598/4620 [13:41:39<3:43:02, 13.09s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3599/4620 [13:41:51<3:32:58, 12.52s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3600/4620 [13:42:03<3:31:55, 12.47s/it]                                                        {'loss': 0.05, 'grad_norm': 0.1953125, 'learning_rate': 0.00011283185840707965, 'epoch': 3.9}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3600/4620 [13:42:03<3:31:55, 12.47s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:52,  3.75s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:38,  2.94s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:29,  2.43s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.12s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.92s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.82s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:13,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:11,  1.66s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.69s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.66s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.92s/it][A                                                        
                                               [A{'eval_loss': 0.06833980232477188, 'eval_runtime': 91.8769, 'eval_samples_per_second': 5.442, 'eval_steps_per_second': 0.174, 'epoch': 3.9}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3600/4620 [13:43:35<3:31:55, 12.47s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.92s/it][A
                                               [A 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3601/4620 [13:43:45<11:09:53, 39.44s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3602/4620 [13:43:59<8:57:04, 31.65s/it]  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3603/4620 [13:44:11<7:15:32, 25.70s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3604/4620 [13:44:23<6:07:03, 21.68s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3605/4620 [13:44:36<5:21:45, 19.02s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3606/4620 [13:44:48<4:49:41, 17.14s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3607/4620 [13:45:05<4:44:49, 16.87s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3608/4620 [13:45:18<4:25:04, 15.72s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3609/4620 [13:45:29<4:03:45, 14.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3610/4620 [13:45:41<3:51:11, 13.73s/it]                                                        {'loss': 0.0634, 'grad_norm': 0.279296875, 'learning_rate': 0.00011172566371681415, 'epoch': 3.91}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3610/4620 [13:45:41<3:51:11, 13.73s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3611/4620 [13:45:56<3:54:20, 13.93s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3612/4620 [13:46:08<3:45:12, 13.41s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3613/4620 [13:46:20<3:39:09, 13.06s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3614/4620 [13:46:33<3:35:50, 12.87s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3615/4620 [13:46:45<3:33:30, 12.75s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3616/4620 [13:46:59<3:39:26, 13.11s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3617/4620 [13:47:12<3:40:45, 13.21s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3618/4620 [13:47:26<3:41:31, 13.27s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3619/4620 [13:47:37<3:32:53, 12.76s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3620/4620 [13:47:50<3:30:53, 12.65s/it]                                                        {'loss': 0.0589, 'grad_norm': 0.123046875, 'learning_rate': 0.00011061946902654868, 'epoch': 3.92}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3620/4620 [13:47:50<3:30:53, 12.65s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3621/4620 [13:48:04<3:40:13, 13.23s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3622/4620 [13:48:17<3:35:25, 12.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3623/4620 [13:48:29<3:33:23, 12.84s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3624/4620 [13:48:42<3:32:15, 12.79s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3625/4620 [13:48:55<3:32:59, 12.84s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3626/4620 [13:49:09<3:37:05, 13.10s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3627/4620 [13:49:22<3:36:55, 13.11s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3628/4620 [13:49:35<3:38:42, 13.23s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3629/4620 [13:49:47<3:33:32, 12.93s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3630/4620 [13:50:01<3:37:30, 13.18s/it]                                                        {'loss': 0.0586, 'grad_norm': 0.396484375, 'learning_rate': 0.00010951327433628319, 'epoch': 3.93}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3630/4620 [13:50:01<3:37:30, 13.18s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3631/4620 [13:50:14<3:33:04, 12.93s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3632/4620 [13:50:26<3:31:19, 12.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3633/4620 [13:50:39<3:30:40, 12.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3634/4620 [13:50:51<3:28:13, 12.67s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3635/4620 [13:51:04<3:28:25, 12.70s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3636/4620 [13:51:18<3:34:19, 13.07s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3637/4620 [13:51:30<3:30:26, 12.85s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3638/4620 [13:51:44<3:35:00, 13.14s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3639/4620 [13:51:56<3:28:59, 12.78s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3640/4620 [13:52:10<3:34:15, 13.12s/it]                                                        {'loss': 0.0517, 'grad_norm': 0.2021484375, 'learning_rate': 0.00010840707964601771, 'epoch': 3.94}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3640/4620 [13:52:10<3:34:15, 13.12s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3641/4620 [13:52:22<3:28:24, 12.77s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3642/4620 [13:52:34<3:26:31, 12.67s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3643/4620 [13:52:47<3:27:08, 12.72s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3644/4620 [13:53:00<3:25:07, 12.61s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3645/4620 [13:53:12<3:22:36, 12.47s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3646/4620 [13:53:26<3:30:39, 12.98s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3647/4620 [13:53:39<3:32:04, 13.08s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3648/4620 [13:53:52<3:32:32, 13.12s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3649/4620 [13:54:06<3:34:28, 13.25s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3650/4620 [13:54:18<3:26:46, 12.79s/it]                                                        {'loss': 0.0508, 'grad_norm': 0.2431640625, 'learning_rate': 0.00010730088495575221, 'epoch': 3.95}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3650/4620 [13:54:18<3:26:46, 12.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3651/4620 [13:54:30<3:24:43, 12.68s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3652/4620 [13:54:42<3:22:44, 12.57s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3653/4620 [13:54:55<3:22:02, 12.54s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3654/4620 [13:55:08<3:23:04, 12.61s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3655/4620 [13:55:20<3:23:04, 12.63s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3656/4620 [13:55:34<3:30:13, 13.08s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3657/4620 [13:55:48<3:31:59, 13.21s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3658/4620 [13:56:01<3:32:59, 13.28s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3659/4620 [13:56:13<3:27:03, 12.93s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3660/4620 [13:56:26<3:23:54, 12.74s/it]                                                        {'loss': 0.0523, 'grad_norm': 0.09619140625, 'learning_rate': 0.00010619469026548673, 'epoch': 3.96}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3660/4620 [13:56:26<3:23:54, 12.74s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3661/4620 [13:56:38<3:22:07, 12.65s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3662/4620 [13:56:51<3:20:51, 12.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3663/4620 [13:57:04<3:22:11, 12.68s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3664/4620 [13:57:16<3:22:18, 12.70s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3665/4620 [13:57:29<3:21:40, 12.67s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3666/4620 [13:57:42<3:26:01, 12.96s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3667/4620 [13:57:56<3:27:07, 13.04s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3668/4620 [13:58:08<3:22:41, 12.77s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3669/4620 [13:58:22<3:27:46, 13.11s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3670/4620 [13:58:34<3:24:08, 12.89s/it]                                                        {'loss': 0.0668, 'grad_norm': 0.345703125, 'learning_rate': 0.00010508849557522123, 'epoch': 3.97}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3670/4620 [13:58:34<3:24:08, 12.89s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3671/4620 [13:58:47<3:22:19, 12.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3672/4620 [13:58:59<3:21:30, 12.75s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3673/4620 [13:59:12<3:20:03, 12.68s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3674/4620 [13:59:25<3:21:32, 12.78s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3675/4620 [13:59:38<3:22:42, 12.87s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3676/4620 [13:59:52<3:27:06, 13.16s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3677/4620 [14:00:04<3:22:54, 12.91s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3678/4620 [14:00:18<3:27:39, 13.23s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3679/4620 [14:00:31<3:27:37, 13.24s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3680/4620 [14:00:41<3:10:49, 12.18s/it]                                                        {'loss': 0.0505, 'grad_norm': 0.2236328125, 'learning_rate': 0.00010398230088495575, 'epoch': 3.98}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3680/4620 [14:00:41<3:10:49, 12.18s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3681/4620 [14:00:50<2:54:33, 11.15s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3682/4620 [14:00:59<2:42:54, 10.42s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3683/4620 [14:01:07<2:34:43,  9.91s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3684/4620 [14:01:16<2:29:22,  9.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3685/4620 [14:01:25<2:26:34,  9.41s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3686/4620 [14:01:34<2:24:17,  9.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3687/4620 [14:01:43<2:21:42,  9.11s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3688/4620 [14:01:52<2:20:25,  9.04s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3689/4620 [14:02:01<2:21:20,  9.11s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3690/4620 [14:02:10<2:19:35,  9.01s/it]                                                        {'loss': 0.0607, 'grad_norm': 0.30078125, 'learning_rate': 0.00010287610619469026, 'epoch': 3.99}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3690/4620 [14:02:10<2:19:35,  9.01s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3691/4620 [14:02:18<2:18:20,  8.93s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3692/4620 [14:02:27<2:17:01,  8.86s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3693/4620 [14:02:36<2:15:53,  8.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3694/4620 [14:02:45<2:15:25,  8.77s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3695/4620 [14:02:54<2:18:04,  8.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3696/4620 [14:03:05<2:26:46,  9.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3697/4620 [14:05:21<12:12:05, 47.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3698/4620 [14:05:51<10:51:46, 42.41s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3699/4620 [14:06:13<9:14:55, 36.15s/it]  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3700/4620 [14:06:30<7:48:20, 30.54s/it]                                                        {'loss': 0.0492, 'grad_norm': 0.14453125, 'learning_rate': 0.00010176991150442479, 'epoch': 4.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3700/4620 [14:06:31<7:48:20, 30.54s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:36,  2.62s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:31,  2.41s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:25,  2.15s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.93s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.85s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:14<00:16,  1.84s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:14,  1.76s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:11,  1.69s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.63s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.62s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.61s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:05,  1.69s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.69s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06972998380661011, 'eval_runtime': 69.3511, 'eval_samples_per_second': 7.21, 'eval_steps_per_second': 0.231, 'epoch': 4.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3700/4620 [14:07:40<7:48:20, 30.54s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A
                                               [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3701/4620 [14:07:50<11:33:11, 45.26s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3702/4620 [14:08:02<9:01:18, 35.38s/it]  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3703/4620 [14:08:15<7:15:26, 28.49s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3704/4620 [14:08:28<6:04:16, 23.86s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3705/4620 [14:08:42<5:21:06, 21.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3706/4620 [14:08:54<4:39:13, 18.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3707/4620 [14:09:08<4:16:39, 16.87s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3708/4620 [14:09:21<3:59:58, 15.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3709/4620 [14:09:34<3:48:13, 15.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3710/4620 [14:09:46<3:34:44, 14.16s/it]                                                        {'loss': 0.057, 'grad_norm': 0.2080078125, 'learning_rate': 0.0001006637168141593, 'epoch': 4.02}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3710/4620 [14:09:47<3:34:44, 14.16s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3711/4620 [14:09:59<3:26:44, 13.65s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3712/4620 [14:10:12<3:22:17, 13.37s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3713/4620 [14:10:25<3:20:18, 13.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3714/4620 [14:10:37<3:16:38, 13.02s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3715/4620 [14:10:51<3:21:03, 13.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3716/4620 [14:11:03<3:15:57, 13.01s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3717/4620 [14:11:17<3:18:37, 13.20s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3718/4620 [14:11:30<3:18:58, 13.24s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3719/4620 [14:11:44<3:21:36, 13.43s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3720/4620 [14:11:57<3:18:03, 13.20s/it]                                                        {'loss': 0.0554, 'grad_norm': 0.451171875, 'learning_rate': 9.955752212389381e-05, 'epoch': 4.03}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3720/4620 [14:11:57<3:18:03, 13.20s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3721/4620 [14:12:10<3:15:25, 13.04s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3722/4620 [14:12:22<3:12:34, 12.87s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3723/4620 [14:12:35<3:12:57, 12.91s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3724/4620 [14:12:48<3:13:07, 12.93s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3725/4620 [14:13:02<3:19:07, 13.35s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3726/4620 [14:13:15<3:17:19, 13.24s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3727/4620 [14:13:28<3:16:18, 13.19s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3728/4620 [14:13:42<3:17:01, 13.25s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3729/4620 [14:13:54<3:11:05, 12.87s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3730/4620 [14:14:06<3:09:13, 12.76s/it]                                                        {'loss': 0.0496, 'grad_norm': 0.1923828125, 'learning_rate': 9.845132743362832e-05, 'epoch': 4.04}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3730/4620 [14:14:06<3:09:13, 12.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3731/4620 [14:14:19<3:08:19, 12.71s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3732/4620 [14:14:31<3:06:14, 12.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3733/4620 [14:14:44<3:05:57, 12.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3734/4620 [14:14:56<3:06:07, 12.60s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3735/4620 [14:15:10<3:11:43, 13.00s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3736/4620 [14:15:22<3:07:32, 12.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3737/4620 [14:15:37<3:13:39, 13.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3738/4620 [14:15:50<3:14:40, 13.24s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3739/4620 [14:16:02<3:07:42, 12.78s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3740/4620 [14:16:14<3:05:10, 12.63s/it]                                                        {'loss': 0.0576, 'grad_norm': 0.25, 'learning_rate': 9.734513274336283e-05, 'epoch': 4.05}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3740/4620 [14:16:14<3:05:10, 12.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3741/4620 [14:16:27<3:06:44, 12.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3742/4620 [14:16:40<3:06:44, 12.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3743/4620 [14:16:52<3:04:32, 12.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3744/4620 [14:17:05<3:05:35, 12.71s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3745/4620 [14:17:20<3:13:13, 13.25s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3746/4620 [14:17:33<3:11:43, 13.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3747/4620 [14:17:46<3:11:53, 13.19s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3748/4620 [14:17:59<3:12:11, 13.22s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3749/4620 [14:18:11<3:06:27, 12.84s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3750/4620 [14:18:23<3:03:37, 12.66s/it]                                                        {'loss': 0.0581, 'grad_norm': 0.314453125, 'learning_rate': 9.623893805309734e-05, 'epoch': 4.06}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3750/4620 [14:18:23<3:03:37, 12.66s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3751/4620 [14:18:36<3:03:33, 12.67s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3752/4620 [14:18:49<3:04:34, 12.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3753/4620 [14:19:02<3:03:58, 12.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3754/4620 [14:19:14<3:03:58, 12.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3755/4620 [14:19:28<3:09:39, 13.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3756/4620 [14:19:41<3:08:43, 13.11s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3757/4620 [14:19:55<3:08:28, 13.10s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3758/4620 [14:20:08<3:07:53, 13.08s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3759/4620 [14:20:20<3:04:01, 12.82s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3760/4620 [14:20:32<3:01:41, 12.68s/it]                                                        {'loss': 0.0568, 'grad_norm': 0.306640625, 'learning_rate': 9.513274336283186e-05, 'epoch': 4.07}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3760/4620 [14:20:32<3:01:41, 12.68s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3761/4620 [14:20:45<3:03:09, 12.79s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3762/4620 [14:20:58<3:02:13, 12.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3763/4620 [14:21:10<3:00:58, 12.67s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3764/4620 [14:21:23<3:00:38, 12.66s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3765/4620 [14:21:39<3:13:32, 13.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3766/4620 [14:21:52<3:10:28, 13.38s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3767/4620 [14:22:04<3:05:01, 13.01s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3768/4620 [14:22:17<3:07:35, 13.21s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3769/4620 [14:22:30<3:03:08, 12.91s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3770/4620 [14:22:42<3:00:34, 12.75s/it]                                                        {'loss': 0.0475, 'grad_norm': 0.263671875, 'learning_rate': 9.402654867256636e-05, 'epoch': 4.08}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3770/4620 [14:22:42<3:00:34, 12.75s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3771/4620 [14:22:55<2:59:48, 12.71s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3772/4620 [14:23:07<2:58:21, 12.62s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3773/4620 [14:23:20<2:58:01, 12.61s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3774/4620 [14:23:32<2:57:53, 12.62s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3775/4620 [14:23:47<3:05:37, 13.18s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3776/4620 [14:24:00<3:05:40, 13.20s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3777/4620 [14:24:12<3:00:02, 12.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3778/4620 [14:24:25<3:02:54, 13.03s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3779/4620 [14:24:38<2:58:32, 12.74s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3780/4620 [14:24:50<2:56:53, 12.64s/it]                                                        {'loss': 0.042, 'grad_norm': 0.2216796875, 'learning_rate': 9.29203539823009e-05, 'epoch': 4.09}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3780/4620 [14:24:50<2:56:53, 12.64s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3781/4620 [14:25:03<2:57:15, 12.68s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3782/4620 [14:25:15<2:56:50, 12.66s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3783/4620 [14:25:28<2:56:29, 12.65s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3784/4620 [14:25:42<3:02:03, 13.07s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3785/4620 [14:25:56<3:06:30, 13.40s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3786/4620 [14:26:08<2:59:10, 12.89s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3787/4620 [14:26:20<2:56:27, 12.71s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3788/4620 [14:26:33<2:56:37, 12.74s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3789/4620 [14:26:47<3:02:49, 13.20s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3790/4620 [14:26:59<2:57:55, 12.86s/it]                                                        {'loss': 0.0592, 'grad_norm': 0.1494140625, 'learning_rate': 9.18141592920354e-05, 'epoch': 4.1}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3790/4620 [14:26:59<2:57:55, 12.86s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3791/4620 [14:27:12<2:55:05, 12.67s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3792/4620 [14:27:25<2:56:20, 12.78s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3793/4620 [14:27:37<2:55:24, 12.73s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3794/4620 [14:27:51<3:00:15, 13.09s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3795/4620 [14:28:03<2:54:03, 12.66s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3796/4620 [14:28:15<2:52:24, 12.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3797/4620 [14:28:28<2:52:36, 12.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3798/4620 [14:28:40<2:51:50, 12.54s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3799/4620 [14:28:54<2:57:42, 12.99s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3800/4620 [14:29:06<2:53:58, 12.73s/it]                                                        {'loss': 0.0427, 'grad_norm': 0.08251953125, 'learning_rate': 9.070796460176992e-05, 'epoch': 4.11}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3800/4620 [14:29:06<2:53:58, 12.73s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:40,  2.91s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:32,  2.48s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:09<00:26,  2.17s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:11<00:22,  2.02s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:12<00:18,  1.89s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:14<00:15,  1.76s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.67s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:11,  1.63s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.64s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.66s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:22<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.58s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06981249898672104, 'eval_runtime': 66.7379, 'eval_samples_per_second': 7.492, 'eval_steps_per_second': 0.24, 'epoch': 4.11}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3800/4620 [14:30:13<2:53:58, 12.73s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.86s/it][A
                                               [A 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3801/4620 [14:30:24<7:19:20, 32.19s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3802/4620 [14:30:35<5:54:07, 25.97s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3803/4620 [14:30:49<5:02:44, 22.23s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3804/4620 [14:31:02<4:25:44, 19.54s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3805/4620 [14:31:16<4:01:13, 17.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3806/4620 [14:31:28<3:36:43, 15.97s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3807/4620 [14:31:40<3:21:13, 14.85s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3808/4620 [14:31:54<3:17:48, 14.62s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3809/4620 [14:32:06<3:07:48, 13.89s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3810/4620 [14:32:19<3:01:35, 13.45s/it]                                                        {'loss': 0.0478, 'grad_norm': 0.1279296875, 'learning_rate': 8.960176991150442e-05, 'epoch': 4.12}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3810/4620 [14:32:19<3:01:35, 13.45s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3811/4620 [14:32:31<2:57:18, 13.15s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3812/4620 [14:32:43<2:53:34, 12.89s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3813/4620 [14:32:57<2:57:41, 13.21s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3814/4620 [14:33:10<2:54:06, 12.96s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3815/4620 [14:33:24<2:58:45, 13.32s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3816/4620 [14:33:36<2:54:01, 12.99s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3817/4620 [14:33:48<2:52:01, 12.85s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3818/4620 [14:34:03<2:57:39, 13.29s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3819/4620 [14:34:15<2:54:56, 13.10s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3820/4620 [14:34:28<2:52:14, 12.92s/it]                                                        {'loss': 0.0513, 'grad_norm': 0.33203125, 'learning_rate': 8.849557522123894e-05, 'epoch': 4.13}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3820/4620 [14:34:28<2:52:14, 12.92s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3821/4620 [14:34:41<2:51:17, 12.86s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3822/4620 [14:34:55<2:56:01, 13.24s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3823/4620 [14:35:09<2:58:07, 13.41s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3824/4620 [14:35:21<2:52:00, 12.96s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3825/4620 [14:35:35<2:55:54, 13.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3826/4620 [14:35:47<2:52:49, 13.06s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3827/4620 [14:36:01<2:54:05, 13.17s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3828/4620 [14:36:13<2:49:26, 12.84s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3829/4620 [14:36:25<2:47:07, 12.68s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3830/4620 [14:36:38<2:47:59, 12.76s/it]                                                        {'loss': 0.0488, 'grad_norm': 0.1953125, 'learning_rate': 8.738938053097344e-05, 'epoch': 4.15}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3830/4620 [14:36:38<2:47:59, 12.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3831/4620 [14:36:51<2:48:08, 12.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3832/4620 [14:37:06<2:56:03, 13.41s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3833/4620 [14:37:18<2:50:46, 13.02s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3834/4620 [14:37:30<2:47:51, 12.81s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3835/4620 [14:37:44<2:52:28, 13.18s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3836/4620 [14:37:56<2:48:18, 12.88s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3837/4620 [14:38:10<2:52:40, 13.23s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3838/4620 [14:38:22<2:47:36, 12.86s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3839/4620 [14:38:35<2:46:09, 12.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3840/4620 [14:38:48<2:46:16, 12.79s/it]                                                        {'loss': 0.0541, 'grad_norm': 0.26171875, 'learning_rate': 8.628318584070796e-05, 'epoch': 4.16}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3840/4620 [14:38:48<2:46:16, 12.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3841/4620 [14:39:01<2:50:12, 13.11s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3842/4620 [14:39:14<2:46:41, 12.85s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3843/4620 [14:39:26<2:44:03, 12.67s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3844/4620 [14:39:38<2:43:00, 12.60s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3845/4620 [14:39:52<2:46:32, 12.89s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3846/4620 [14:40:06<2:49:42, 13.16s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3847/4620 [14:40:18<2:44:45, 12.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3848/4620 [14:40:30<2:42:09, 12.60s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3849/4620 [14:40:43<2:42:12, 12.62s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3850/4620 [14:40:57<2:48:55, 13.16s/it]                                                        {'loss': 0.0503, 'grad_norm': 0.23046875, 'learning_rate': 8.517699115044247e-05, 'epoch': 4.17}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3850/4620 [14:40:57<2:48:55, 13.16s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3851/4620 [14:41:09<2:44:58, 12.87s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3852/4620 [14:41:23<2:47:51, 13.11s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3853/4620 [14:41:35<2:43:30, 12.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3854/4620 [14:41:47<2:41:11, 12.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3855/4620 [14:42:01<2:45:23, 12.97s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3856/4620 [14:42:14<2:46:57, 13.11s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3857/4620 [14:42:26<2:41:37, 12.71s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3858/4620 [14:42:39<2:40:12, 12.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3859/4620 [14:42:51<2:39:18, 12.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3860/4620 [14:43:03<2:38:31, 12.52s/it]                                                        {'loss': 0.0539, 'grad_norm': 0.1865234375, 'learning_rate': 8.4070796460177e-05, 'epoch': 4.18}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3860/4620 [14:43:03<2:38:31, 12.52s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3861/4620 [14:43:18<2:45:17, 13.07s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3862/4620 [14:43:31<2:44:51, 13.05s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3863/4620 [14:43:42<2:39:50, 12.67s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3864/4620 [14:43:55<2:38:23, 12.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3865/4620 [14:44:09<2:43:51, 13.02s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3866/4620 [14:44:21<2:41:03, 12.82s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3867/4620 [14:44:34<2:38:59, 12.67s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3868/4620 [14:44:46<2:37:57, 12.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3869/4620 [14:44:59<2:39:39, 12.76s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3870/4620 [14:45:12<2:38:33, 12.68s/it]                                                        {'loss': 0.0539, 'grad_norm': 0.11181640625, 'learning_rate': 8.296460176991152e-05, 'epoch': 4.19}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3870/4620 [14:45:12<2:38:33, 12.68s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3871/4620 [14:45:26<2:46:13, 13.32s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3872/4620 [14:45:40<2:46:36, 13.36s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3873/4620 [14:45:52<2:41:06, 12.94s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3874/4620 [14:46:06<2:44:44, 13.25s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3875/4620 [14:46:19<2:43:34, 13.17s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3876/4620 [14:46:31<2:39:08, 12.83s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3877/4620 [14:46:43<2:37:16, 12.70s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3878/4620 [14:46:56<2:37:07, 12.71s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3879/4620 [14:47:09<2:36:24, 12.66s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3880/4620 [14:47:21<2:35:56, 12.64s/it]                                                        {'loss': 0.0566, 'grad_norm': 0.318359375, 'learning_rate': 8.185840707964602e-05, 'epoch': 4.2}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3880/4620 [14:47:21<2:35:56, 12.64s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3881/4620 [14:47:35<2:40:58, 13.07s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3882/4620 [14:47:47<2:36:53, 12.76s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3883/4620 [14:48:01<2:40:29, 13.07s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3884/4620 [14:48:15<2:42:34, 13.25s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3885/4620 [14:48:28<2:42:21, 13.25s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3886/4620 [14:48:40<2:36:31, 12.79s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3887/4620 [14:48:52<2:33:02, 12.53s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3888/4620 [14:49:04<2:33:42, 12.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3889/4620 [14:49:17<2:34:28, 12.68s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3890/4620 [14:49:30<2:33:49, 12.64s/it]                                                        {'loss': 0.0566, 'grad_norm': 0.0546875, 'learning_rate': 8.075221238938054e-05, 'epoch': 4.21}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3890/4620 [14:49:30<2:33:49, 12.64s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3891/4620 [14:49:42<2:31:47, 12.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3892/4620 [14:49:56<2:37:12, 12.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3893/4620 [14:50:10<2:39:35, 13.17s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3894/4620 [14:50:23<2:38:58, 13.14s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3895/4620 [14:50:35<2:35:25, 12.86s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3896/4620 [14:50:47<2:33:00, 12.68s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3897/4620 [14:50:59<2:31:19, 12.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3898/4620 [14:51:12<2:29:59, 12.46s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3899/4620 [14:51:24<2:30:22, 12.51s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3900/4620 [14:51:37<2:30:12, 12.52s/it]                                                        {'loss': 0.049, 'grad_norm': 0.28125, 'learning_rate': 7.964601769911504e-05, 'epoch': 4.22}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3900/4620 [14:51:37<2:30:12, 12.52s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:52,  3.72s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:37,  2.88s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:29,  2.48s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:24,  2.21s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:20,  2.03s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.86s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.77s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.71s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.64s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.64s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.89s/it][A                                                        
                                               [A{'eval_loss': 0.07011078298091888, 'eval_runtime': 67.9973, 'eval_samples_per_second': 7.353, 'eval_steps_per_second': 0.235, 'epoch': 4.22}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3900/4620 [14:52:45<2:30:12, 12.52s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.89s/it][A
                                               [A 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3901/4620 [14:52:56<6:27:57, 32.37s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3902/4620 [14:53:09<5:18:22, 26.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3903/4620 [14:53:22<4:30:25, 22.63s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3904/4620 [14:53:35<3:56:26, 19.81s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3905/4620 [14:53:47<3:27:49, 17.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3906/4620 [14:54:00<3:09:25, 15.92s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3907/4620 [14:54:12<2:56:55, 14.89s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3908/4620 [14:54:25<2:49:21, 14.27s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3909/4620 [14:54:38<2:43:32, 13.80s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3910/4620 [14:54:50<2:38:33, 13.40s/it]                                                        {'loss': 0.0566, 'grad_norm': 0.1669921875, 'learning_rate': 7.853982300884956e-05, 'epoch': 4.23}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3910/4620 [14:54:50<2:38:33, 13.40s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3911/4620 [14:55:04<2:38:49, 13.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3912/4620 [14:55:16<2:34:57, 13.13s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3913/4620 [14:55:32<2:43:30, 13.88s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3914/4620 [14:55:43<2:35:19, 13.20s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3915/4620 [14:55:55<2:31:32, 12.90s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3916/4620 [14:56:08<2:29:15, 12.72s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3917/4620 [14:56:20<2:28:35, 12.68s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3918/4620 [14:56:33<2:27:32, 12.61s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3919/4620 [14:56:46<2:27:45, 12.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3920/4620 [14:56:59<2:28:51, 12.76s/it]                                                        {'loss': 0.0515, 'grad_norm': 0.2353515625, 'learning_rate': 7.743362831858407e-05, 'epoch': 4.24}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3920/4620 [14:56:59<2:28:51, 12.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3921/4620 [14:57:12<2:32:06, 13.06s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3922/4620 [14:57:25<2:29:25, 12.84s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3923/4620 [14:57:39<2:34:02, 13.26s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3924/4620 [14:57:52<2:33:28, 13.23s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3925/4620 [14:58:04<2:29:19, 12.89s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3926/4620 [14:58:17<2:27:23, 12.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3927/4620 [14:58:29<2:26:47, 12.71s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3928/4620 [14:58:41<2:25:07, 12.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3929/4620 [14:58:54<2:25:03, 12.60s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3930/4620 [14:59:08<2:30:17, 13.07s/it]                                                        {'loss': 0.0575, 'grad_norm': 0.212890625, 'learning_rate': 7.632743362831859e-05, 'epoch': 4.25}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3930/4620 [14:59:08<2:30:17, 13.07s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3931/4620 [14:59:20<2:25:52, 12.70s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3932/4620 [14:59:32<2:24:15, 12.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3933/4620 [14:59:47<2:30:08, 13.11s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3934/4620 [15:00:02<2:36:23, 13.68s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3935/4620 [15:00:13<2:28:24, 13.00s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3936/4620 [15:00:25<2:25:27, 12.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3937/4620 [15:00:38<2:23:15, 12.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3938/4620 [15:00:50<2:22:16, 12.52s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3939/4620 [15:01:02<2:21:56, 12.51s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3940/4620 [15:01:17<2:28:22, 13.09s/it]                                                        {'loss': 0.0601, 'grad_norm': 0.16015625, 'learning_rate': 7.52212389380531e-05, 'epoch': 4.26}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3940/4620 [15:01:17<2:28:22, 13.09s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3941/4620 [15:01:29<2:24:35, 12.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3942/4620 [15:01:41<2:22:37, 12.62s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3943/4620 [15:01:54<2:22:28, 12.63s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3944/4620 [15:02:09<2:32:40, 13.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3945/4620 [15:02:21<2:25:09, 12.90s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3946/4620 [15:02:33<2:22:18, 12.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3947/4620 [15:02:46<2:22:05, 12.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3948/4620 [15:02:58<2:20:59, 12.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3949/4620 [15:03:12<2:24:22, 12.91s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3950/4620 [15:03:24<2:20:47, 12.61s/it]                                                        {'loss': 0.0568, 'grad_norm': 0.08837890625, 'learning_rate': 7.411504424778762e-05, 'epoch': 4.27}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3950/4620 [15:03:24<2:20:47, 12.61s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3951/4620 [15:03:36<2:20:08, 12.57s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3952/4620 [15:03:49<2:21:05, 12.67s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3953/4620 [15:04:01<2:19:12, 12.52s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3954/4620 [15:04:15<2:23:57, 12.97s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3955/4620 [15:04:28<2:24:14, 13.01s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3956/4620 [15:04:40<2:19:25, 12.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3957/4620 [15:04:52<2:17:47, 12.47s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3958/4620 [15:05:06<2:22:57, 12.96s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3959/4620 [15:05:18<2:19:35, 12.67s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3960/4620 [15:05:30<2:17:04, 12.46s/it]                                                        {'loss': 0.0518, 'grad_norm': 0.23828125, 'learning_rate': 7.300884955752213e-05, 'epoch': 4.29}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3960/4620 [15:05:30<2:17:04, 12.46s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3961/4620 [15:05:42<2:16:07, 12.39s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3962/4620 [15:05:55<2:17:26, 12.53s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3963/4620 [15:06:08<2:17:47, 12.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3964/4620 [15:06:22<2:20:49, 12.88s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3965/4620 [15:06:35<2:21:30, 12.96s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3966/4620 [15:06:47<2:17:43, 12.64s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3967/4620 [15:06:59<2:16:52, 12.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3968/4620 [15:07:13<2:21:01, 12.98s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3969/4620 [15:07:25<2:17:56, 12.71s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3970/4620 [15:07:37<2:15:53, 12.54s/it]                                                        {'loss': 0.0496, 'grad_norm': 0.1728515625, 'learning_rate': 7.190265486725664e-05, 'epoch': 4.3}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3970/4620 [15:07:37<2:15:53, 12.54s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3971/4620 [15:07:50<2:15:13, 12.50s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3972/4620 [15:08:02<2:15:15, 12.52s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3973/4620 [15:08:15<2:16:06, 12.62s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3974/4620 [15:08:29<2:21:46, 13.17s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3975/4620 [15:08:43<2:21:42, 13.18s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3976/4620 [15:08:55<2:17:51, 12.84s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3977/4620 [15:09:08<2:19:25, 13.01s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3978/4620 [15:09:20<2:14:56, 12.61s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3979/4620 [15:09:32<2:13:24, 12.49s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3980/4620 [15:09:44<2:12:06, 12.39s/it]                                                        {'loss': 0.0543, 'grad_norm': 0.30078125, 'learning_rate': 7.079646017699115e-05, 'epoch': 4.31}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3980/4620 [15:09:44<2:12:06, 12.39s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3981/4620 [15:09:57<2:12:24, 12.43s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3982/4620 [15:10:09<2:12:29, 12.46s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3983/4620 [15:10:21<2:11:24, 12.38s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3984/4620 [15:10:36<2:17:01, 12.93s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3985/4620 [15:10:50<2:22:57, 13.51s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3986/4620 [15:11:02<2:15:26, 12.82s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3987/4620 [15:11:16<2:18:50, 13.16s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3988/4620 [15:11:28<2:14:34, 12.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3989/4620 [15:11:40<2:13:12, 12.67s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3990/4620 [15:11:52<2:12:15, 12.60s/it]                                                        {'loss': 0.0617, 'grad_norm': 0.265625, 'learning_rate': 6.969026548672567e-05, 'epoch': 4.32}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3990/4620 [15:11:52<2:12:15, 12.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3991/4620 [15:12:05<2:12:01, 12.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3992/4620 [15:12:18<2:11:49, 12.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3993/4620 [15:12:30<2:10:52, 12.52s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3994/4620 [15:12:44<2:14:32, 12.90s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3995/4620 [15:12:59<2:21:17, 13.56s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3996/4620 [15:13:12<2:18:50, 13.35s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3997/4620 [15:13:23<2:13:39, 12.87s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3998/4620 [15:13:36<2:11:56, 12.73s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3999/4620 [15:13:48<2:10:55, 12.65s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4000/4620 [15:14:01<2:09:42, 12.55s/it]                                                        {'loss': 0.0392, 'grad_norm': 0.447265625, 'learning_rate': 6.858407079646017e-05, 'epoch': 4.33}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4000/4620 [15:14:01<2:09:42, 12.55s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:52,  3.73s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:15<01:10,  5.43s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:17<00:52,  4.34s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:19<00:37,  3.44s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:21<00:27,  2.80s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:22<00:21,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:24<00:17,  2.14s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:25<00:13,  1.97s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:27<00:10,  1.80s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:28<00:08,  1.74s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:30<00:07,  1.76s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:32<00:05,  1.73s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:33<00:03,  1.71s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:35<00:01,  1.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:38<00:00,  1.92s/it][A                                                        
                                               [A{'eval_loss': 0.06956814974546432, 'eval_runtime': 80.4655, 'eval_samples_per_second': 6.214, 'eval_steps_per_second': 0.199, 'epoch': 4.33}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4000/4620 [15:15:21<2:09:42, 12.55s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:38<00:00,  1.92s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4001/4620 [15:15:34<6:20:54, 36.92s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4002/4620 [15:15:46<5:01:45, 29.30s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4003/4620 [15:15:58<4:08:27, 24.16s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4004/4620 [15:16:14<3:41:44, 21.60s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4005/4620 [15:16:26<3:12:51, 18.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4006/4620 [15:16:40<2:56:29, 17.25s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4007/4620 [15:16:51<2:39:10, 15.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4008/4620 [15:17:04<2:29:49, 14.69s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4009/4620 [15:17:17<2:23:23, 14.08s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4010/4620 [15:17:29<2:19:42, 13.74s/it]                                                        {'loss': 0.0482, 'grad_norm': 0.134765625, 'learning_rate': 6.747787610619469e-05, 'epoch': 4.34}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4010/4620 [15:17:30<2:19:42, 13.74s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4011/4620 [15:17:42<2:15:35, 13.36s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4012/4620 [15:17:55<2:14:06, 13.23s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4013/4620 [15:18:08<2:12:54, 13.14s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4014/4620 [15:18:22<2:17:21, 13.60s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4015/4620 [15:18:36<2:16:46, 13.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4016/4620 [15:18:49<2:14:45, 13.39s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4017/4620 [15:19:01<2:09:59, 12.94s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4018/4620 [15:19:13<2:07:45, 12.73s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4019/4620 [15:19:26<2:09:33, 12.93s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4020/4620 [15:19:39<2:08:56, 12.89s/it]                                                        {'loss': 0.0492, 'grad_norm': 0.2197265625, 'learning_rate': 6.637168141592921e-05, 'epoch': 4.35}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4020/4620 [15:19:39<2:08:56, 12.89s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4021/4620 [15:19:51<2:06:39, 12.69s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4022/4620 [15:20:04<2:07:01, 12.74s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4023/4620 [15:20:19<2:11:22, 13.20s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4024/4620 [15:20:32<2:10:44, 13.16s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4025/4620 [15:20:45<2:11:17, 13.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4026/4620 [15:20:58<2:10:18, 13.16s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4027/4620 [15:21:10<2:06:25, 12.79s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4028/4620 [15:21:22<2:03:58, 12.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4029/4620 [15:21:35<2:03:56, 12.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4030/4620 [15:21:48<2:05:12, 12.73s/it]                                                        {'loss': 0.0565, 'grad_norm': 0.1650390625, 'learning_rate': 6.526548672566373e-05, 'epoch': 4.36}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4030/4620 [15:21:48<2:05:12, 12.73s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4031/4620 [15:22:00<2:04:56, 12.73s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4032/4620 [15:22:13<2:05:10, 12.77s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4033/4620 [15:22:28<2:10:05, 13.30s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4034/4620 [15:22:41<2:09:10, 13.23s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4035/4620 [15:22:54<2:08:20, 13.16s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4036/4620 [15:23:07<2:08:53, 13.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4037/4620 [15:23:19<2:03:26, 12.70s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4038/4620 [15:23:31<2:01:58, 12.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4039/4620 [15:23:44<2:02:31, 12.65s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4040/4620 [15:23:57<2:02:03, 12.63s/it]                                                        {'loss': 0.0582, 'grad_norm': 0.283203125, 'learning_rate': 6.415929203539823e-05, 'epoch': 4.37}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4040/4620 [15:23:57<2:02:03, 12.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4041/4620 [15:24:09<2:01:52, 12.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4042/4620 [15:24:23<2:05:56, 13.07s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4043/4620 [15:24:36<2:03:28, 12.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4044/4620 [15:24:51<2:10:13, 13.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4045/4620 [15:25:02<2:04:15, 12.97s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4046/4620 [15:25:15<2:02:26, 12.80s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4047/4620 [15:25:28<2:04:39, 13.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4048/4620 [15:25:40<2:00:26, 12.63s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4049/4620 [15:25:52<1:59:33, 12.56s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4050/4620 [15:26:05<1:59:56, 12.62s/it]                                                        {'loss': 0.0508, 'grad_norm': 0.333984375, 'learning_rate': 6.305309734513275e-05, 'epoch': 4.38}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4050/4620 [15:26:05<1:59:56, 12.62s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4051/4620 [15:26:19<2:03:35, 13.03s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4052/4620 [15:26:31<1:59:59, 12.68s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4053/4620 [15:26:44<1:59:07, 12.61s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4054/4620 [15:26:59<2:07:27, 13.51s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4055/4620 [15:27:10<2:00:45, 12.82s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4056/4620 [15:27:22<1:58:34, 12.61s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4057/4620 [15:27:36<2:01:31, 12.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4058/4620 [15:27:49<1:59:37, 12.77s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4059/4620 [15:28:01<1:58:01, 12.62s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4060/4620 [15:28:14<1:58:32, 12.70s/it]                                                        {'loss': 0.0417, 'grad_norm': 0.0947265625, 'learning_rate': 6.194690265486725e-05, 'epoch': 4.39}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4060/4620 [15:28:14<1:58:32, 12.70s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4061/4620 [15:28:28<2:03:22, 13.24s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4062/4620 [15:28:40<1:58:46, 12.77s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4063/4620 [15:28:52<1:56:52, 12.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4064/4620 [15:29:07<2:03:06, 13.29s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4065/4620 [15:29:19<1:58:25, 12.80s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4066/4620 [15:29:31<1:56:09, 12.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4067/4620 [15:29:45<1:59:48, 13.00s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4068/4620 [15:29:57<1:57:32, 12.78s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4069/4620 [15:30:09<1:56:26, 12.68s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4070/4620 [15:30:24<2:00:10, 13.11s/it]                                                        {'loss': 0.0425, 'grad_norm': 0.1845703125, 'learning_rate': 6.084070796460177e-05, 'epoch': 4.4}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4070/4620 [15:30:24<2:00:10, 13.11s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4071/4620 [15:30:36<1:57:43, 12.87s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4072/4620 [15:30:48<1:56:40, 12.77s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4073/4620 [15:31:01<1:55:56, 12.72s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4074/4620 [15:31:15<1:59:19, 13.11s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4075/4620 [15:31:27<1:55:47, 12.75s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4076/4620 [15:31:39<1:54:58, 12.68s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4077/4620 [15:31:53<1:57:57, 13.03s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4078/4620 [15:32:06<1:55:37, 12.80s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4079/4620 [15:32:18<1:55:01, 12.76s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4080/4620 [15:32:32<1:56:32, 12.95s/it]                                                        {'loss': 0.0517, 'grad_norm': 0.146484375, 'learning_rate': 5.973451327433629e-05, 'epoch': 4.42}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4080/4620 [15:32:32<1:56:32, 12.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4081/4620 [15:32:44<1:54:47, 12.78s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4082/4620 [15:32:57<1:54:23, 12.76s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4083/4620 [15:33:11<1:57:21, 13.11s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4084/4620 [15:33:25<1:59:39, 13.39s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4085/4620 [15:33:37<1:55:59, 13.01s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4086/4620 [15:33:49<1:54:25, 12.86s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4087/4620 [15:34:03<1:57:42, 13.25s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4088/4620 [15:34:16<1:54:39, 12.93s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4089/4620 [15:34:29<1:56:10, 13.13s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4090/4620 [15:34:42<1:53:47, 12.88s/it]                                                        {'loss': 0.055, 'grad_norm': 0.376953125, 'learning_rate': 5.86283185840708e-05, 'epoch': 4.43}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4090/4620 [15:34:42<1:53:47, 12.88s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4091/4620 [15:34:54<1:52:31, 12.76s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4092/4620 [15:35:07<1:52:08, 12.74s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4093/4620 [15:35:20<1:54:18, 13.01s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4094/4620 [15:35:34<1:55:43, 13.20s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4095/4620 [15:35:46<1:52:30, 12.86s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4096/4620 [15:35:59<1:51:19, 12.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4097/4620 [15:36:12<1:53:07, 12.98s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4098/4620 [15:36:24<1:50:36, 12.71s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4099/4620 [15:36:38<1:52:56, 13.01s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4100/4620 [15:36:50<1:49:42, 12.66s/it]                                                        {'loss': 0.0542, 'grad_norm': 0.421875, 'learning_rate': 5.752212389380531e-05, 'epoch': 4.44}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4100/4620 [15:36:50<1:49:42, 12.66s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:05<00:35,  2.55s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:07<00:30,  2.31s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:24,  2.06s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:10<00:21,  1.91s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:18,  1.82s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.73s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:13,  1.75s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:17<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:10,  1.69s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:08,  1.65s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.63s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:23<00:04,  1.60s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:25<00:03,  1.68s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.94s/it][A                                                        
                                               [A{'eval_loss': 0.0697644054889679, 'eval_runtime': 68.0605, 'eval_samples_per_second': 7.346, 'eval_steps_per_second': 0.235, 'epoch': 4.44}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4100/4620 [15:37:58<1:49:42, 12.66s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:29<00:00,  1.94s/it][A
                                               [A 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4101/4620 [15:38:08<4:40:30, 32.43s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4102/4620 [15:38:20<3:45:42, 26.14s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4103/4620 [15:38:33<3:12:54, 22.39s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4104/4620 [15:38:47<2:50:00, 19.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4105/4620 [15:38:59<2:28:36, 17.31s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4106/4620 [15:39:11<2:15:11, 15.78s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4107/4620 [15:39:25<2:11:55, 15.43s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4108/4620 [15:39:39<2:06:32, 14.83s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4109/4620 [15:39:51<1:59:00, 13.97s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4110/4620 [15:40:03<1:54:26, 13.46s/it]                                                        {'loss': 0.047, 'grad_norm': 0.2021484375, 'learning_rate': 5.6415929203539824e-05, 'epoch': 4.45}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4110/4620 [15:40:03<1:54:26, 13.46s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4111/4620 [15:40:16<1:52:49, 13.30s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4112/4620 [15:40:30<1:53:36, 13.42s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4113/4620 [15:40:42<1:49:40, 12.98s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4114/4620 [15:40:56<1:52:14, 13.31s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4115/4620 [15:41:08<1:48:28, 12.89s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4116/4620 [15:41:20<1:46:40, 12.70s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4117/4620 [15:41:34<1:49:31, 13.06s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4118/4620 [15:41:47<1:50:07, 13.16s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4119/4620 [15:41:59<1:46:46, 12.79s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4120/4620 [15:42:11<1:44:51, 12.58s/it]                                                        {'loss': 0.0507, 'grad_norm': 0.1728515625, 'learning_rate': 5.530973451327434e-05, 'epoch': 4.46}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4120/4620 [15:42:11<1:44:51, 12.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4121/4620 [15:42:25<1:48:09, 13.00s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4122/4620 [15:42:37<1:45:54, 12.76s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4123/4620 [15:42:50<1:44:41, 12.64s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4124/4620 [15:43:03<1:47:09, 12.96s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4125/4620 [15:43:16<1:46:28, 12.91s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4126/4620 [15:43:29<1:44:41, 12.72s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4127/4620 [15:43:42<1:46:22, 12.95s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4128/4620 [15:43:56<1:47:44, 13.14s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4129/4620 [15:44:08<1:44:44, 12.80s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4130/4620 [15:44:21<1:46:30, 13.04s/it]                                                        {'loss': 0.0595, 'grad_norm': 0.2412109375, 'learning_rate': 5.4203539823008854e-05, 'epoch': 4.47}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4130/4620 [15:44:21<1:46:30, 13.04s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4131/4620 [15:44:33<1:42:45, 12.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4132/4620 [15:44:45<1:42:04, 12.55s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4133/4620 [15:44:58<1:42:30, 12.63s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4134/4620 [15:45:12<1:45:04, 12.97s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4135/4620 [15:45:24<1:42:23, 12.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4136/4620 [15:45:36<1:41:43, 12.61s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4137/4620 [15:45:50<1:44:45, 13.01s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4138/4620 [15:46:02<1:41:55, 12.69s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4139/4620 [15:46:15<1:41:02, 12.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4140/4620 [15:46:27<1:40:48, 12.60s/it]                                                        {'loss': 0.0499, 'grad_norm': 0.1220703125, 'learning_rate': 5.3097345132743365e-05, 'epoch': 4.48}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4140/4620 [15:46:27<1:40:48, 12.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4141/4620 [15:46:41<1:42:39, 12.86s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4142/4620 [15:46:53<1:40:54, 12.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4143/4620 [15:47:05<1:40:04, 12.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4144/4620 [15:47:19<1:43:21, 13.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4145/4620 [15:47:32<1:41:19, 12.80s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4146/4620 [15:47:46<1:45:48, 13.39s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4147/4620 [15:48:00<1:45:19, 13.36s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4148/4620 [15:48:12<1:41:52, 12.95s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4149/4620 [15:48:24<1:40:49, 12.84s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4150/4620 [15:48:37<1:40:04, 12.78s/it]                                                        {'loss': 0.0459, 'grad_norm': 0.26171875, 'learning_rate': 5.1991150442477876e-05, 'epoch': 4.49}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4150/4620 [15:48:37<1:40:04, 12.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4151/4620 [15:48:50<1:41:58, 13.04s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4152/4620 [15:49:02<1:38:38, 12.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4153/4620 [15:49:15<1:37:57, 12.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4154/4620 [15:49:28<1:40:10, 12.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4155/4620 [15:49:40<1:36:43, 12.48s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4156/4620 [15:49:53<1:39:20, 12.85s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4157/4620 [15:50:06<1:37:52, 12.68s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4158/4620 [15:50:18<1:37:09, 12.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4159/4620 [15:50:31<1:36:28, 12.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4160/4620 [15:50:43<1:36:03, 12.53s/it]                                                        {'loss': 0.0564, 'grad_norm': 0.15625, 'learning_rate': 5.0884955752212395e-05, 'epoch': 4.5}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4160/4620 [15:50:43<1:36:03, 12.53s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4161/4620 [15:50:57<1:39:01, 12.94s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4162/4620 [15:51:09<1:36:55, 12.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4163/4620 [15:51:21<1:35:22, 12.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4164/4620 [15:51:34<1:35:39, 12.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4165/4620 [15:51:48<1:38:56, 13.05s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4166/4620 [15:52:03<1:43:51, 13.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4167/4620 [15:52:15<1:38:28, 13.04s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4168/4620 [15:52:27<1:36:21, 12.79s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4169/4620 [15:52:39<1:35:09, 12.66s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4170/4620 [15:52:52<1:33:42, 12.50s/it]                                                        {'loss': 0.0479, 'grad_norm': 0.4453125, 'learning_rate': 4.9778761061946906e-05, 'epoch': 4.51}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4170/4620 [15:52:52<1:33:42, 12.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4171/4620 [15:53:04<1:33:40, 12.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4172/4620 [15:53:18<1:36:33, 12.93s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4173/4620 [15:53:30<1:34:54, 12.74s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4174/4620 [15:53:42<1:33:25, 12.57s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4175/4620 [15:53:57<1:36:37, 13.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4176/4620 [15:54:10<1:36:35, 13.05s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4177/4620 [15:54:22<1:33:59, 12.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4178/4620 [15:54:34<1:32:57, 12.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4179/4620 [15:54:47<1:33:22, 12.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4180/4620 [15:55:00<1:33:23, 12.73s/it]                                                        {'loss': 0.0449, 'grad_norm': 0.2734375, 'learning_rate': 4.867256637168142e-05, 'epoch': 4.52}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4180/4620 [15:55:00<1:33:23, 12.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4181/4620 [15:55:12<1:32:18, 12.62s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4182/4620 [15:55:26<1:35:03, 13.02s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4183/4620 [15:55:38<1:33:32, 12.84s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4184/4620 [15:55:51<1:32:21, 12.71s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4185/4620 [15:56:07<1:39:35, 13.74s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4186/4620 [15:56:18<1:33:55, 12.99s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4187/4620 [15:56:31<1:32:14, 12.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4188/4620 [15:56:43<1:32:01, 12.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4189/4620 [15:56:56<1:31:04, 12.68s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4190/4620 [15:57:08<1:30:22, 12.61s/it]                                                        {'loss': 0.0446, 'grad_norm': 0.3203125, 'learning_rate': 4.756637168141593e-05, 'epoch': 4.53}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4190/4620 [15:57:08<1:30:22, 12.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4191/4620 [15:57:21<1:30:32, 12.66s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4192/4620 [15:57:35<1:33:42, 13.14s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4193/4620 [15:57:47<1:30:52, 12.77s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4194/4620 [15:58:01<1:31:52, 12.94s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4195/4620 [15:58:16<1:36:11, 13.58s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4196/4620 [15:58:27<1:30:57, 12.87s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4197/4620 [15:58:39<1:29:46, 12.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4198/4620 [15:58:52<1:29:15, 12.69s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4199/4620 [15:59:04<1:28:22, 12.60s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4200/4620 [15:59:17<1:28:22, 12.62s/it]                                                        {'loss': 0.053, 'grad_norm': 0.451171875, 'learning_rate': 4.646017699115045e-05, 'epoch': 4.55}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4200/4620 [15:59:17<1:28:22, 12.62s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:08<00:59,  4.24s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:10<00:42,  3.24s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:12<00:32,  2.72s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:13<00:25,  2.31s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:15<00:21,  2.10s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:17<00:17,  1.95s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:18<00:14,  1.83s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:20<00:12,  1.85s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:22<00:10,  1.78s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:23<00:08,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:25<00:06,  1.68s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:26<00:04,  1.66s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:28<00:03,  1.66s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:30<00:01,  1.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.95s/it][A                                                        
                                               [A{'eval_loss': 0.06978454440832138, 'eval_runtime': 79.4923, 'eval_samples_per_second': 6.29, 'eval_steps_per_second': 0.201, 'epoch': 4.55}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4200/4620 [16:00:36<1:28:22, 12.62s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  1.95s/it][A
                                               [A 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4201/4620 [16:00:48<4:13:04, 36.24s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4202/4620 [16:01:00<3:20:31, 28.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4203/4620 [16:01:12<2:45:38, 23.83s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4204/4620 [16:01:28<2:30:02, 21.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4205/4620 [16:01:42<2:12:13, 19.12s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4206/4620 [16:01:53<1:56:25, 16.87s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4207/4620 [16:02:06<1:47:00, 15.55s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4208/4620 [16:02:18<1:40:39, 14.66s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4209/4620 [16:02:31<1:36:50, 14.14s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4210/4620 [16:02:44<1:33:54, 13.74s/it]                                                        {'loss': 0.0478, 'grad_norm': 0.1767578125, 'learning_rate': 4.535398230088496e-05, 'epoch': 4.56}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4210/4620 [16:02:44<1:33:54, 13.74s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4211/4620 [16:02:58<1:33:31, 13.72s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4212/4620 [16:03:10<1:30:10, 13.26s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4213/4620 [16:03:23<1:30:27, 13.34s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4214/4620 [16:03:37<1:30:13, 13.33s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4215/4620 [16:03:50<1:29:24, 13.25s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4216/4620 [16:04:02<1:26:25, 12.84s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4217/4620 [16:04:14<1:25:21, 12.71s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4218/4620 [16:04:27<1:25:11, 12.71s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4219/4620 [16:04:39<1:24:16, 12.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4220/4620 [16:04:51<1:23:31, 12.53s/it]                                                        {'loss': 0.0468, 'grad_norm': 0.419921875, 'learning_rate': 4.424778761061947e-05, 'epoch': 4.57}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4220/4620 [16:04:52<1:23:31, 12.53s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4221/4620 [16:05:05<1:26:07, 12.95s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4222/4620 [16:05:17<1:23:36, 12.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4223/4620 [16:05:31<1:25:35, 12.94s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4224/4620 [16:05:43<1:23:47, 12.69s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4225/4620 [16:05:57<1:25:33, 13.00s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4226/4620 [16:06:09<1:23:44, 12.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4227/4620 [16:06:21<1:22:32, 12.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4228/4620 [16:06:34<1:22:04, 12.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4229/4620 [16:06:46<1:22:08, 12.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4230/4620 [16:07:00<1:24:33, 13.01s/it]                                                        {'loss': 0.0556, 'grad_norm': 0.1796875, 'learning_rate': 4.314159292035398e-05, 'epoch': 4.58}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4230/4620 [16:07:00<1:24:33, 13.01s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4231/4620 [16:07:12<1:22:27, 12.72s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4232/4620 [16:07:26<1:23:42, 12.94s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4233/4620 [16:07:38<1:21:41, 12.67s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4234/4620 [16:07:51<1:22:38, 12.85s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4235/4620 [16:08:04<1:23:23, 13.00s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4236/4620 [16:08:16<1:21:18, 12.70s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4237/4620 [16:08:29<1:20:15, 12.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4238/4620 [16:08:41<1:19:42, 12.52s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4239/4620 [16:08:54<1:19:45, 12.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4240/4620 [16:09:07<1:21:39, 12.89s/it]                                                        {'loss': 0.0575, 'grad_norm': 0.216796875, 'learning_rate': 4.20353982300885e-05, 'epoch': 4.59}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4240/4620 [16:09:08<1:21:39, 12.89s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4241/4620 [16:09:20<1:20:09, 12.69s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4242/4620 [16:09:34<1:22:09, 13.04s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4243/4620 [16:09:46<1:20:54, 12.88s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4244/4620 [16:10:00<1:22:32, 13.17s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4245/4620 [16:10:13<1:23:00, 13.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4246/4620 [16:10:25<1:20:08, 12.86s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4247/4620 [16:10:38<1:18:49, 12.68s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4248/4620 [16:10:50<1:18:22, 12.64s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4249/4620 [16:11:04<1:20:13, 12.98s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4250/4620 [16:11:16<1:18:17, 12.70s/it]                                                        {'loss': 0.049, 'grad_norm': 0.296875, 'learning_rate': 4.092920353982301e-05, 'epoch': 4.6}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4250/4620 [16:11:16<1:18:17, 12.70s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4251/4620 [16:11:29<1:19:15, 12.89s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4252/4620 [16:11:41<1:17:22, 12.61s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4253/4620 [16:11:54<1:16:43, 12.54s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4254/4620 [16:12:08<1:19:05, 12.97s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4255/4620 [16:12:20<1:18:47, 12.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4256/4620 [16:12:33<1:16:51, 12.67s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4257/4620 [16:12:45<1:16:35, 12.66s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4258/4620 [16:12:58<1:16:46, 12.73s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4259/4620 [16:13:12<1:18:02, 12.97s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4260/4620 [16:13:24<1:16:16, 12.71s/it]                                                        {'loss': 0.0463, 'grad_norm': 0.162109375, 'learning_rate': 3.982300884955752e-05, 'epoch': 4.61}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4260/4620 [16:13:24<1:16:16, 12.71s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4261/4620 [16:13:37<1:17:49, 13.01s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4262/4620 [16:13:49<1:15:02, 12.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4263/4620 [16:14:01<1:14:14, 12.48s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4264/4620 [16:14:14<1:14:05, 12.49s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4265/4620 [16:14:28<1:16:55, 13.00s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4266/4620 [16:14:40<1:15:02, 12.72s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4267/4620 [16:14:52<1:13:35, 12.51s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4268/4620 [16:15:06<1:15:54, 12.94s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4269/4620 [16:15:18<1:14:49, 12.79s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4270/4620 [16:15:32<1:16:22, 13.09s/it]                                                        {'loss': 0.0426, 'grad_norm': 0.1357421875, 'learning_rate': 3.8716814159292034e-05, 'epoch': 4.62}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4270/4620 [16:15:32<1:16:22, 13.09s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4271/4620 [16:15:44<1:14:44, 12.85s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4272/4620 [16:15:57<1:13:50, 12.73s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4273/4620 [16:16:09<1:12:59, 12.62s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4274/4620 [16:16:22<1:12:45, 12.62s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4275/4620 [16:16:37<1:16:21, 13.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4276/4620 [16:16:49<1:13:53, 12.89s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4277/4620 [16:17:02<1:14:42, 13.07s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4278/4620 [16:17:14<1:13:10, 12.84s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4279/4620 [16:17:27<1:11:49, 12.64s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4280/4620 [16:17:40<1:13:34, 12.98s/it]                                                        {'loss': 0.0451, 'grad_norm': 0.1728515625, 'learning_rate': 3.761061946902655e-05, 'epoch': 4.63}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4280/4620 [16:17:40<1:13:34, 12.98s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4281/4620 [16:17:52<1:11:14, 12.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4282/4620 [16:18:05<1:10:57, 12.60s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4283/4620 [16:18:17<1:10:59, 12.64s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4284/4620 [16:18:30<1:10:30, 12.59s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4285/4620 [16:18:44<1:13:01, 13.08s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4286/4620 [16:18:57<1:11:59, 12.93s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4287/4620 [16:19:09<1:10:53, 12.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4288/4620 [16:19:23<1:12:38, 13.13s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4289/4620 [16:19:37<1:13:28, 13.32s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4290/4620 [16:19:49<1:10:31, 12.82s/it]                                                        {'loss': 0.0477, 'grad_norm': 0.2490234375, 'learning_rate': 3.650442477876106e-05, 'epoch': 4.64}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4290/4620 [16:19:49<1:10:31, 12.82s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4291/4620 [16:20:01<1:09:19, 12.64s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4292/4620 [16:20:13<1:08:54, 12.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4293/4620 [16:20:26<1:09:00, 12.66s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4294/4620 [16:20:39<1:08:54, 12.68s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4295/4620 [16:20:53<1:10:26, 13.00s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4296/4620 [16:21:05<1:08:34, 12.70s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4297/4620 [16:21:17<1:08:09, 12.66s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4298/4620 [16:21:31<1:10:07, 13.07s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4299/4620 [16:21:43<1:08:12, 12.75s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4300/4620 [16:21:56<1:07:24, 12.64s/it]                                                        {'loss': 0.0533, 'grad_norm': 0.384765625, 'learning_rate': 3.5398230088495574e-05, 'epoch': 4.65}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4300/4620 [16:21:56<1:07:24, 12.64s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:06<00:46,  3.35s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:38,  2.98s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:30,  2.50s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.16s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.96s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:17,  1.89s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.87s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.78s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.73s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.68s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.65s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.67s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.70s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.86s/it][A                                                        
                                               [A{'eval_loss': 0.06984199583530426, 'eval_runtime': 83.448, 'eval_samples_per_second': 5.992, 'eval_steps_per_second': 0.192, 'epoch': 4.65}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4300/4620 [16:23:19<1:07:24, 12.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.86s/it][A
                                               [A 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4301/4620 [16:23:29<3:16:56, 37.04s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4302/4620 [16:23:41<2:35:32, 29.35s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4303/4620 [16:23:53<2:08:14, 24.27s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4304/4620 [16:24:06<1:50:06, 20.91s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4305/4620 [16:24:21<1:39:20, 18.92s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4306/4620 [16:24:33<1:28:18, 16.87s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4307/4620 [16:24:45<1:21:01, 15.53s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4308/4620 [16:24:59<1:18:45, 15.14s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4309/4620 [16:25:11<1:13:16, 14.14s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4310/4620 [16:25:23<1:10:10, 13.58s/it]                                                        {'loss': 0.0556, 'grad_norm': 0.30078125, 'learning_rate': 3.4292035398230086e-05, 'epoch': 4.66}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4310/4620 [16:25:24<1:10:10, 13.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4311/4620 [16:25:37<1:09:34, 13.51s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4312/4620 [16:25:50<1:08:33, 13.35s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4313/4620 [16:26:03<1:07:21, 13.17s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4314/4620 [16:26:15<1:06:32, 13.05s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4315/4620 [16:26:30<1:08:43, 13.52s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4316/4620 [16:26:42<1:05:55, 13.01s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4317/4620 [16:26:54<1:04:58, 12.87s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4318/4620 [16:27:08<1:06:27, 13.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4319/4620 [16:27:20<1:04:34, 12.87s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4320/4620 [16:27:33<1:03:33, 12.71s/it]                                                        {'loss': 0.0636, 'grad_norm': 0.2060546875, 'learning_rate': 3.3185840707964604e-05, 'epoch': 4.68}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4320/4620 [16:27:33<1:03:33, 12.71s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4321/4620 [16:27:45<1:03:12, 12.68s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4322/4620 [16:27:58<1:03:07, 12.71s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4323/4620 [16:28:11<1:03:08, 12.76s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4324/4620 [16:28:23<1:02:24, 12.65s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4325/4620 [16:28:38<1:04:40, 13.15s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4326/4620 [16:28:50<1:03:08, 12.89s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4327/4620 [16:29:04<1:04:21, 13.18s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4328/4620 [16:29:16<1:02:51, 12.92s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4329/4620 [16:29:30<1:04:33, 13.31s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4330/4620 [16:29:42<1:02:28, 12.92s/it]                                                        {'loss': 0.0636, 'grad_norm': 0.2578125, 'learning_rate': 3.2079646017699115e-05, 'epoch': 4.69}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4330/4620 [16:29:42<1:02:28, 12.92s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4331/4620 [16:29:55<1:01:31, 12.77s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4332/4620 [16:30:07<1:00:58, 12.70s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4333/4620 [16:30:20<1:00:31, 12.65s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4334/4620 [16:30:32<1:00:14, 12.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4335/4620 [16:30:46<1:02:00, 13.05s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4336/4620 [16:31:00<1:02:32, 13.21s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4337/4620 [16:31:13<1:02:02, 13.15s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4338/4620 [16:31:27<1:02:25, 13.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4339/4620 [16:31:39<1:00:10, 12.85s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4340/4620 [16:31:51<58:58, 12.64s/it]                                                        {'loss': 0.0483, 'grad_norm': 0.486328125, 'learning_rate': 3.097345132743363e-05, 'epoch': 4.7}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4340/4620 [16:31:51<58:58, 12.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4341/4620 [16:32:03<58:42, 12.63s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4342/4620 [16:32:16<58:49, 12.70s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4343/4620 [16:32:29<58:29, 12.67s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4344/4620 [16:32:41<58:08, 12.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4345/4620 [16:32:56<1:00:13, 13.14s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4346/4620 [16:33:09<1:00:35, 13.27s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4347/4620 [16:33:22<1:00:14, 13.24s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4348/4620 [16:33:34<58:24, 12.89s/it]   94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4349/4620 [16:33:47<57:27, 12.72s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4350/4620 [16:33:59<57:04, 12.68s/it]                                                      {'loss': 0.0495, 'grad_norm': 0.2109375, 'learning_rate': 2.9867256637168145e-05, 'epoch': 4.71}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4350/4620 [16:33:59<57:04, 12.68s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4351/4620 [16:34:12<57:01, 12.72s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4352/4620 [16:34:25<56:53, 12.74s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4353/4620 [16:34:37<56:23, 12.67s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4354/4620 [16:34:50<56:05, 12.65s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4355/4620 [16:35:04<57:30, 13.02s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4356/4620 [16:35:17<57:49, 13.14s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4357/4620 [16:35:31<57:49, 13.19s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4358/4620 [16:35:43<56:20, 12.90s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4359/4620 [16:35:55<55:43, 12.81s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4360/4620 [16:36:08<55:43, 12.86s/it]                                                      {'loss': 0.0561, 'grad_norm': 0.462890625, 'learning_rate': 2.8761061946902656e-05, 'epoch': 4.72}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4360/4620 [16:36:09<55:43, 12.86s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4361/4620 [16:36:21<55:04, 12.76s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4362/4620 [16:36:33<54:32, 12.69s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4363/4620 [16:36:47<54:46, 12.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4364/4620 [16:36:59<54:37, 12.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4365/4620 [16:37:14<56:38, 13.33s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4366/4620 [16:37:27<56:39, 13.38s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4367/4620 [16:37:39<54:31, 12.93s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4368/4620 [16:37:52<53:38, 12.77s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4369/4620 [16:38:05<53:41, 12.84s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4370/4620 [16:38:17<53:02, 12.73s/it]                                                      {'loss': 0.0605, 'grad_norm': 0.263671875, 'learning_rate': 2.765486725663717e-05, 'epoch': 4.73}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4370/4620 [16:38:17<53:02, 12.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4371/4620 [16:38:30<52:33, 12.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4372/4620 [16:38:42<52:05, 12.60s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4373/4620 [16:38:55<52:09, 12.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4374/4620 [16:39:08<52:22, 12.78s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4375/4620 [16:39:22<53:44, 13.16s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4376/4620 [16:39:38<56:35, 13.92s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4377/4620 [16:39:49<53:09, 13.12s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4378/4620 [16:40:01<51:41, 12.81s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4379/4620 [16:40:14<51:17, 12.77s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4380/4620 [16:40:27<51:05, 12.77s/it]                                                      {'loss': 0.0588, 'grad_norm': 0.1728515625, 'learning_rate': 2.6548672566371683e-05, 'epoch': 4.74}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4380/4620 [16:40:27<51:05, 12.77s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4381/4620 [16:40:39<50:41, 12.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4382/4620 [16:40:51<49:56, 12.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4383/4620 [16:41:04<49:54, 12.63s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4384/4620 [16:41:18<51:17, 13.04s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4385/4620 [16:41:32<51:41, 13.20s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4386/4620 [16:41:45<52:02, 13.34s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4387/4620 [16:41:57<50:05, 12.90s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4388/4620 [16:42:09<49:01, 12.68s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4389/4620 [16:42:22<48:27, 12.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4390/4620 [16:42:34<48:18, 12.60s/it]                                                      {'loss': 0.0487, 'grad_norm': 0.1640625, 'learning_rate': 2.5442477876106197e-05, 'epoch': 4.75}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4390/4620 [16:42:34<48:18, 12.60s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4391/4620 [16:42:47<48:13, 12.64s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4392/4620 [16:43:00<47:47, 12.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4393/4620 [16:43:13<48:57, 12.94s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4394/4620 [16:43:25<47:43, 12.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4395/4620 [16:43:41<50:17, 13.41s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4396/4620 [16:43:52<47:44, 12.79s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4397/4620 [16:44:04<47:15, 12.71s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4398/4620 [16:44:17<46:55, 12.68s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4399/4620 [16:44:30<46:38, 12.66s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4400/4620 [16:44:43<46:43, 12.75s/it]                                                      {'loss': 0.0497, 'grad_norm': 0.43359375, 'learning_rate': 2.433628318584071e-05, 'epoch': 4.76}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4400/4620 [16:44:43<46:43, 12.75s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:49,  3.56s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:37,  2.88s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:10<00:29,  2.44s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.12s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:13<00:19,  1.92s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.82s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:14,  1.78s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:12,  1.76s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.72s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:21<00:08,  1.66s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:23<00:06,  1.63s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.59s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:26<00:03,  1.62s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:28<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00,  1.91s/it][A                                                      
                                               [A{'eval_loss': 0.07001306861639023, 'eval_runtime': 74.7336, 'eval_samples_per_second': 6.69, 'eval_steps_per_second': 0.214, 'epoch': 4.76}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4400/4620 [16:45:57<46:43, 12.75s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.91s/it][A
                                               [A 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4401/4620 [16:46:08<2:05:57, 34.51s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4402/4620 [16:46:20<1:40:32, 27.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4403/4620 [16:46:34<1:25:15, 23.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4404/4620 [16:46:47<1:14:10, 20.60s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4405/4620 [16:47:01<1:06:06, 18.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4406/4620 [16:47:13<58:49, 16.49s/it]   95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4407/4620 [16:47:25<54:11, 15.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4408/4620 [16:47:38<51:30, 14.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4409/4620 [16:47:50<48:57, 13.92s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4410/4620 [16:48:03<47:10, 13.48s/it]                                                      {'loss': 0.0525, 'grad_norm': 0.375, 'learning_rate': 2.3230088495575223e-05, 'epoch': 4.77}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4410/4620 [16:48:03<47:10, 13.48s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4411/4620 [16:48:16<46:06, 13.24s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4412/4620 [16:48:28<44:59, 12.98s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4413/4620 [16:48:42<45:41, 13.25s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4414/4620 [16:48:57<47:47, 13.92s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4415/4620 [16:49:09<45:01, 13.18s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4416/4620 [16:49:21<43:41, 12.85s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4417/4620 [16:49:34<43:23, 12.83s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4418/4620 [16:49:47<43:29, 12.92s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4419/4620 [16:50:00<43:23, 12.95s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4420/4620 [16:50:12<42:49, 12.85s/it]                                                      {'loss': 0.0474, 'grad_norm': 0.2255859375, 'learning_rate': 2.2123893805309735e-05, 'epoch': 4.78}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4420/4620 [16:50:12<42:49, 12.85s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4421/4620 [16:50:25<42:37, 12.85s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4422/4620 [16:50:39<43:30, 13.18s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4423/4620 [16:50:53<43:27, 13.23s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4424/4620 [16:51:04<41:53, 12.82s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4425/4620 [16:51:17<41:17, 12.71s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4426/4620 [16:51:29<40:51, 12.64s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4427/4620 [16:51:41<40:14, 12.51s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4428/4620 [16:51:54<40:17, 12.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4429/4620 [16:52:07<40:19, 12.67s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4430/4620 [16:52:20<40:05, 12.66s/it]                                                      {'loss': 0.0488, 'grad_norm': 0.23828125, 'learning_rate': 2.101769911504425e-05, 'epoch': 4.79}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4430/4620 [16:52:20<40:05, 12.66s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4431/4620 [16:52:34<41:29, 13.17s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4432/4620 [16:52:46<40:25, 12.90s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4433/4620 [16:53:00<41:09, 13.20s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4434/4620 [16:53:14<41:04, 13.25s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4435/4620 [16:53:26<39:46, 12.90s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4436/4620 [16:53:38<39:25, 12.86s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4437/4620 [16:53:51<38:55, 12.76s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4438/4620 [16:54:04<38:48, 12.80s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4439/4620 [16:54:17<38:40, 12.82s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4440/4620 [16:54:30<38:38, 12.88s/it]                                                      {'loss': 0.0554, 'grad_norm': 0.1572265625, 'learning_rate': 1.991150442477876e-05, 'epoch': 4.81}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4440/4620 [16:54:30<38:38, 12.88s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4441/4620 [16:54:42<38:13, 12.81s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4442/4620 [16:54:57<39:47, 13.41s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4443/4620 [16:55:09<38:15, 12.97s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4444/4620 [16:55:23<38:38, 13.17s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4445/4620 [16:55:35<37:46, 12.95s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4446/4620 [16:55:48<37:04, 12.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4447/4620 [16:56:00<36:43, 12.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4448/4620 [16:56:13<36:24, 12.70s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4449/4620 [16:56:26<36:18, 12.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4450/4620 [16:56:39<36:05, 12.74s/it]                                                      {'loss': 0.0575, 'grad_norm': 0.185546875, 'learning_rate': 1.8805309734513276e-05, 'epoch': 4.82}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4450/4620 [16:56:39<36:05, 12.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4451/4620 [16:56:53<37:25, 13.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4452/4620 [16:57:06<36:57, 13.20s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4453/4620 [16:57:18<35:33, 12.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4454/4620 [16:57:31<35:48, 12.94s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4455/4620 [16:57:43<35:02, 12.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4456/4620 [16:57:56<34:33, 12.65s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4457/4620 [16:58:08<34:11, 12.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4458/4620 [16:58:21<33:47, 12.51s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4459/4620 [16:58:33<33:30, 12.49s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4460/4620 [16:58:46<33:20, 12.50s/it]                                                      {'loss': 0.0543, 'grad_norm': 0.18359375, 'learning_rate': 1.7699115044247787e-05, 'epoch': 4.83}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4460/4620 [16:58:46<33:20, 12.50s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4461/4620 [16:59:01<35:01, 13.22s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4462/4620 [16:59:14<35:07, 13.34s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4463/4620 [16:59:26<34:04, 13.02s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4464/4620 [16:59:39<33:15, 12.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4465/4620 [16:59:53<33:57, 13.15s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4466/4620 [17:00:05<32:58, 12.85s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4467/4620 [17:00:18<32:47, 12.86s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4468/4620 [17:00:30<32:31, 12.84s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4469/4620 [17:00:43<32:06, 12.76s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4470/4620 [17:00:57<33:00, 13.20s/it]                                                      {'loss': 0.0498, 'grad_norm': 0.09375, 'learning_rate': 1.6592920353982302e-05, 'epoch': 4.84}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4470/4620 [17:00:57<33:00, 13.20s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4471/4620 [17:01:09<31:44, 12.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4472/4620 [17:01:21<31:03, 12.59s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4473/4620 [17:01:35<31:46, 12.97s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4474/4620 [17:01:47<31:01, 12.75s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4475/4620 [17:02:01<31:33, 13.06s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4476/4620 [17:02:13<30:47, 12.83s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4477/4620 [17:02:26<30:21, 12.74s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4478/4620 [17:02:38<29:56, 12.65s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4479/4620 [17:02:51<29:41, 12.63s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4480/4620 [17:03:05<30:37, 13.12s/it]                                                      {'loss': 0.0433, 'grad_norm': 0.224609375, 'learning_rate': 1.5486725663716813e-05, 'epoch': 4.85}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4480/4620 [17:03:05<30:37, 13.12s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4481/4620 [17:03:17<29:42, 12.82s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4482/4620 [17:03:30<29:08, 12.67s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4483/4620 [17:03:44<29:47, 13.05s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4484/4620 [17:03:56<28:51, 12.73s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4485/4620 [17:04:09<29:23, 13.06s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4486/4620 [17:04:21<28:19, 12.68s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4487/4620 [17:04:33<27:46, 12.53s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4488/4620 [17:04:46<27:36, 12.55s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4489/4620 [17:05:00<28:16, 12.95s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4490/4620 [17:05:12<27:45, 12.82s/it]                                                      {'loss': 0.0572, 'grad_norm': 0.1318359375, 'learning_rate': 1.4380530973451328e-05, 'epoch': 4.86}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4490/4620 [17:05:12<27:45, 12.82s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4491/4620 [17:05:25<27:34, 12.82s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4492/4620 [17:05:38<27:09, 12.73s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4493/4620 [17:05:51<27:34, 13.03s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4494/4620 [17:06:04<26:50, 12.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4495/4620 [17:06:18<27:23, 13.14s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4496/4620 [17:06:29<26:21, 12.75s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4497/4620 [17:06:42<26:03, 12.71s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4498/4620 [17:06:55<25:53, 12.73s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4499/4620 [17:07:09<26:21, 13.07s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4500/4620 [17:07:20<25:19, 12.66s/it]                                                      {'loss': 0.0513, 'grad_norm': 0.09033203125, 'learning_rate': 1.3274336283185841e-05, 'epoch': 4.87}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4500/4620 [17:07:21<25:19, 12.66s/it]
  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:07<00:53,  3.85s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:09<00:39,  3.01s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:11<00:29,  2.49s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:12<00:23,  2.16s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:14<00:19,  1.97s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:15<00:16,  1.84s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:17<00:13,  1.75s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:19<00:12,  1.76s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:20<00:10,  1.75s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:22<00:08,  1.72s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:24<00:06,  1.67s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:25<00:04,  1.66s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:27<00:03,  1.63s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:29<00:01,  1.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.91s/it][A                                                      
                                               [A{'eval_loss': 0.0699756070971489, 'eval_runtime': 75.5538, 'eval_samples_per_second': 6.618, 'eval_steps_per_second': 0.212, 'epoch': 4.87}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4500/4620 [17:08:36<25:19, 12.66s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:31<00:00,  1.91s/it][A
                                               [A/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4501/4620 [17:08:49<1:10:20, 35.47s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4502/4620 [17:09:00<55:22, 28.15s/it]   97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4503/4620 [17:09:14<46:17, 23.74s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4504/4620 [17:09:26<39:14, 20.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4505/4620 [17:09:40<35:14, 18.39s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4506/4620 [17:09:52<31:10, 16.40s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4507/4620 [17:10:04<28:28, 15.12s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4508/4620 [17:10:18<27:52, 14.94s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4509/4620 [17:10:32<26:58, 14.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4510/4620 [17:10:45<25:38, 13.99s/it]                                                      {'loss': 0.0485, 'grad_norm': 0.2177734375, 'learning_rate': 1.2168141592920354e-05, 'epoch': 4.88}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4510/4620 [17:10:45<25:38, 13.99s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4511/4620 [17:10:57<24:35, 13.54s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4512/4620 [17:11:10<23:51, 13.26s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4513/4620 [17:11:22<23:16, 13.05s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4514/4620 [17:11:36<23:33, 13.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4515/4620 [17:11:50<23:33, 13.46s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4516/4620 [17:12:02<22:24, 12.93s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4517/4620 [17:12:14<21:47, 12.69s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4518/4620 [17:12:28<22:31, 13.25s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4519/4620 [17:12:41<22:08, 13.15s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4520/4620 [17:12:54<21:31, 12.92s/it]                                                      {'loss': 0.0478, 'grad_norm': 0.1748046875, 'learning_rate': 1.1061946902654867e-05, 'epoch': 4.89}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4520/4620 [17:12:54<21:31, 12.92s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4521/4620 [17:13:06<21:11, 12.85s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4522/4620 [17:13:19<20:56, 12.82s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4523/4620 [17:13:32<20:39, 12.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4524/4620 [17:13:46<21:16, 13.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4525/4620 [17:14:00<21:15, 13.43s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4526/4620 [17:14:12<20:23, 13.02s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4527/4620 [17:14:26<20:39, 13.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4528/4620 [17:14:38<19:50, 12.94s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4529/4620 [17:14:52<20:01, 13.20s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4530/4620 [17:15:05<19:36, 13.07s/it]                                                      {'loss': 0.0552, 'grad_norm': 0.1591796875, 'learning_rate': 9.95575221238938e-06, 'epoch': 4.9}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4530/4620 [17:15:05<19:36, 13.07s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4531/4620 [17:15:17<19:08, 12.91s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4532/4620 [17:15:30<18:50, 12.84s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4533/4620 [17:15:43<18:27, 12.73s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4534/4620 [17:15:56<18:46, 13.10s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4535/4620 [17:16:10<18:48, 13.27s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4536/4620 [17:16:22<17:56, 12.82s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4537/4620 [17:16:36<18:08, 13.11s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4538/4620 [17:16:47<17:22, 12.71s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4539/4620 [17:17:01<17:28, 12.94s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4540/4620 [17:17:13<16:58, 12.73s/it]                                                      {'loss': 0.0535, 'grad_norm': 0.3203125, 'learning_rate': 8.849557522123894e-06, 'epoch': 4.91}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4540/4620 [17:17:13<16:58, 12.73s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4541/4620 [17:17:26<16:37, 12.63s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4542/4620 [17:17:38<16:19, 12.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4543/4620 [17:17:51<16:10, 12.61s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4544/4620 [17:18:05<16:34, 13.08s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4545/4620 [17:18:18<16:28, 13.18s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4546/4620 [17:18:31<16:13, 13.15s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4547/4620 [17:18:43<15:30, 12.75s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4548/4620 [17:18:56<15:14, 12.70s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4549/4620 [17:19:09<15:17, 12.93s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4550/4620 [17:19:21<14:44, 12.63s/it]                                                      {'loss': 0.0597, 'grad_norm': 0.2275390625, 'learning_rate': 7.743362831858407e-06, 'epoch': 4.92}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4550/4620 [17:19:21<14:44, 12.63s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4551/4620 [17:19:33<14:24, 12.53s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4552/4620 [17:19:46<14:20, 12.65s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4553/4620 [17:20:00<14:36, 13.09s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4554/4620 [17:20:13<14:16, 12.98s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4555/4620 [17:20:27<14:24, 13.30s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4556/4620 [17:20:41<14:10, 13.29s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4557/4620 [17:20:52<13:27, 12.82s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4558/4620 [17:21:05<13:10, 12.75s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4559/4620 [17:21:19<13:25, 13.20s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4560/4620 [17:21:31<12:48, 12.81s/it]                                                      {'loss': 0.0483, 'grad_norm': 0.2177734375, 'learning_rate': 6.637168141592921e-06, 'epoch': 4.94}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4560/4620 [17:21:31<12:48, 12.81s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4561/4620 [17:21:43<12:24, 12.62s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4562/4620 [17:21:56<12:20, 12.76s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4563/4620 [17:22:10<12:28, 13.13s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4564/4620 [17:22:22<11:56, 12.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4565/4620 [17:22:36<11:58, 13.06s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4566/4620 [17:22:48<11:33, 12.84s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4567/4620 [17:23:01<11:15, 12.75s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4568/4620 [17:23:14<11:05, 12.80s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4569/4620 [17:23:28<11:14, 13.23s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4570/4620 [17:23:40<10:46, 12.92s/it]                                                      {'loss': 0.0514, 'grad_norm': 0.2431640625, 'learning_rate': 5.530973451327434e-06, 'epoch': 4.95}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4570/4620 [17:23:40<10:46, 12.92s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4571/4620 [17:23:53<10:24, 12.75s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4572/4620 [17:24:05<10:14, 12.80s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4573/4620 [17:24:19<10:19, 13.18s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4574/4620 [17:24:33<10:11, 13.29s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4575/4620 [17:24:45<09:38, 12.85s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4576/4620 [17:24:57<09:19, 12.72s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4577/4620 [17:25:10<09:03, 12.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4578/4620 [17:25:23<08:53, 12.70s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4579/4620 [17:25:36<08:52, 13.00s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4580/4620 [17:25:48<08:25, 12.64s/it]                                                      {'loss': 0.0563, 'grad_norm': 0.37890625, 'learning_rate': 4.424778761061947e-06, 'epoch': 4.96}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4580/4620 [17:25:48<08:25, 12.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4581/4620 [17:26:01<08:10, 12.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4582/4620 [17:26:13<08:00, 12.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4583/4620 [17:26:27<08:01, 13.02s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4584/4620 [17:26:40<07:51, 13.08s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4585/4620 [17:26:54<07:45, 13.30s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4586/4620 [17:27:07<07:21, 12.99s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4587/4620 [17:27:19<07:01, 12.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4588/4620 [17:27:31<06:45, 12.67s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4589/4620 [17:27:46<06:47, 13.16s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4590/4620 [17:27:57<06:24, 12.80s/it]                                                      {'loss': 0.0563, 'grad_norm': 0.212890625, 'learning_rate': 3.3185840707964603e-06, 'epoch': 4.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4590/4620 [17:27:58<06:24, 12.80s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4591/4620 [17:28:10<06:07, 12.66s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4592/4620 [17:28:23<05:55, 12.69s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4593/4620 [17:28:37<05:56, 13.20s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4594/4620 [17:28:49<05:33, 12.83s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4595/4620 [17:29:02<05:24, 12.98s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4596/4620 [17:29:14<05:05, 12.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4597/4620 [17:29:27<04:51, 12.68s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4598/4620 [17:29:39<04:37, 12.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4599/4620 [17:29:53<04:33, 13.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4600/4620 [17:30:06<04:16, 12.83s/it]                                                      {'loss': 0.0469, 'grad_norm': 0.29296875, 'learning_rate': 2.2123893805309734e-06, 'epoch': 4.98}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4600/4620 [17:30:06<04:16, 12.83s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

  0%|          | 0/16 [00:00<?, ?it/s][A
 12%|â–ˆâ–Ž        | 2/16 [00:04<00:32,  2.33s/it][A
 19%|â–ˆâ–‰        | 3/16 [00:06<00:27,  2.14s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:08<00:23,  2.00s/it][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:09<00:20,  1.85s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:11<00:17,  1.78s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:13<00:15,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:14<00:14,  1.76s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:16<00:12,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:18<00:09,  1.67s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:19<00:08,  1.63s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:21<00:06,  1.60s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:22<00:04,  1.62s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:24<00:03,  1.69s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:26<00:01,  1.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.89s/it][A                                                      
                                               [A{'eval_loss': 0.06991668045520782, 'eval_runtime': 83.0844, 'eval_samples_per_second': 6.018, 'eval_steps_per_second': 0.193, 'epoch': 4.98}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4600/4620 [17:31:29<04:16, 12.83s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:28<00:00,  1.89s/it][A
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4601/4620 [17:31:39<11:43, 37.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4602/4620 [17:31:53<09:00, 30.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4603/4620 [17:32:05<06:57, 24.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4604/4620 [17:32:15<05:23, 20.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4605/4620 [17:32:24<04:14, 16.93s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4606/4620 [17:32:33<03:23, 14.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4607/4620 [17:32:42<02:46, 12.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4608/4620 [17:32:51<02:19, 11.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4609/4620 [17:33:00<01:59, 10.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4610/4620 [17:33:09<01:42, 10.25s/it]                                                      {'loss': 0.0529, 'grad_norm': 0.2236328125, 'learning_rate': 1.1061946902654867e-06, 'epoch': 4.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4610/4620 [17:33:09<01:42, 10.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4611/4620 [17:33:17<01:28,  9.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4612/4620 [17:33:26<01:16,  9.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4613/4620 [17:33:35<01:06,  9.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4614/4620 [17:33:44<00:55,  9.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4615/4620 [17:33:53<00:45,  9.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4616/4620 [17:34:02<00:36,  9.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4617/4620 [17:34:11<00:27,  9.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4618/4620 [17:34:20<00:18,  9.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4619/4620 [17:34:29<00:08,  8.96s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4620/4620 [17:34:40<00:00,  9.55s/it]                                                      {'loss': 0.0509, 'grad_norm': 0.10400390625, 'learning_rate': 0.0, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4620/4620 [17:34:40<00:00,  9.55s/it]/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                      {'train_runtime': 63291.1642, 'train_samples_per_second': 9.337, 'train_steps_per_second': 0.073, 'train_loss': 0.07897579024235407, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4620/4620 [17:34:42<00:00,  9.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4620/4620 [17:34:42<00:00, 13.70s/it]
Time: 63289.36
Samples/second: 9.34
Time: 63291.16
Samples/second: 9.34
Time: 63289.37
Samples/second: 9.34
Time: 63289.34
Samples/second: 9.34
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ivi/ilps/personal/jqiao/models/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
Running evaluation
Evaluating on validation set
Running evaluation
Evaluating on validation set
Running evaluation
Evaluating on validation set
Running evaluation
Evaluating on validation set
  0%|          | 0/63 [00:00<?, ?it/s]  0%|          | 0/63 [00:00<?, ?it/s]  0%|          | 0/63 [00:00<?, ?it/s]  0%|          | 0/63 [00:00<?, ?it/s]  2%|â–         | 1/63 [00:03<03:40,  3.55s/it]  2%|â–         | 1/63 [00:03<03:38,  3.52s/it]  2%|â–         | 1/63 [00:03<03:43,  3.60s/it]  2%|â–         | 1/63 [00:03<03:37,  3.50s/it]  3%|â–Ž         | 2/63 [00:06<03:24,  3.36s/it]  3%|â–Ž         | 2/63 [00:06<03:28,  3.42s/it]  3%|â–Ž         | 2/63 [00:07<03:34,  3.51s/it]  3%|â–Ž         | 2/63 [00:06<03:29,  3.43s/it]  5%|â–         | 3/63 [00:10<03:20,  3.35s/it]  5%|â–         | 3/63 [00:10<03:21,  3.35s/it]  5%|â–         | 3/63 [00:10<03:21,  3.36s/it]  5%|â–         | 3/63 [00:10<03:26,  3.45s/it]  6%|â–‹         | 4/63 [00:13<03:11,  3.25s/it]  6%|â–‹         | 4/63 [00:13<03:08,  3.19s/it]  6%|â–‹         | 4/63 [00:13<03:08,  3.20s/it]  6%|â–‹         | 4/63 [00:13<03:17,  3.34s/it]  8%|â–Š         | 5/63 [00:16<03:09,  3.27s/it]  8%|â–Š         | 5/63 [00:16<03:04,  3.18s/it]  8%|â–Š         | 5/63 [00:16<03:04,  3.18s/it]  8%|â–Š         | 5/63 [00:16<03:12,  3.32s/it] 10%|â–‰         | 6/63 [00:19<03:08,  3.31s/it] 10%|â–‰         | 6/63 [00:19<03:05,  3.25s/it] 10%|â–‰         | 6/63 [00:19<03:07,  3.28s/it] 10%|â–‰         | 6/63 [00:20<03:09,  3.33s/it] 11%|â–ˆ         | 7/63 [00:23<03:06,  3.33s/it] 11%|â–ˆ         | 7/63 [00:23<03:04,  3.30s/it] 11%|â–ˆ         | 7/63 [00:23<03:08,  3.36s/it] 11%|â–ˆ         | 7/63 [00:23<03:07,  3.35s/it] 13%|â–ˆâ–Ž        | 8/63 [00:26<02:59,  3.26s/it] 13%|â–ˆâ–Ž        | 8/63 [00:26<03:00,  3.29s/it] 13%|â–ˆâ–Ž        | 8/63 [00:26<03:04,  3.36s/it] 13%|â–ˆâ–Ž        | 8/63 [00:26<03:04,  3.35s/it] 14%|â–ˆâ–        | 9/63 [00:29<02:56,  3.28s/it] 14%|â–ˆâ–        | 9/63 [00:29<02:57,  3.28s/it] 14%|â–ˆâ–        | 9/63 [00:29<02:59,  3.32s/it] 14%|â–ˆâ–        | 9/63 [00:30<03:01,  3.37s/it] 16%|â–ˆâ–Œ        | 10/63 [00:32<02:50,  3.21s/it] 16%|â–ˆâ–Œ        | 10/63 [00:32<02:49,  3.20s/it] 16%|â–ˆâ–Œ        | 10/63 [00:32<02:50,  3.21s/it] 16%|â–ˆâ–Œ        | 10/63 [00:33<02:51,  3.24s/it] 17%|â–ˆâ–‹        | 11/63 [00:36<02:53,  3.33s/it] 17%|â–ˆâ–‹        | 11/63 [00:36<02:49,  3.27s/it] 17%|â–ˆâ–‹        | 11/63 [00:36<02:51,  3.30s/it] 17%|â–ˆâ–‹        | 11/63 [00:36<02:53,  3.34s/it] 19%|â–ˆâ–‰        | 12/63 [00:39<02:46,  3.26s/it] 19%|â–ˆâ–‰        | 12/63 [00:39<02:50,  3.34s/it] 19%|â–ˆâ–‰        | 12/63 [00:39<02:46,  3.26s/it] 19%|â–ˆâ–‰        | 12/63 [00:40<02:48,  3.31s/it] 21%|â–ˆâ–ˆ        | 13/63 [00:42<02:43,  3.28s/it] 21%|â–ˆâ–ˆ        | 13/63 [00:42<02:43,  3.27s/it] 21%|â–ˆâ–ˆ        | 13/63 [00:43<02:48,  3.36s/it] 21%|â–ˆâ–ˆ        | 13/63 [00:43<02:46,  3.32s/it] 22%|â–ˆâ–ˆâ–       | 14/63 [00:46<02:41,  3.30s/it] 22%|â–ˆâ–ˆâ–       | 14/63 [00:46<02:40,  3.28s/it] 22%|â–ˆâ–ˆâ–       | 14/63 [00:46<02:44,  3.36s/it] 22%|â–ˆâ–ˆâ–       | 14/63 [00:46<02:44,  3.35s/it] 24%|â–ˆâ–ˆâ–       | 15/63 [00:49<02:41,  3.36s/it] 24%|â–ˆâ–ˆâ–       | 15/63 [00:49<02:42,  3.39s/it] 24%|â–ˆâ–ˆâ–       | 15/63 [00:50<02:44,  3.43s/it] 24%|â–ˆâ–ˆâ–       | 15/63 [00:50<02:43,  3.41s/it] 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:52<02:34,  3.30s/it] 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:52<02:36,  3.32s/it] 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:53<02:37,  3.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:53<02:36,  3.34s/it] 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:55<02:30,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:56<02:32,  3.30s/it] 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:56<02:31,  3.30s/it] 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:56<02:33,  3.34s/it] 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:59<02:28,  3.31s/it] 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:59<02:27,  3.29s/it] 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:59<02:27,  3.28s/it] 29%|â–ˆâ–ˆâ–Š       | 18/63 [01:00<02:29,  3.32s/it] 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [01:02<02:26,  3.33s/it] 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [01:03<02:27,  3.35s/it] 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [01:02<02:29,  3.40s/it] 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [01:03<02:27,  3.36s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [01:06<02:26,  3.41s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [01:06<02:27,  3.42s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [01:06<02:30,  3.49s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [01:07<02:28,  3.45s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/63 [01:10<02:26,  3.48s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/63 [01:10<02:28,  3.54s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/63 [01:10<02:29,  3.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/63 [01:10<02:27,  3.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [01:13<02:19,  3.39s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [01:13<02:21,  3.45s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [01:13<02:22,  3.48s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [01:14<02:20,  3.44s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [01:16<02:11,  3.30s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [01:16<02:15,  3.38s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [01:16<02:14,  3.37s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [01:17<02:14,  3.35s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [01:20<02:11,  3.36s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [01:20<02:11,  3.38s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [01:20<02:14,  3.45s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [01:20<02:13,  3.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [01:23<02:03,  3.26s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [01:23<02:04,  3.27s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [01:23<02:06,  3.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [01:24<02:06,  3.32s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [01:26<02:00,  3.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [01:26<02:00,  3.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [01:26<02:01,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [01:27<02:02,  3.31s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/63 [01:29<01:55,  3.20s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/63 [01:29<01:54,  3.17s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/63 [01:29<01:54,  3.18s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/63 [01:30<01:56,  3.24s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [01:32<01:52,  3.22s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [01:32<01:50,  3.16s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [01:32<01:50,  3.15s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [01:33<01:53,  3.24s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [01:36<01:49,  3.23s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [01:35<01:48,  3.19s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [01:35<01:47,  3.17s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [01:36<01:49,  3.21s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [01:39<01:48,  3.29s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [01:39<01:48,  3.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [01:39<01:50,  3.36s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [01:40<01:48,  3.29s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [01:42<01:44,  3.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [01:42<01:45,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [01:43<01:47,  3.37s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [01:43<01:44,  3.27s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [01:45<01:40,  3.25s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [01:45<01:41,  3.27s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [01:46<01:43,  3.35s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [01:46<01:41,  3.27s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [01:48<01:34,  3.17s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [01:48<01:35,  3.18s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [01:49<01:38,  3.28s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [01:49<01:36,  3.23s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [01:51<01:31,  3.17s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [01:51<01:32,  3.18s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [01:52<01:34,  3.24s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [01:53<01:34,  3.25s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [01:56<01:37,  3.47s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [01:56<01:37,  3.48s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [01:56<01:40,  3.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [01:57<01:40,  3.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [01:59<01:30,  3.34s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [01:59<01:30,  3.36s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [01:59<01:31,  3.39s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [02:00<01:32,  3.41s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [02:02<01:26,  3.33s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [02:02<01:27,  3.36s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [02:03<01:26,  3.34s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [02:03<01:28,  3.41s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [02:05<01:22,  3.31s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [02:05<01:23,  3.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [02:06<01:22,  3.31s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [02:07<01:24,  3.38s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [02:08<01:17,  3.24s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [02:08<01:17,  3.24s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [02:09<01:16,  3.21s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [02:10<01:19,  3.32s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/63 [02:12<01:15,  3.26s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/63 [02:12<01:16,  3.31s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/63 [02:12<01:14,  3.25s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/63 [02:13<01:16,  3.34s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [02:15<01:11,  3.24s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [02:15<01:13,  3.33s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [02:16<01:11,  3.27s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [02:17<01:13,  3.33s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [02:18<01:08,  3.24s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [02:18<01:09,  3.32s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [02:19<01:09,  3.32s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [02:20<01:09,  3.32s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [02:21<01:02,  3.14s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [02:22<01:05,  3.26s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [02:22<01:04,  3.24s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [02:23<01:04,  3.23s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [02:24<01:00,  3.17s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [02:25<01:02,  3.30s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [02:26<01:02,  3.31s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [02:26<01:02,  3.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [02:27<00:56,  3.13s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [02:28<00:58,  3.24s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [02:29<00:58,  3.24s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [02:29<00:57,  3.19s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 46/63 [02:30<00:53,  3.13s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 46/63 [02:31<00:54,  3.21s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 46/63 [02:32<00:55,  3.25s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 46/63 [02:33<00:54,  3.21s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [02:33<00:49,  3.12s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [02:34<00:50,  3.18s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [02:35<00:52,  3.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [02:36<00:51,  3.23s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [02:37<00:48,  3.23s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [02:38<00:48,  3.26s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [02:39<00:49,  3.30s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [02:39<00:49,  3.32s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [02:40<00:44,  3.14s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [02:41<00:44,  3.15s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [02:42<00:45,  3.23s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [02:42<00:45,  3.23s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [02:43<00:40,  3.11s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [02:44<00:40,  3.12s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [02:45<00:41,  3.20s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [02:46<00:42,  3.24s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [02:46<00:37,  3.11s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [02:47<00:37,  3.09s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [02:48<00:38,  3.18s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [02:49<00:38,  3.20s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 52/63 [02:49<00:34,  3.15s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 52/63 [02:50<00:34,  3.11s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 52/63 [02:51<00:35,  3.20s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [02:52<00:31,  3.15s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 52/63 [02:52<00:36,  3.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [02:53<00:30,  3.09s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [02:54<00:31,  3.19s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [02:56<00:32,  3.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [02:56<00:29,  3.25s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [02:56<00:28,  3.19s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [02:58<00:29,  3.30s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [02:59<00:26,  3.26s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [02:59<00:30,  3.40s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [03:00<00:25,  3.20s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [03:01<00:26,  3.29s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [03:03<00:22,  3.27s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [03:03<00:22,  3.19s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [03:03<00:27,  3.44s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [03:04<00:22,  3.27s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [03:06<00:19,  3.22s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [03:06<00:19,  3.20s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [03:06<00:23,  3.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [03:08<00:19,  3.26s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [03:09<00:16,  3.27s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [03:09<00:16,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [03:09<00:20,  3.36s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [03:11<00:16,  3.29s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 59/63 [03:12<00:12,  3.24s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 59/63 [03:13<00:13,  3.25s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [03:13<00:16,  3.34s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 59/63 [03:14<00:12,  3.23s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [03:15<00:09,  3.23s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [03:16<00:09,  3.24s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 59/63 [03:16<00:13,  3.29s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [03:17<00:09,  3.23s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [03:19<00:06,  3.30s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [03:19<00:09,  3.26s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [03:19<00:06,  3.34s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [03:21<00:06,  3.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [03:22<00:03,  3.24s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [03:23<00:03,  3.30s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [03:23<00:06,  3.35s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [03:24<00:03,  3.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:24<00:00,  2.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:24<00:00,  3.24s/it]
0it [00:00, ?it/s]0it [00:00, ?it/s]
Embeddings computed, evaluating
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:25<00:00,  2.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:25<00:00,  3.26s/it]
0it [00:00, ?it/s]0it [00:00, ?it/s]
Embeddings computed, evaluating
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:26<00:00,  2.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:26<00:00,  3.28s/it]
0it [00:00, ?it/s]0it [00:00, ?it/s]
Embeddings computed, evaluating
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [03:26<00:03,  3.34s/it]MTEB metrics: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Metrics for validation set: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Evaluating syntheticDocQA_energy
MTEB metrics: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Metrics for validation set: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Evaluating syntheticDocQA_energy
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:28<00:00,  2.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [03:28<00:00,  3.31s/it]
0it [00:00, ?it/s]0it [00:00, ?it/s]
Embeddings computed, evaluating
MTEB metrics: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Metrics for validation set: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Evaluating syntheticDocQA_energy
Traceback (most recent call last):
  File "/ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py", line 31, in <module>
    typer.run(main)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/main.py", line 1080, in run
    app()
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/main.py", line 338, in __call__
    raise e
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/main.py", line 321, in __call__
    return get_command(self)(*args, **kwargs)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/core.py", line 665, in main
    return _main(
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/core.py", line 197, in _main
    rv = self.invoke(ctx)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/typer/main.py", line 703, in wrapper
    return callback(**use_params)
  File "/ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py", line 26, in main
    app.eval()
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/colpali_engine/trainer/colmodel_training.py", line 224, in eval
    test_ds = test_dataset_loading_func()
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/colpali_engine/utils/dataset_transformation.py", line 184, in __call__
    dataset = load_dataset(self.dataset_path, split="test")
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/datasets/load.py", line 2132, in load_dataset
    builder_instance = load_dataset_builder(
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/datasets/load.py", line 1853, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/datasets/load.py", line 1735, in dataset_module_factory
    raise FileNotFoundError(f"Couldn't find any data file at {relative_to_absolute_path(path)}.")
FileNotFoundError: Couldn't find any data file at /ivi/ilps/personal/jqiao/colpali/data_dir/syntheticDocQA_energy_test.
[rank3]: â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank3]: â”‚ /ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py:26 in main   â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   23 â”‚   â”‚   app.save(config_file=config_file)                               â”‚
[rank3]: â”‚   24 â”‚   if config.run_eval:                                                 â”‚
[rank3]: â”‚   25 â”‚   â”‚   print("Running evaluation")                                     â”‚
[rank3]: â”‚ â± 26 â”‚   â”‚   app.eval()                                                      â”‚
[rank3]: â”‚   27 â”‚   print("Done!")                                                      â”‚
[rank3]: â”‚   28                                                                         â”‚
[rank3]: â”‚   29                                                                         â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚         app = <colpali_engine.trainer.colmodel_training.ColModelTraining â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ce66fe650>                                  â”‚ â”‚
[rank3]: â”‚ â”‚      config = ColModelTrainingConfig(                                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   model=PeftModelForFeatureExtraction(                   â”‚ â”‚
[rank3]: â”‚ â”‚                 (base_model): LoraModel(                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   (model): ColQwen2(                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     (visual): Qwen2VisionTransformerPretrainedModel(     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (patch_embed): PatchEmbed(                         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), â”‚ â”‚
[rank3]: â”‚ â”‚               stride=(2, 14, 14), bias=False)                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (rotary_pos_emb): VisionRotaryEmbedding()          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (blocks): ModuleList(                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (0-31): 32 x Qwen2VLVisionBlock(                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm1): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank3]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm2): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank3]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (attn): VisionFlashAttention2(                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (qkv): Linear(in_features=1280,              â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=3840, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (proj): Linear(in_features=1280,             â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): VisionMlp(                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc1): Linear(in_features=1280,              â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (act): QuickGELUActivation()                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc2): Linear(in_features=5120,              â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (merger): PatchMerger(                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (ln_q): LayerNorm((1280,), eps=1e-06,            â”‚ â”‚
[rank3]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (mlp): Sequential(                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (0): Linear(in_features=5120,                  â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (1): GELU(approximate='none')                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (2): Linear(in_features=5120,                  â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     (model): Qwen2VLModel(                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (embed_tokens): Embedding(151936, 1536)            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (layers): ModuleList(                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (0-27): 28 x Qwen2VLDecoderLayer(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (self_attn): Qwen2VLFlashAttention2(           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (q_proj): lora.Linear(                       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (k_proj): lora.Linear(                       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (v_proj): lora.Linear(                       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (o_proj): lora.Linear(                       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (rotary_emb): Qwen2VLRotaryEmbedding()       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): Qwen2MLP(                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (gate_proj): lora.Linear(                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (up_proj): lora.Linear(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (down_proj): lora.Linear(                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=8960,     â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=8960,      â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚     (act_fn): SiLU()                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (input_layernorm): Qwen2RMSNorm((1536,),       â”‚ â”‚
[rank3]: â”‚ â”‚               eps=1e-06)                                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   â”‚   (post_attention_layernorm):                    â”‚ â”‚
[rank3]: â”‚ â”‚               Qwen2RMSNorm((1536,), eps=1e-06)                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (norm): Qwen2RMSNorm((1536,), eps=1e-06)           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (rotary_emb): Qwen2VLRotaryEmbedding()             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     (lm_head): Linear(in_features=1536,                  â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=151936, bias=False)                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     (custom_text_proj): lora.Linear(                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (base_layer): Linear(in_features=1536,             â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=128, bias=True)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (lora_dropout): ModuleDict(                        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (default): Dropout(p=0.1, inplace=False)         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (lora_A): ModuleDict(                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=1536,              â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (lora_B): ModuleDict(                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=32,                â”‚ â”‚
[rank3]: â”‚ â”‚               out_features=128, bias=False)                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_A): ParameterDict()                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_B): ParameterDict()                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   )                                                      â”‚ â”‚
[rank3]: â”‚ â”‚                 )                                                        â”‚ â”‚
[rank3]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   tr_args=TrainingArguments(                             â”‚ â”‚
[rank3]: â”‚ â”‚               _n_gpu=1,                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               accelerator_config={'split_batches': False,                â”‚ â”‚
[rank3]: â”‚ â”‚               'dispatch_batches': None, 'even_batches': True,            â”‚ â”‚
[rank3]: â”‚ â”‚               'use_seedable_sampler': True, 'non_blocking': False,       â”‚ â”‚
[rank3]: â”‚ â”‚               'gradient_accumulation_kwargs': None,                      â”‚ â”‚
[rank3]: â”‚ â”‚               'use_configured_state': False},                            â”‚ â”‚
[rank3]: â”‚ â”‚               adafactor=False,                                           â”‚ â”‚
[rank3]: â”‚ â”‚               adam_beta1=0.9,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               adam_beta2=0.999,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               adam_epsilon=1e-08,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               auto_find_batch_size=False,                                â”‚ â”‚
[rank3]: â”‚ â”‚               average_tokens_across_devices=False,                       â”‚ â”‚
[rank3]: â”‚ â”‚               batch_eval_metrics=False,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               bf16=True,                                                 â”‚ â”‚
[rank3]: â”‚ â”‚               bf16_full_eval=False,                                      â”‚ â”‚
[rank3]: â”‚ â”‚               data_seed=None,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               dataloader_drop_last=False,                                â”‚ â”‚
[rank3]: â”‚ â”‚               dataloader_num_workers=8,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               dataloader_persistent_workers=False,                       â”‚ â”‚
[rank3]: â”‚ â”‚               dataloader_pin_memory=True,                                â”‚ â”‚
[rank3]: â”‚ â”‚               dataloader_prefetch_factor=None,                           â”‚ â”‚
[rank3]: â”‚ â”‚               ddp_backend=None,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               ddp_broadcast_buffers=None,                                â”‚ â”‚
[rank3]: â”‚ â”‚               ddp_bucket_cap_mb=None,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               ddp_find_unused_parameters=None,                           â”‚ â”‚
[rank3]: â”‚ â”‚               ddp_timeout=1800,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               debug=[],                                                  â”‚ â”‚
[rank3]: â”‚ â”‚               deepspeed=None,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               disable_tqdm=False,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               dispatch_batches=None,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               do_eval=True,                                              â”‚ â”‚
[rank3]: â”‚ â”‚               do_predict=False,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               do_train=False,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               eval_accumulation_steps=None,                              â”‚ â”‚
[rank3]: â”‚ â”‚               eval_delay=0,                                              â”‚ â”‚
[rank3]: â”‚ â”‚               eval_do_concat_batches=True,                               â”‚ â”‚
[rank3]: â”‚ â”‚               eval_on_start=False,                                       â”‚ â”‚
[rank3]: â”‚ â”‚               eval_steps=100,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               eval_strategy=steps,                                       â”‚ â”‚
[rank3]: â”‚ â”‚               eval_use_gather_object=False,                              â”‚ â”‚
[rank3]: â”‚ â”‚               evaluation_strategy=None,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               fp16=False,                                                â”‚ â”‚
[rank3]: â”‚ â”‚               fp16_backend=auto,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               fp16_full_eval=False,                                      â”‚ â”‚
[rank3]: â”‚ â”‚               fp16_opt_level=O1,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               fsdp=[],                                                   â”‚ â”‚
[rank3]: â”‚ â”‚               fsdp_config={'min_num_params': 0, 'xla': False,            â”‚ â”‚
[rank3]: â”‚ â”‚               'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},        â”‚ â”‚
[rank3]: â”‚ â”‚               fsdp_min_num_params=0,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               fsdp_transformer_layer_cls_to_wrap=None,                   â”‚ â”‚
[rank3]: â”‚ â”‚               full_determinism=False,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               gradient_accumulation_steps=1,                             â”‚ â”‚
[rank3]: â”‚ â”‚               gradient_checkpointing=True,                               â”‚ â”‚
[rank3]: â”‚ â”‚               gradient_checkpointing_kwargs={'use_reentrant': False},    â”‚ â”‚
[rank3]: â”‚ â”‚               greater_is_better=None,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               group_by_length=False,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               half_precision_backend=auto,                               â”‚ â”‚
[rank3]: â”‚ â”‚               hub_always_push=False,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               hub_model_id=None,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               hub_private_repo=False,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               hub_strategy=every_save,                                   â”‚ â”‚
[rank3]: â”‚ â”‚               hub_token=<HUB_TOKEN>,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               ignore_data_skip=False,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               include_for_metrics=[],                                    â”‚ â”‚
[rank3]: â”‚ â”‚               include_inputs_for_metrics=False,                          â”‚ â”‚
[rank3]: â”‚ â”‚               include_num_input_tokens_seen=False,                       â”‚ â”‚
[rank3]: â”‚ â”‚               include_tokens_per_second=False,                           â”‚ â”‚
[rank3]: â”‚ â”‚               jit_mode_eval=False,                                       â”‚ â”‚
[rank3]: â”‚ â”‚               label_names=None,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               label_smoothing_factor=0.0,                                â”‚ â”‚
[rank3]: â”‚ â”‚               learning_rate=0.0005,                                      â”‚ â”‚
[rank3]: â”‚ â”‚               length_column_name=length,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               load_best_model_at_end=False,                              â”‚ â”‚
[rank3]: â”‚ â”‚               local_rank=3,                                              â”‚ â”‚
[rank3]: â”‚ â”‚               log_level=passive,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               log_level_replica=warning,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               log_on_each_node=True,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               logging_dir=None,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               logging_first_step=False,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               logging_nan_inf_filter=True,                               â”‚ â”‚
[rank3]: â”‚ â”‚               logging_steps=10,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               logging_strategy=steps,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               lr_scheduler_kwargs={},                                    â”‚ â”‚
[rank3]: â”‚ â”‚               lr_scheduler_type=linear,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               max_grad_norm=1.0,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               max_steps=-1,                                              â”‚ â”‚
[rank3]: â”‚ â”‚               metric_for_best_model=None,                                â”‚ â”‚
[rank3]: â”‚ â”‚               mp_parameters=,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               neftune_noise_alpha=None,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               no_cuda=False,                                             â”‚ â”‚
[rank3]: â”‚ â”‚               num_train_epochs=5,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               optim=adamw_torch,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               optim_args=None,                                           â”‚ â”‚
[rank3]: â”‚ â”‚               optim_target_modules=None,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               output_dir=/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               overwrite_output_dir=True,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               past_index=-1,                                             â”‚ â”‚
[rank3]: â”‚ â”‚               per_device_eval_batch_size=8,                              â”‚ â”‚
[rank3]: â”‚ â”‚               per_device_train_batch_size=32,                            â”‚ â”‚
[rank3]: â”‚ â”‚               prediction_loss_only=False,                                â”‚ â”‚
[rank3]: â”‚ â”‚               push_to_hub=False,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               push_to_hub_model_id=None,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               push_to_hub_organization=None,                             â”‚ â”‚
[rank3]: â”‚ â”‚               push_to_hub_token=<PUSH_TO_HUB_TOKEN>,                     â”‚ â”‚
[rank3]: â”‚ â”‚               ray_scope=last,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               remove_unused_columns=False,                               â”‚ â”‚
[rank3]: â”‚ â”‚               report_to=['wandb'],                                       â”‚ â”‚
[rank3]: â”‚ â”‚               restore_callback_states_from_checkpoint=False,             â”‚ â”‚
[rank3]: â”‚ â”‚               resume_from_checkpoint=None,                               â”‚ â”‚
[rank3]: â”‚ â”‚               run_name=None,                                             â”‚ â”‚
[rank3]: â”‚ â”‚               save_on_each_node=False,                                   â”‚ â”‚
[rank3]: â”‚ â”‚               save_only_model=False,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               save_safetensors=True,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               save_steps=500,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               save_strategy=steps,                                       â”‚ â”‚
[rank3]: â”‚ â”‚               save_total_limit=1,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               seed=42,                                                   â”‚ â”‚
[rank3]: â”‚ â”‚               skip_memory_metrics=True,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               split_batches=None,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               tf32=None,                                                 â”‚ â”‚
[rank3]: â”‚ â”‚               torch_compile=False,                                       â”‚ â”‚
[rank3]: â”‚ â”‚               torch_compile_backend=None,                                â”‚ â”‚
[rank3]: â”‚ â”‚               torch_compile_mode=None,                                   â”‚ â”‚
[rank3]: â”‚ â”‚               torch_empty_cache_steps=None,                              â”‚ â”‚
[rank3]: â”‚ â”‚               torchdynamo=None,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               tpu_metrics_debug=False,                                   â”‚ â”‚
[rank3]: â”‚ â”‚               tpu_num_cores=None,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               use_cpu=False,                                             â”‚ â”‚
[rank3]: â”‚ â”‚               use_ipex=False,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               use_legacy_prediction_loop=False,                          â”‚ â”‚
[rank3]: â”‚ â”‚               use_liger_kernel=False,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               use_mps_device=False,                                      â”‚ â”‚
[rank3]: â”‚ â”‚               warmup_ratio=0.0,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               warmup_steps=100,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               weight_decay=0.0,                                          â”‚ â”‚
[rank3]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚                                                          â”‚ â”‚
[rank3]: â”‚ â”‚               output_dir='/ivi/ilps/personal/jqiao/colpali/scripts/confâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   max_length=256,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   run_eval=True,                                         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   run_train=True,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   peft_config=LoraConfig(                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   peft_type=<PeftType.LORA: 'LORA'>,                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   auto_mapping=None,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank3]: â”‚ â”‚               base_model_name_or_path='/ivi/ilps/personal/jqiao/models/â€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   revision=None,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   task_type='FEATURE_EXTRACTION',                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   inference_mode=False,                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   r=32,                                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank3]: â”‚ â”‚               target_modules='(.*(model).*(down_proj|gate_proj|up_proj|â€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   lora_alpha=32,                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   lora_dropout=0.1,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   fan_in_fan_out=False,                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   bias='none',                                       â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   use_rslora=False,                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   modules_to_save=None,                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   init_lora_weights='gaussian',                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   layers_to_transform=None,                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   layers_pattern=None,                               â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   rank_pattern={},                                   â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   alpha_pattern={},                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   megatron_config=None,                              â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   megatron_core='megatron.core',                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   loftq_config={},                                   â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   use_dora=False,                                    â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   layer_replication=None                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   ),                                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   add_suffix=False,                                      â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   processor=ColQwen2Processor:                           â”‚ â”‚
[rank3]: â”‚ â”‚               - image_processor: Qwen2VLImageProcessor {                 â”‚ â”‚
[rank3]: â”‚ â”‚                 "do_convert_rgb": true,                                  â”‚ â”‚
[rank3]: â”‚ â”‚                 "do_normalize": true,                                    â”‚ â”‚
[rank3]: â”‚ â”‚                 "do_rescale": true,                                      â”‚ â”‚
[rank3]: â”‚ â”‚                 "do_resize": true,                                       â”‚ â”‚
[rank3]: â”‚ â”‚                 "image_mean": [                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.48145466,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.4578275,                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.40821073                                             â”‚ â”‚
[rank3]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank3]: â”‚ â”‚                 "image_processor_type": "Qwen2VLImageProcessor",         â”‚ â”‚
[rank3]: â”‚ â”‚                 "image_std": [                                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.26862954,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.26130258,                                            â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   0.27577711                                             â”‚ â”‚
[rank3]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank3]: â”‚ â”‚                 "max_pixels": 12845056,                                  â”‚ â”‚
[rank3]: â”‚ â”‚                 "merge_size": 2,                                         â”‚ â”‚
[rank3]: â”‚ â”‚                 "min_pixels": 3136,                                      â”‚ â”‚
[rank3]: â”‚ â”‚                 "patch_size": 14,                                        â”‚ â”‚
[rank3]: â”‚ â”‚                 "processor_class": "ColQwen2Processor",                  â”‚ â”‚
[rank3]: â”‚ â”‚                 "resample": 3,                                           â”‚ â”‚
[rank3]: â”‚ â”‚                 "rescale_factor": 0.00392156862745098,                   â”‚ â”‚
[rank3]: â”‚ â”‚                 "size": {                                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   "max_pixels": 12845056,                                â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   "min_pixels": 3136                                     â”‚ â”‚
[rank3]: â”‚ â”‚                 },                                                       â”‚ â”‚
[rank3]: â”‚ â”‚                 "temporal_patch_size": 2                                 â”‚ â”‚
[rank3]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank3]: â”‚ â”‚                                                                          â”‚ â”‚
[rank3]: â”‚ â”‚               - tokenizer:                                               â”‚ â”‚
[rank3]: â”‚ â”‚               Qwen2TokenizerFast(name_or_path='/ivi/ilps/personal/jqiaoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               vocab_size=151643, model_max_length=32768, is_fast=True,   â”‚ â”‚
[rank3]: â”‚ â”‚               padding_side='left', truncation_side='right',              â”‚ â”‚
[rank3]: â”‚ â”‚               special_tokens={'eos_token': '<|im_end|>', 'pad_token':    â”‚ â”‚
[rank3]: â”‚ â”‚               '<|endoftext|>', 'additional_special_tokens':              â”‚ â”‚
[rank3]: â”‚ â”‚               ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>',     â”‚ â”‚
[rank3]: â”‚ â”‚               '<|object_ref_end|>', '<|box_start|>', '<|box_end|>',      â”‚ â”‚
[rank3]: â”‚ â”‚               '<|quad_start|>', '<|quad_end|>', '<|vision_start|>',      â”‚ â”‚
[rank3]: â”‚ â”‚               '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>',       â”‚ â”‚
[rank3]: â”‚ â”‚               '<|video_pad|>']}, clean_up_tokenization_spaces=False),    â”‚ â”‚
[rank3]: â”‚ â”‚               added_tokens_decoder={                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151643: AddedToken("<|endoftext|>", rstrip=False,  â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151644: AddedToken("<|im_start|>", rstrip=False,   â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151645: AddedToken("<|im_end|>", rstrip=False,     â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151646: AddedToken("<|object_ref_start|>",         â”‚ â”‚
[rank3]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank3]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151647: AddedToken("<|object_ref_end|>",           â”‚ â”‚
[rank3]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank3]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151648: AddedToken("<|box_start|>", rstrip=False,  â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151649: AddedToken("<|box_end|>", rstrip=False,    â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151650: AddedToken("<|quad_start|>", rstrip=False, â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151651: AddedToken("<|quad_end|>", rstrip=False,   â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151652: AddedToken("<|vision_start|>",             â”‚ â”‚
[rank3]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank3]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151653: AddedToken("<|vision_end|>", rstrip=False, â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151654: AddedToken("<|vision_pad|>", rstrip=False, â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151655: AddedToken("<|image_pad|>", rstrip=False,  â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   151656: AddedToken("<|video_pad|>", rstrip=False,  â”‚ â”‚
[rank3]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank3]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank3]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank3]: â”‚ â”‚                                                                          â”‚ â”‚
[rank3]: â”‚ â”‚               {                                                          â”‚ â”‚
[rank3]: â”‚ â”‚                 "processor_class": "ColQwen2Processor"                   â”‚ â”‚
[rank3]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank3]: â”‚ â”‚               ,                                                          â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   tokenizer=None,                                        â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   loss_func=ColbertPairwiseCELoss(                       â”‚ â”‚
[rank3]: â”‚ â”‚                 (ce_loss): CrossEntropyLoss()                            â”‚ â”‚
[rank3]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   dataset_loading_func=<function load_train_set at       â”‚ â”‚
[rank3]: â”‚ â”‚               0x7f3ce68abeb0>,                                           â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   eval_dataset_loader={                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_energy':                           â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fac0>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_healthcare_industry':              â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008f6d0>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_artificial_intelligence_test':     â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008f5b0>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_government_reports':               â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008f9d0>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'infovqa_subsampled':                              â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008f940>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'docvqa_subsampled':                               â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fb20>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'arxivqa_subsampled':                              â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fb80>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'tabfquad_subsampled':                             â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fbe0>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'tatdqa':                                          â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fc40>,                                 â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   â”‚   'shift_project':                                   â”‚ â”‚
[rank3]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚               object at 0x7f3ca008fca0>                                  â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   },                                                     â”‚ â”‚
[rank3]: â”‚ â”‚               â”‚   pretrained_peft_model_name_or_path=None                â”‚ â”‚
[rank3]: â”‚ â”‚               )                                                          â”‚ â”‚
[rank3]: â”‚ â”‚ config_file = PosixPath('/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank3]: â”‚ /colpali_engine/trainer/colmodel_training.py:224 in eval                     â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   221 â”‚   â”‚   if self.config.eval_dataset_loader is not None:                â”‚
[rank3]: â”‚   222 â”‚   â”‚   â”‚   for test_name, test_dataset_loading_func in self.config.ev â”‚
[rank3]: â”‚   223 â”‚   â”‚   â”‚   â”‚   print(f"Evaluating {test_name}")                       â”‚
[rank3]: â”‚ â± 224 â”‚   â”‚   â”‚   â”‚   test_ds = test_dataset_loading_func()                  â”‚
[rank3]: â”‚   225 â”‚   â”‚   â”‚   â”‚   metrics = self.eval_dataset(test_ds)                   â”‚
[rank3]: â”‚   226 â”‚   â”‚   â”‚   â”‚   all_metrics[test_name] = metrics                       â”‚
[rank3]: â”‚   227 â”‚   â”‚   â”‚   â”‚   print(f"Metrics for {test_name}: {metrics}")           â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚               all_metrics = {                                            â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'validation_set': {                      â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_1': 0.842,                  â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_3': 0.88859,                â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_5': 0.89281,                â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_10': 0.89876,               â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_20': 0.90318,               â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_50': 0.90714,               â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_100': 0.90876,              â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'map_at_1': 0.842,                   â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'map_at_3': 0.87767,                 â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   'map_at_5': 0.88007,                 â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   â”‚   ... +46                              â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   }                                        â”‚ â”‚
[rank3]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank3]: â”‚ â”‚                   metrics = {                                            â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_1': 0.842,                      â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_3': 0.88859,                    â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_5': 0.89281,                    â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_10': 0.89876,                   â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_20': 0.90318,                   â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_50': 0.90714,                   â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'ndcg_at_100': 0.90876,                  â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'map_at_1': 0.842,                       â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'map_at_3': 0.87767,                     â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   'map_at_5': 0.88007,                     â”‚ â”‚
[rank3]: â”‚ â”‚                             â”‚   ... +46                                  â”‚ â”‚
[rank3]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank3]: â”‚ â”‚                      self = <colpali_engine.trainer.colmodel_training.Câ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚                             object at 0x7f3ce66fe650>                    â”‚ â”‚
[rank3]: â”‚ â”‚ test_dataset_loading_func = <colpali_engine.utils.dataset_transformatioâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚                             object at 0x7f3ca008fac0>                    â”‚ â”‚
[rank3]: â”‚ â”‚                 test_name = 'syntheticDocQA_energy'                      â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank3]: â”‚ /colpali_engine/utils/dataset_transformation.py:184 in __call__              â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   181 â”‚   â”‚   self.dataset_path = dataset_path                               â”‚
[rank3]: â”‚   182 â”‚                                                                      â”‚
[rank3]: â”‚   183 â”‚   def __call__(self, *args, **kwargs):                               â”‚
[rank3]: â”‚ â± 184 â”‚   â”‚   dataset = load_dataset(self.dataset_path, split="test")        â”‚
[rank3]: â”‚   185 â”‚   â”‚   return dataset                                                 â”‚
[rank3]: â”‚   186                                                                        â”‚
[rank3]: â”‚   187                                                                        â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚   args = ()                                                              â”‚ â”‚
[rank3]: â”‚ â”‚ kwargs = {}                                                              â”‚ â”‚
[rank3]: â”‚ â”‚   self = <colpali_engine.utils.dataset_transformation.TestSetFactory     â”‚ â”‚
[rank3]: â”‚ â”‚          object at 0x7f3ca008fac0>                                       â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank3]: â”‚ /datasets/load.py:2132 in load_dataset                                       â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   2129 â”‚   )                                                                 â”‚
[rank3]: â”‚   2130 â”‚                                                                     â”‚
[rank3]: â”‚   2131 â”‚   # Create a dataset builder                                        â”‚
[rank3]: â”‚ â± 2132 â”‚   builder_instance = load_dataset_builder(                          â”‚
[rank3]: â”‚   2133 â”‚   â”‚   path=path,                                                    â”‚
[rank3]: â”‚   2134 â”‚   â”‚   name=name,                                                    â”‚
[rank3]: â”‚   2135 â”‚   â”‚   data_dir=data_dir,                                            â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚         cache_dir = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚     config_kwargs = {}                                                   â”‚ â”‚
[rank3]: â”‚ â”‚          data_dir = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚        data_files = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚   download_config = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚     download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:               â”‚ â”‚
[rank3]: â”‚ â”‚                     'reuse_dataset_if_exists'>                           â”‚ â”‚
[rank3]: â”‚ â”‚          features = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚    keep_in_memory = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚              name = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚          num_proc = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚              path = '/ivi/ilps/personal/jqiao/colpali/scripts/configs/qâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚          revision = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚        save_infos = False                                                â”‚ â”‚
[rank3]: â”‚ â”‚             split = 'test'                                               â”‚ â”‚
[rank3]: â”‚ â”‚   storage_options = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚         streaming = False                                                â”‚ â”‚
[rank3]: â”‚ â”‚             token = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚ trust_remote_code = None                                                 â”‚ â”‚
[rank3]: â”‚ â”‚ verification_mode = <VerificationMode.BASIC_CHECKS: 'basic_checks'>      â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank3]: â”‚ /datasets/load.py:1853 in load_dataset_builder                               â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   1850 â”‚   if storage_options is not None:                                   â”‚
[rank3]: â”‚   1851 â”‚   â”‚   download_config = download_config.copy() if download_config e â”‚
[rank3]: â”‚   1852 â”‚   â”‚   download_config.storage_options.update(storage_options)       â”‚
[rank3]: â”‚ â± 1853 â”‚   dataset_module = dataset_module_factory(                          â”‚
[rank3]: â”‚   1854 â”‚   â”‚   path,                                                         â”‚
[rank3]: â”‚   1855 â”‚   â”‚   revision=revision,                                            â”‚
[rank3]: â”‚   1856 â”‚   â”‚   download_config=download_config,                              â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank3]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                config_kwargs = {}                                        â”‚ â”‚
[rank3]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚              download_config = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank3]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank3]: â”‚ â”‚                     features = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                         name = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚              storage_options = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                        token = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank3]: â”‚ /datasets/load.py:1735 in dataset_module_factory                             â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚   1732 â”‚   â”‚   â”‚   f"Couldn't find a dataset script at {relative_to_absolute â”‚
[rank3]: â”‚   1733 â”‚   â”‚   )                                                             â”‚
[rank3]: â”‚   1734 â”‚   else:                                                             â”‚
[rank3]: â”‚ â± 1735 â”‚   â”‚   raise FileNotFoundError(f"Couldn't find any data file at {rel â”‚
[rank3]: â”‚   1736                                                                       â”‚
[rank3]: â”‚   1737                                                                       â”‚
[rank3]: â”‚   1738 def load_dataset_builder(                                             â”‚
[rank3]: â”‚                                                                              â”‚
[rank3]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank3]: â”‚ â”‚      _require_custom_configs = False                                     â”‚ â”‚
[rank3]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank3]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                combined_path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚              download_config = DownloadConfig(                           â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   cache_dir=None,                       â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   force_download=False,                 â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   resume_download=False,                â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   local_files_only=False,               â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   proxies=None,                         â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   user_agent=None,                      â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   extract_compressed_file=True,         â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   force_extract=True,                   â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   delete_extracted=False,               â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   extract_on_the_fly=False,             â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   use_etag=True,                        â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   num_proc=None,                        â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   max_retries=1,                        â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   token=None,                           â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   storage_options={},                   â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   download_desc=None,                   â”‚ â”‚
[rank3]: â”‚ â”‚                                â”‚   disable_tqdm=False                    â”‚ â”‚
[rank3]: â”‚ â”‚                                )                                         â”‚ â”‚
[rank3]: â”‚ â”‚              download_kwargs = {}                                        â”‚ â”‚
[rank3]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank3]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank3]: â”‚ â”‚         dynamic_modules_path = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚                     filename = 'syntheticDocQA_energy_test.py'           â”‚ â”‚
[rank3]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank3]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank3]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank3]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank3]: â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank3]: FileNotFoundError: Couldn't find any data file at 
[rank3]: /ivi/ilps/personal/jqiao/colpali/data_dir/syntheticDocQA_energy_test.
MTEB metrics: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Metrics for validation set: {'ndcg_at_1': 0.842, 'ndcg_at_3': 0.88859, 'ndcg_at_5': 0.89281, 'ndcg_at_10': 0.89876, 'ndcg_at_20': 0.90318, 'ndcg_at_50': 0.90714, 'ndcg_at_100': 0.90876, 'map_at_1': 0.842, 'map_at_3': 0.87767, 'map_at_5': 0.88007, 'map_at_10': 0.8826, 'map_at_20': 0.88373, 'map_at_50': 0.88436, 'map_at_100': 0.8845, 'recall_at_1': 0.842, 'recall_at_3': 0.92, 'recall_at_5': 0.93, 'recall_at_10': 0.948, 'recall_at_20': 0.966, 'recall_at_50': 0.986, 'recall_at_100': 0.996, 'precision_at_1': 0.842, 'precision_at_3': 0.30667, 'precision_at_5': 0.186, 'precision_at_10': 0.0948, 'precision_at_20': 0.0483, 'precision_at_50': 0.01972, 'precision_at_100': 0.00996, 'mrr_at_1': 0.842, 'mrr_at_3': 0.8776666666666665, 'mrr_at_5': 0.8800666666666664, 'mrr_at_10': 0.8825960317460317, 'mrr_at_20': 0.8837334626158156, 'mrr_at_50': 0.8843634648309222, 'mrr_at_100': 0.8845040192992382, 'naucs_at_1_max': 0.16046754149648104, 'naucs_at_1_std': -0.31849557412258395, 'naucs_at_1_diff1': 0.9447851618269626, 'naucs_at_3_max': 0.1531979458450056, 'naucs_at_3_std': -0.09795751633986692, 'naucs_at_3_diff1': 0.9246031746031768, 'naucs_at_5_max': 0.135427504335068, 'naucs_at_5_std': -0.005162064825928207, 'naucs_at_5_diff1': 0.9250366813392017, 'naucs_at_10_max': 0.06683186094950792, 'naucs_at_10_std': 0.1415463621345969, 'naucs_at_10_diff1': 0.9470300940889194, 'naucs_at_20_max': 0.07203273466249606, 'naucs_at_20_std': 0.2628384687208281, 'naucs_at_20_diff1': 0.9605920799692436, 'naucs_at_50_max': 0.16353207949846627, 'naucs_at_50_std': 0.35194077631051657, 'naucs_at_50_diff1': 0.9416433239962615, 'naucs_at_100_max': 0.7770774976657331, 'naucs_at_100_std': 0.12278244631180925, 'naucs_at_100_diff1': 0.9346405228758466}
Evaluating syntheticDocQA_energy
[rank0]: â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank0]: â”‚ /ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py:26 in main   â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   23 â”‚   â”‚   app.save(config_file=config_file)                               â”‚
[rank0]: â”‚   24 â”‚   if config.run_eval:                                                 â”‚
[rank0]: â”‚   25 â”‚   â”‚   print("Running evaluation")                                     â”‚
[rank0]: â”‚ â± 26 â”‚   â”‚   app.eval()                                                      â”‚
[rank0]: â”‚   27 â”‚   print("Done!")                                                      â”‚
[rank0]: â”‚   28                                                                         â”‚
[rank0]: â”‚   29                                                                         â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚         app = <colpali_engine.trainer.colmodel_training.ColModelTraining â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd32ab0a650>                                  â”‚ â”‚
[rank0]: â”‚ â”‚      config = ColModelTrainingConfig(                                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   model=PeftModelForFeatureExtraction(                   â”‚ â”‚
[rank0]: â”‚ â”‚                 (base_model): LoraModel(                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   (model): ColQwen2(                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     (visual): Qwen2VisionTransformerPretrainedModel(     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (patch_embed): PatchEmbed(                         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), â”‚ â”‚
[rank0]: â”‚ â”‚               stride=(2, 14, 14), bias=False)                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (rotary_pos_emb): VisionRotaryEmbedding()          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (blocks): ModuleList(                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (0-31): 32 x Qwen2VLVisionBlock(                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm1): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank0]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm2): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank0]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (attn): VisionFlashAttention2(                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (qkv): Linear(in_features=1280,              â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=3840, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (proj): Linear(in_features=1280,             â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): VisionMlp(                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc1): Linear(in_features=1280,              â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (act): QuickGELUActivation()                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc2): Linear(in_features=5120,              â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (merger): PatchMerger(                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (ln_q): LayerNorm((1280,), eps=1e-06,            â”‚ â”‚
[rank0]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (mlp): Sequential(                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (0): Linear(in_features=5120,                  â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (1): GELU(approximate='none')                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (2): Linear(in_features=5120,                  â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     (model): Qwen2VLModel(                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (embed_tokens): Embedding(151936, 1536)            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (layers): ModuleList(                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (0-27): 28 x Qwen2VLDecoderLayer(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (self_attn): Qwen2VLFlashAttention2(           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (q_proj): lora.Linear(                       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (k_proj): lora.Linear(                       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (v_proj): lora.Linear(                       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (o_proj): lora.Linear(                       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (rotary_emb): Qwen2VLRotaryEmbedding()       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): Qwen2MLP(                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (gate_proj): lora.Linear(                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (up_proj): lora.Linear(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (down_proj): lora.Linear(                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=8960,     â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=8960,      â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚     (act_fn): SiLU()                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (input_layernorm): Qwen2RMSNorm((1536,),       â”‚ â”‚
[rank0]: â”‚ â”‚               eps=1e-06)                                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   â”‚   (post_attention_layernorm):                    â”‚ â”‚
[rank0]: â”‚ â”‚               Qwen2RMSNorm((1536,), eps=1e-06)                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (norm): Qwen2RMSNorm((1536,), eps=1e-06)           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (rotary_emb): Qwen2VLRotaryEmbedding()             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     (lm_head): Linear(in_features=1536,                  â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=151936, bias=False)                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     (custom_text_proj): lora.Linear(                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (base_layer): Linear(in_features=1536,             â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=128, bias=True)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (lora_dropout): ModuleDict(                        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (default): Dropout(p=0.1, inplace=False)         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (lora_A): ModuleDict(                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=1536,              â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (lora_B): ModuleDict(                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=32,                â”‚ â”‚
[rank0]: â”‚ â”‚               out_features=128, bias=False)                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_A): ParameterDict()                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_B): ParameterDict()                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   )                                                      â”‚ â”‚
[rank0]: â”‚ â”‚                 )                                                        â”‚ â”‚
[rank0]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   tr_args=TrainingArguments(                             â”‚ â”‚
[rank0]: â”‚ â”‚               _n_gpu=1,                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               accelerator_config={'split_batches': False,                â”‚ â”‚
[rank0]: â”‚ â”‚               'dispatch_batches': None, 'even_batches': True,            â”‚ â”‚
[rank0]: â”‚ â”‚               'use_seedable_sampler': True, 'non_blocking': False,       â”‚ â”‚
[rank0]: â”‚ â”‚               'gradient_accumulation_kwargs': None,                      â”‚ â”‚
[rank0]: â”‚ â”‚               'use_configured_state': False},                            â”‚ â”‚
[rank0]: â”‚ â”‚               adafactor=False,                                           â”‚ â”‚
[rank0]: â”‚ â”‚               adam_beta1=0.9,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               adam_beta2=0.999,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               adam_epsilon=1e-08,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               auto_find_batch_size=False,                                â”‚ â”‚
[rank0]: â”‚ â”‚               average_tokens_across_devices=False,                       â”‚ â”‚
[rank0]: â”‚ â”‚               batch_eval_metrics=False,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               bf16=True,                                                 â”‚ â”‚
[rank0]: â”‚ â”‚               bf16_full_eval=False,                                      â”‚ â”‚
[rank0]: â”‚ â”‚               data_seed=None,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               dataloader_drop_last=False,                                â”‚ â”‚
[rank0]: â”‚ â”‚               dataloader_num_workers=8,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               dataloader_persistent_workers=False,                       â”‚ â”‚
[rank0]: â”‚ â”‚               dataloader_pin_memory=True,                                â”‚ â”‚
[rank0]: â”‚ â”‚               dataloader_prefetch_factor=None,                           â”‚ â”‚
[rank0]: â”‚ â”‚               ddp_backend=None,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               ddp_broadcast_buffers=None,                                â”‚ â”‚
[rank0]: â”‚ â”‚               ddp_bucket_cap_mb=None,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               ddp_find_unused_parameters=None,                           â”‚ â”‚
[rank0]: â”‚ â”‚               ddp_timeout=1800,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               debug=[],                                                  â”‚ â”‚
[rank0]: â”‚ â”‚               deepspeed=None,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               disable_tqdm=False,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               dispatch_batches=None,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               do_eval=True,                                              â”‚ â”‚
[rank0]: â”‚ â”‚               do_predict=False,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               do_train=False,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               eval_accumulation_steps=None,                              â”‚ â”‚
[rank0]: â”‚ â”‚               eval_delay=0,                                              â”‚ â”‚
[rank0]: â”‚ â”‚               eval_do_concat_batches=True,                               â”‚ â”‚
[rank0]: â”‚ â”‚               eval_on_start=False,                                       â”‚ â”‚
[rank0]: â”‚ â”‚               eval_steps=100,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               eval_strategy=steps,                                       â”‚ â”‚
[rank0]: â”‚ â”‚               eval_use_gather_object=False,                              â”‚ â”‚
[rank0]: â”‚ â”‚               evaluation_strategy=None,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               fp16=False,                                                â”‚ â”‚
[rank0]: â”‚ â”‚               fp16_backend=auto,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               fp16_full_eval=False,                                      â”‚ â”‚
[rank0]: â”‚ â”‚               fp16_opt_level=O1,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               fsdp=[],                                                   â”‚ â”‚
[rank0]: â”‚ â”‚               fsdp_config={'min_num_params': 0, 'xla': False,            â”‚ â”‚
[rank0]: â”‚ â”‚               'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},        â”‚ â”‚
[rank0]: â”‚ â”‚               fsdp_min_num_params=0,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               fsdp_transformer_layer_cls_to_wrap=None,                   â”‚ â”‚
[rank0]: â”‚ â”‚               full_determinism=False,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               gradient_accumulation_steps=1,                             â”‚ â”‚
[rank0]: â”‚ â”‚               gradient_checkpointing=True,                               â”‚ â”‚
[rank0]: â”‚ â”‚               gradient_checkpointing_kwargs={'use_reentrant': False},    â”‚ â”‚
[rank0]: â”‚ â”‚               greater_is_better=None,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               group_by_length=False,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               half_precision_backend=auto,                               â”‚ â”‚
[rank0]: â”‚ â”‚               hub_always_push=False,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               hub_model_id=None,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               hub_private_repo=False,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               hub_strategy=every_save,                                   â”‚ â”‚
[rank0]: â”‚ â”‚               hub_token=<HUB_TOKEN>,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               ignore_data_skip=False,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               include_for_metrics=[],                                    â”‚ â”‚
[rank0]: â”‚ â”‚               include_inputs_for_metrics=False,                          â”‚ â”‚
[rank0]: â”‚ â”‚               include_num_input_tokens_seen=False,                       â”‚ â”‚
[rank0]: â”‚ â”‚               include_tokens_per_second=False,                           â”‚ â”‚
[rank0]: â”‚ â”‚               jit_mode_eval=False,                                       â”‚ â”‚
[rank0]: â”‚ â”‚               label_names=None,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               label_smoothing_factor=0.0,                                â”‚ â”‚
[rank0]: â”‚ â”‚               learning_rate=0.0005,                                      â”‚ â”‚
[rank0]: â”‚ â”‚               length_column_name=length,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               load_best_model_at_end=False,                              â”‚ â”‚
[rank0]: â”‚ â”‚               local_rank=0,                                              â”‚ â”‚
[rank0]: â”‚ â”‚               log_level=passive,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               log_level_replica=warning,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               log_on_each_node=True,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               logging_dir=None,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               logging_first_step=False,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               logging_nan_inf_filter=True,                               â”‚ â”‚
[rank0]: â”‚ â”‚               logging_steps=10,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               logging_strategy=steps,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               lr_scheduler_kwargs={},                                    â”‚ â”‚
[rank0]: â”‚ â”‚               lr_scheduler_type=linear,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               max_grad_norm=1.0,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               max_steps=-1,                                              â”‚ â”‚
[rank0]: â”‚ â”‚               metric_for_best_model=None,                                â”‚ â”‚
[rank0]: â”‚ â”‚               mp_parameters=,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               neftune_noise_alpha=None,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               no_cuda=False,                                             â”‚ â”‚
[rank0]: â”‚ â”‚               num_train_epochs=5,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               optim=adamw_torch,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               optim_args=None,                                           â”‚ â”‚
[rank0]: â”‚ â”‚               optim_target_modules=None,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               output_dir=/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               overwrite_output_dir=True,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               past_index=-1,                                             â”‚ â”‚
[rank0]: â”‚ â”‚               per_device_eval_batch_size=8,                              â”‚ â”‚
[rank0]: â”‚ â”‚               per_device_train_batch_size=32,                            â”‚ â”‚
[rank0]: â”‚ â”‚               prediction_loss_only=False,                                â”‚ â”‚
[rank0]: â”‚ â”‚               push_to_hub=False,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               push_to_hub_model_id=None,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               push_to_hub_organization=None,                             â”‚ â”‚
[rank0]: â”‚ â”‚               push_to_hub_token=<PUSH_TO_HUB_TOKEN>,                     â”‚ â”‚
[rank0]: â”‚ â”‚               ray_scope=last,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               remove_unused_columns=False,                               â”‚ â”‚
[rank0]: â”‚ â”‚               report_to=['wandb'],                                       â”‚ â”‚
[rank0]: â”‚ â”‚               restore_callback_states_from_checkpoint=False,             â”‚ â”‚
[rank0]: â”‚ â”‚               resume_from_checkpoint=None,                               â”‚ â”‚
[rank0]: â”‚ â”‚               run_name=None,                                             â”‚ â”‚
[rank0]: â”‚ â”‚               save_on_each_node=False,                                   â”‚ â”‚
[rank0]: â”‚ â”‚               save_only_model=False,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               save_safetensors=True,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               save_steps=500,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               save_strategy=steps,                                       â”‚ â”‚
[rank0]: â”‚ â”‚               save_total_limit=1,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               seed=42,                                                   â”‚ â”‚
[rank0]: â”‚ â”‚               skip_memory_metrics=True,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               split_batches=None,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               tf32=None,                                                 â”‚ â”‚
[rank0]: â”‚ â”‚               torch_compile=False,                                       â”‚ â”‚
[rank0]: â”‚ â”‚               torch_compile_backend=None,                                â”‚ â”‚
[rank0]: â”‚ â”‚               torch_compile_mode=None,                                   â”‚ â”‚
[rank0]: â”‚ â”‚               torch_empty_cache_steps=None,                              â”‚ â”‚
[rank0]: â”‚ â”‚               torchdynamo=None,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               tpu_metrics_debug=False,                                   â”‚ â”‚
[rank0]: â”‚ â”‚               tpu_num_cores=None,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               use_cpu=False,                                             â”‚ â”‚
[rank0]: â”‚ â”‚               use_ipex=False,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               use_legacy_prediction_loop=False,                          â”‚ â”‚
[rank0]: â”‚ â”‚               use_liger_kernel=False,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               use_mps_device=False,                                      â”‚ â”‚
[rank0]: â”‚ â”‚               warmup_ratio=0.0,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               warmup_steps=100,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               weight_decay=0.0,                                          â”‚ â”‚
[rank0]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚                                                          â”‚ â”‚
[rank0]: â”‚ â”‚               output_dir='/ivi/ilps/personal/jqiao/colpali/scripts/confâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   max_length=256,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   run_eval=True,                                         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   run_train=True,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   peft_config=LoraConfig(                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   peft_type=<PeftType.LORA: 'LORA'>,                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   auto_mapping=None,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank0]: â”‚ â”‚               base_model_name_or_path='/ivi/ilps/personal/jqiao/models/â€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   revision=None,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   task_type='FEATURE_EXTRACTION',                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   inference_mode=False,                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   r=32,                                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank0]: â”‚ â”‚               target_modules='(.*(model).*(down_proj|gate_proj|up_proj|â€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   lora_alpha=32,                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   lora_dropout=0.1,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   fan_in_fan_out=False,                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   bias='none',                                       â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   use_rslora=False,                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   modules_to_save=None,                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   init_lora_weights='gaussian',                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   layers_to_transform=None,                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   layers_pattern=None,                               â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   rank_pattern={},                                   â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   alpha_pattern={},                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   megatron_config=None,                              â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   megatron_core='megatron.core',                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   loftq_config={},                                   â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   use_dora=False,                                    â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   layer_replication=None                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   ),                                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   add_suffix=False,                                      â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   processor=ColQwen2Processor:                           â”‚ â”‚
[rank0]: â”‚ â”‚               - image_processor: Qwen2VLImageProcessor {                 â”‚ â”‚
[rank0]: â”‚ â”‚                 "do_convert_rgb": true,                                  â”‚ â”‚
[rank0]: â”‚ â”‚                 "do_normalize": true,                                    â”‚ â”‚
[rank0]: â”‚ â”‚                 "do_rescale": true,                                      â”‚ â”‚
[rank0]: â”‚ â”‚                 "do_resize": true,                                       â”‚ â”‚
[rank0]: â”‚ â”‚                 "image_mean": [                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.48145466,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.4578275,                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.40821073                                             â”‚ â”‚
[rank0]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank0]: â”‚ â”‚                 "image_processor_type": "Qwen2VLImageProcessor",         â”‚ â”‚
[rank0]: â”‚ â”‚                 "image_std": [                                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.26862954,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.26130258,                                            â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   0.27577711                                             â”‚ â”‚
[rank0]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank0]: â”‚ â”‚                 "max_pixels": 12845056,                                  â”‚ â”‚
[rank0]: â”‚ â”‚                 "merge_size": 2,                                         â”‚ â”‚
[rank0]: â”‚ â”‚                 "min_pixels": 3136,                                      â”‚ â”‚
[rank0]: â”‚ â”‚                 "patch_size": 14,                                        â”‚ â”‚
[rank0]: â”‚ â”‚                 "processor_class": "ColQwen2Processor",                  â”‚ â”‚
[rank0]: â”‚ â”‚                 "resample": 3,                                           â”‚ â”‚
[rank0]: â”‚ â”‚                 "rescale_factor": 0.00392156862745098,                   â”‚ â”‚
[rank0]: â”‚ â”‚                 "size": {                                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   "max_pixels": 12845056,                                â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   "min_pixels": 3136                                     â”‚ â”‚
[rank0]: â”‚ â”‚                 },                                                       â”‚ â”‚
[rank0]: â”‚ â”‚                 "temporal_patch_size": 2                                 â”‚ â”‚
[rank0]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank0]: â”‚ â”‚                                                                          â”‚ â”‚
[rank0]: â”‚ â”‚               - tokenizer:                                               â”‚ â”‚
[rank0]: â”‚ â”‚               Qwen2TokenizerFast(name_or_path='/ivi/ilps/personal/jqiaoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               vocab_size=151643, model_max_length=32768, is_fast=True,   â”‚ â”‚
[rank0]: â”‚ â”‚               padding_side='left', truncation_side='right',              â”‚ â”‚
[rank0]: â”‚ â”‚               special_tokens={'eos_token': '<|im_end|>', 'pad_token':    â”‚ â”‚
[rank0]: â”‚ â”‚               '<|endoftext|>', 'additional_special_tokens':              â”‚ â”‚
[rank0]: â”‚ â”‚               ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>',     â”‚ â”‚
[rank0]: â”‚ â”‚               '<|object_ref_end|>', '<|box_start|>', '<|box_end|>',      â”‚ â”‚
[rank0]: â”‚ â”‚               '<|quad_start|>', '<|quad_end|>', '<|vision_start|>',      â”‚ â”‚
[rank0]: â”‚ â”‚               '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>',       â”‚ â”‚
[rank0]: â”‚ â”‚               '<|video_pad|>']}, clean_up_tokenization_spaces=False),    â”‚ â”‚
[rank0]: â”‚ â”‚               added_tokens_decoder={                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151643: AddedToken("<|endoftext|>", rstrip=False,  â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151644: AddedToken("<|im_start|>", rstrip=False,   â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151645: AddedToken("<|im_end|>", rstrip=False,     â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151646: AddedToken("<|object_ref_start|>",         â”‚ â”‚
[rank0]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank0]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151647: AddedToken("<|object_ref_end|>",           â”‚ â”‚
[rank0]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank0]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151648: AddedToken("<|box_start|>", rstrip=False,  â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151649: AddedToken("<|box_end|>", rstrip=False,    â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151650: AddedToken("<|quad_start|>", rstrip=False, â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151651: AddedToken("<|quad_end|>", rstrip=False,   â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151652: AddedToken("<|vision_start|>",             â”‚ â”‚
[rank0]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank0]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151653: AddedToken("<|vision_end|>", rstrip=False, â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151654: AddedToken("<|vision_pad|>", rstrip=False, â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151655: AddedToken("<|image_pad|>", rstrip=False,  â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   151656: AddedToken("<|video_pad|>", rstrip=False,  â”‚ â”‚
[rank0]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank0]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank0]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank0]: â”‚ â”‚                                                                          â”‚ â”‚
[rank0]: â”‚ â”‚               {                                                          â”‚ â”‚
[rank0]: â”‚ â”‚                 "processor_class": "ColQwen2Processor"                   â”‚ â”‚
[rank0]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank0]: â”‚ â”‚               ,                                                          â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   tokenizer=None,                                        â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   loss_func=ColbertPairwiseCELoss(                       â”‚ â”‚
[rank0]: â”‚ â”‚                 (ce_loss): CrossEntropyLoss()                            â”‚ â”‚
[rank0]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   dataset_loading_func=<function load_train_set at       â”‚ â”‚
[rank0]: â”‚ â”‚               0x7fd32acb3eb0>,                                           â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   eval_dataset_loader={                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_energy':                           â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afac0>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_healthcare_industry':              â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44af6d0>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_artificial_intelligence_test':     â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44af5b0>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_government_reports':               â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44af9d0>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'infovqa_subsampled':                              â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44af940>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'docvqa_subsampled':                               â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afb20>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'arxivqa_subsampled':                              â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afb80>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'tabfquad_subsampled':                             â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afbe0>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'tatdqa':                                          â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afc40>,                                 â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   â”‚   'shift_project':                                   â”‚ â”‚
[rank0]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚               object at 0x7fd2a44afca0>                                  â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   },                                                     â”‚ â”‚
[rank0]: â”‚ â”‚               â”‚   pretrained_peft_model_name_or_path=None                â”‚ â”‚
[rank0]: â”‚ â”‚               )                                                          â”‚ â”‚
[rank0]: â”‚ â”‚ config_file = PosixPath('/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank0]: â”‚ /colpali_engine/trainer/colmodel_training.py:224 in eval                     â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   221 â”‚   â”‚   if self.config.eval_dataset_loader is not None:                â”‚
[rank0]: â”‚   222 â”‚   â”‚   â”‚   for test_name, test_dataset_loading_func in self.config.ev â”‚
[rank0]: â”‚   223 â”‚   â”‚   â”‚   â”‚   print(f"Evaluating {test_name}")                       â”‚
[rank0]: â”‚ â± 224 â”‚   â”‚   â”‚   â”‚   test_ds = test_dataset_loading_func()                  â”‚
[rank0]: â”‚   225 â”‚   â”‚   â”‚   â”‚   metrics = self.eval_dataset(test_ds)                   â”‚
[rank0]: â”‚   226 â”‚   â”‚   â”‚   â”‚   all_metrics[test_name] = metrics                       â”‚
[rank0]: â”‚   227 â”‚   â”‚   â”‚   â”‚   print(f"Metrics for {test_name}: {metrics}")           â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚               all_metrics = {                                            â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'validation_set': {                      â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_1': 0.842,                  â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_3': 0.88859,                â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_5': 0.89281,                â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_10': 0.89876,               â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_20': 0.90318,               â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_50': 0.90714,               â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_100': 0.90876,              â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'map_at_1': 0.842,                   â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'map_at_3': 0.87767,                 â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   'map_at_5': 0.88007,                 â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   â”‚   ... +46                              â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   }                                        â”‚ â”‚
[rank0]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank0]: â”‚ â”‚                   metrics = {                                            â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_1': 0.842,                      â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_3': 0.88859,                    â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_5': 0.89281,                    â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_10': 0.89876,                   â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_20': 0.90318,                   â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_50': 0.90714,                   â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'ndcg_at_100': 0.90876,                  â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'map_at_1': 0.842,                       â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'map_at_3': 0.87767,                     â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   'map_at_5': 0.88007,                     â”‚ â”‚
[rank0]: â”‚ â”‚                             â”‚   ... +46                                  â”‚ â”‚
[rank0]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank0]: â”‚ â”‚                      self = <colpali_engine.trainer.colmodel_training.Câ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚                             object at 0x7fd32ab0a650>                    â”‚ â”‚
[rank0]: â”‚ â”‚ test_dataset_loading_func = <colpali_engine.utils.dataset_transformatioâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚                             object at 0x7fd2a44afac0>                    â”‚ â”‚
[rank0]: â”‚ â”‚                 test_name = 'syntheticDocQA_energy'                      â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank0]: â”‚ /colpali_engine/utils/dataset_transformation.py:184 in __call__              â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   181 â”‚   â”‚   self.dataset_path = dataset_path                               â”‚
[rank0]: â”‚   182 â”‚                                                                      â”‚
[rank0]: â”‚   183 â”‚   def __call__(self, *args, **kwargs):                               â”‚
[rank0]: â”‚ â± 184 â”‚   â”‚   dataset = load_dataset(self.dataset_path, split="test")        â”‚
[rank0]: â”‚   185 â”‚   â”‚   return dataset                                                 â”‚
[rank0]: â”‚   186                                                                        â”‚
[rank0]: â”‚   187                                                                        â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚   args = ()                                                              â”‚ â”‚
[rank0]: â”‚ â”‚ kwargs = {}                                                              â”‚ â”‚
[rank0]: â”‚ â”‚   self = <colpali_engine.utils.dataset_transformation.TestSetFactory     â”‚ â”‚
[rank0]: â”‚ â”‚          object at 0x7fd2a44afac0>                                       â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank0]: â”‚ /datasets/load.py:2132 in load_dataset                                       â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   2129 â”‚   )                                                                 â”‚
[rank0]: â”‚   2130 â”‚                                                                     â”‚
[rank0]: â”‚   2131 â”‚   # Create a dataset builder                                        â”‚
[rank0]: â”‚ â± 2132 â”‚   builder_instance = load_dataset_builder(                          â”‚
[rank0]: â”‚   2133 â”‚   â”‚   path=path,                                                    â”‚
[rank0]: â”‚   2134 â”‚   â”‚   name=name,                                                    â”‚
[rank0]: â”‚   2135 â”‚   â”‚   data_dir=data_dir,                                            â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚         cache_dir = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚     config_kwargs = {}                                                   â”‚ â”‚
[rank0]: â”‚ â”‚          data_dir = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚        data_files = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚   download_config = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚     download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:               â”‚ â”‚
[rank0]: â”‚ â”‚                     'reuse_dataset_if_exists'>                           â”‚ â”‚
[rank0]: â”‚ â”‚          features = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚    keep_in_memory = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚              name = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚          num_proc = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚              path = '/ivi/ilps/personal/jqiao/colpali/scripts/configs/qâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚          revision = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚        save_infos = False                                                â”‚ â”‚
[rank0]: â”‚ â”‚             split = 'test'                                               â”‚ â”‚
[rank0]: â”‚ â”‚   storage_options = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚         streaming = False                                                â”‚ â”‚
[rank0]: â”‚ â”‚             token = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚ trust_remote_code = None                                                 â”‚ â”‚
[rank0]: â”‚ â”‚ verification_mode = <VerificationMode.BASIC_CHECKS: 'basic_checks'>      â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank0]: â”‚ /datasets/load.py:1853 in load_dataset_builder                               â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   1850 â”‚   if storage_options is not None:                                   â”‚
[rank0]: â”‚   1851 â”‚   â”‚   download_config = download_config.copy() if download_config e â”‚
[rank0]: â”‚   1852 â”‚   â”‚   download_config.storage_options.update(storage_options)       â”‚
[rank0]: â”‚ â± 1853 â”‚   dataset_module = dataset_module_factory(                          â”‚
[rank0]: â”‚   1854 â”‚   â”‚   path,                                                         â”‚
[rank0]: â”‚   1855 â”‚   â”‚   revision=revision,                                            â”‚
[rank0]: â”‚   1856 â”‚   â”‚   download_config=download_config,                              â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank0]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                config_kwargs = {}                                        â”‚ â”‚
[rank0]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚              download_config = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank0]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank0]: â”‚ â”‚                     features = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                         name = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚              storage_options = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                        token = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank0]: â”‚ /datasets/load.py:1735 in dataset_module_factory                             â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚   1732 â”‚   â”‚   â”‚   f"Couldn't find a dataset script at {relative_to_absolute â”‚
[rank0]: â”‚   1733 â”‚   â”‚   )                                                             â”‚
[rank0]: â”‚   1734 â”‚   else:                                                             â”‚
[rank0]: â”‚ â± 1735 â”‚   â”‚   raise FileNotFoundError(f"Couldn't find any data file at {rel â”‚
[rank0]: â”‚   1736                                                                       â”‚
[rank0]: â”‚   1737                                                                       â”‚
[rank0]: â”‚   1738 def load_dataset_builder(                                             â”‚
[rank0]: â”‚                                                                              â”‚
[rank0]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank0]: â”‚ â”‚      _require_custom_configs = False                                     â”‚ â”‚
[rank0]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank0]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                combined_path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚              download_config = DownloadConfig(                           â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   cache_dir=None,                       â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   force_download=False,                 â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   resume_download=False,                â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   local_files_only=False,               â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   proxies=None,                         â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   user_agent=None,                      â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   extract_compressed_file=True,         â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   force_extract=True,                   â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   delete_extracted=False,               â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   extract_on_the_fly=False,             â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   use_etag=True,                        â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   num_proc=None,                        â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   max_retries=1,                        â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   token=None,                           â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   storage_options={},                   â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   download_desc=None,                   â”‚ â”‚
[rank0]: â”‚ â”‚                                â”‚   disable_tqdm=False                    â”‚ â”‚
[rank0]: â”‚ â”‚                                )                                         â”‚ â”‚
[rank0]: â”‚ â”‚              download_kwargs = {}                                        â”‚ â”‚
[rank0]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank0]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank0]: â”‚ â”‚         dynamic_modules_path = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚                     filename = 'syntheticDocQA_energy_test.py'           â”‚ â”‚
[rank0]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank0]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank0]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank0]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank0]: â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank0]: FileNotFoundError: Couldn't find any data file at 
[rank0]: /ivi/ilps/personal/jqiao/colpali/data_dir/syntheticDocQA_energy_test.
[rank2]: â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank2]: â”‚ /ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py:26 in main   â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   23 â”‚   â”‚   app.save(config_file=config_file)                               â”‚
[rank2]: â”‚   24 â”‚   if config.run_eval:                                                 â”‚
[rank2]: â”‚   25 â”‚   â”‚   print("Running evaluation")                                     â”‚
[rank2]: â”‚ â± 26 â”‚   â”‚   app.eval()                                                      â”‚
[rank2]: â”‚   27 â”‚   print("Done!")                                                      â”‚
[rank2]: â”‚   28                                                                         â”‚
[rank2]: â”‚   29                                                                         â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚         app = <colpali_engine.trainer.colmodel_training.ColModelTraining â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894a80760>                                  â”‚ â”‚
[rank2]: â”‚ â”‚      config = ColModelTrainingConfig(                                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   model=PeftModelForFeatureExtraction(                   â”‚ â”‚
[rank2]: â”‚ â”‚                 (base_model): LoraModel(                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   (model): ColQwen2(                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     (visual): Qwen2VisionTransformerPretrainedModel(     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (patch_embed): PatchEmbed(                         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), â”‚ â”‚
[rank2]: â”‚ â”‚               stride=(2, 14, 14), bias=False)                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (rotary_pos_emb): VisionRotaryEmbedding()          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (blocks): ModuleList(                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (0-31): 32 x Qwen2VLVisionBlock(                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm1): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank2]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm2): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank2]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (attn): VisionFlashAttention2(                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (qkv): Linear(in_features=1280,              â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=3840, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (proj): Linear(in_features=1280,             â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): VisionMlp(                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc1): Linear(in_features=1280,              â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (act): QuickGELUActivation()                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc2): Linear(in_features=5120,              â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (merger): PatchMerger(                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (ln_q): LayerNorm((1280,), eps=1e-06,            â”‚ â”‚
[rank2]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (mlp): Sequential(                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (0): Linear(in_features=5120,                  â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (1): GELU(approximate='none')                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (2): Linear(in_features=5120,                  â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     (model): Qwen2VLModel(                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (embed_tokens): Embedding(151936, 1536)            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (layers): ModuleList(                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (0-27): 28 x Qwen2VLDecoderLayer(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (self_attn): Qwen2VLFlashAttention2(           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (q_proj): lora.Linear(                       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (k_proj): lora.Linear(                       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (v_proj): lora.Linear(                       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (o_proj): lora.Linear(                       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (rotary_emb): Qwen2VLRotaryEmbedding()       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): Qwen2MLP(                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (gate_proj): lora.Linear(                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (up_proj): lora.Linear(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (down_proj): lora.Linear(                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=8960,     â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=8960,      â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚     (act_fn): SiLU()                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (input_layernorm): Qwen2RMSNorm((1536,),       â”‚ â”‚
[rank2]: â”‚ â”‚               eps=1e-06)                                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   â”‚   (post_attention_layernorm):                    â”‚ â”‚
[rank2]: â”‚ â”‚               Qwen2RMSNorm((1536,), eps=1e-06)                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (norm): Qwen2RMSNorm((1536,), eps=1e-06)           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (rotary_emb): Qwen2VLRotaryEmbedding()             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     (lm_head): Linear(in_features=1536,                  â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=151936, bias=False)                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     (custom_text_proj): lora.Linear(                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (base_layer): Linear(in_features=1536,             â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=128, bias=True)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (lora_dropout): ModuleDict(                        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (default): Dropout(p=0.1, inplace=False)         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (lora_A): ModuleDict(                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=1536,              â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (lora_B): ModuleDict(                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=32,                â”‚ â”‚
[rank2]: â”‚ â”‚               out_features=128, bias=False)                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_A): ParameterDict()                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_B): ParameterDict()                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   )                                                      â”‚ â”‚
[rank2]: â”‚ â”‚                 )                                                        â”‚ â”‚
[rank2]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   tr_args=TrainingArguments(                             â”‚ â”‚
[rank2]: â”‚ â”‚               _n_gpu=1,                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               accelerator_config={'split_batches': False,                â”‚ â”‚
[rank2]: â”‚ â”‚               'dispatch_batches': None, 'even_batches': True,            â”‚ â”‚
[rank2]: â”‚ â”‚               'use_seedable_sampler': True, 'non_blocking': False,       â”‚ â”‚
[rank2]: â”‚ â”‚               'gradient_accumulation_kwargs': None,                      â”‚ â”‚
[rank2]: â”‚ â”‚               'use_configured_state': False},                            â”‚ â”‚
[rank2]: â”‚ â”‚               adafactor=False,                                           â”‚ â”‚
[rank2]: â”‚ â”‚               adam_beta1=0.9,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               adam_beta2=0.999,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               adam_epsilon=1e-08,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               auto_find_batch_size=False,                                â”‚ â”‚
[rank2]: â”‚ â”‚               average_tokens_across_devices=False,                       â”‚ â”‚
[rank2]: â”‚ â”‚               batch_eval_metrics=False,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               bf16=True,                                                 â”‚ â”‚
[rank2]: â”‚ â”‚               bf16_full_eval=False,                                      â”‚ â”‚
[rank2]: â”‚ â”‚               data_seed=None,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               dataloader_drop_last=False,                                â”‚ â”‚
[rank2]: â”‚ â”‚               dataloader_num_workers=8,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               dataloader_persistent_workers=False,                       â”‚ â”‚
[rank2]: â”‚ â”‚               dataloader_pin_memory=True,                                â”‚ â”‚
[rank2]: â”‚ â”‚               dataloader_prefetch_factor=None,                           â”‚ â”‚
[rank2]: â”‚ â”‚               ddp_backend=None,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               ddp_broadcast_buffers=None,                                â”‚ â”‚
[rank2]: â”‚ â”‚               ddp_bucket_cap_mb=None,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               ddp_find_unused_parameters=None,                           â”‚ â”‚
[rank2]: â”‚ â”‚               ddp_timeout=1800,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               debug=[],                                                  â”‚ â”‚
[rank2]: â”‚ â”‚               deepspeed=None,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               disable_tqdm=False,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               dispatch_batches=None,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               do_eval=True,                                              â”‚ â”‚
[rank2]: â”‚ â”‚               do_predict=False,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               do_train=False,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               eval_accumulation_steps=None,                              â”‚ â”‚
[rank2]: â”‚ â”‚               eval_delay=0,                                              â”‚ â”‚
[rank2]: â”‚ â”‚               eval_do_concat_batches=True,                               â”‚ â”‚
[rank2]: â”‚ â”‚               eval_on_start=False,                                       â”‚ â”‚
[rank2]: â”‚ â”‚               eval_steps=100,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               eval_strategy=steps,                                       â”‚ â”‚
[rank2]: â”‚ â”‚               eval_use_gather_object=False,                              â”‚ â”‚
[rank2]: â”‚ â”‚               evaluation_strategy=None,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               fp16=False,                                                â”‚ â”‚
[rank2]: â”‚ â”‚               fp16_backend=auto,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               fp16_full_eval=False,                                      â”‚ â”‚
[rank2]: â”‚ â”‚               fp16_opt_level=O1,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               fsdp=[],                                                   â”‚ â”‚
[rank2]: â”‚ â”‚               fsdp_config={'min_num_params': 0, 'xla': False,            â”‚ â”‚
[rank2]: â”‚ â”‚               'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},        â”‚ â”‚
[rank2]: â”‚ â”‚               fsdp_min_num_params=0,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               fsdp_transformer_layer_cls_to_wrap=None,                   â”‚ â”‚
[rank2]: â”‚ â”‚               full_determinism=False,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               gradient_accumulation_steps=1,                             â”‚ â”‚
[rank2]: â”‚ â”‚               gradient_checkpointing=True,                               â”‚ â”‚
[rank2]: â”‚ â”‚               gradient_checkpointing_kwargs={'use_reentrant': False},    â”‚ â”‚
[rank2]: â”‚ â”‚               greater_is_better=None,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               group_by_length=False,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               half_precision_backend=auto,                               â”‚ â”‚
[rank2]: â”‚ â”‚               hub_always_push=False,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               hub_model_id=None,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               hub_private_repo=False,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               hub_strategy=every_save,                                   â”‚ â”‚
[rank2]: â”‚ â”‚               hub_token=<HUB_TOKEN>,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               ignore_data_skip=False,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               include_for_metrics=[],                                    â”‚ â”‚
[rank2]: â”‚ â”‚               include_inputs_for_metrics=False,                          â”‚ â”‚
[rank2]: â”‚ â”‚               include_num_input_tokens_seen=False,                       â”‚ â”‚
[rank2]: â”‚ â”‚               include_tokens_per_second=False,                           â”‚ â”‚
[rank2]: â”‚ â”‚               jit_mode_eval=False,                                       â”‚ â”‚
[rank2]: â”‚ â”‚               label_names=None,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               label_smoothing_factor=0.0,                                â”‚ â”‚
[rank2]: â”‚ â”‚               learning_rate=0.0005,                                      â”‚ â”‚
[rank2]: â”‚ â”‚               length_column_name=length,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               load_best_model_at_end=False,                              â”‚ â”‚
[rank2]: â”‚ â”‚               local_rank=2,                                              â”‚ â”‚
[rank2]: â”‚ â”‚               log_level=passive,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               log_level_replica=warning,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               log_on_each_node=True,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               logging_dir=None,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               logging_first_step=False,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               logging_nan_inf_filter=True,                               â”‚ â”‚
[rank2]: â”‚ â”‚               logging_steps=10,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               logging_strategy=steps,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               lr_scheduler_kwargs={},                                    â”‚ â”‚
[rank2]: â”‚ â”‚               lr_scheduler_type=linear,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               max_grad_norm=1.0,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               max_steps=-1,                                              â”‚ â”‚
[rank2]: â”‚ â”‚               metric_for_best_model=None,                                â”‚ â”‚
[rank2]: â”‚ â”‚               mp_parameters=,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               neftune_noise_alpha=None,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               no_cuda=False,                                             â”‚ â”‚
[rank2]: â”‚ â”‚               num_train_epochs=5,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               optim=adamw_torch,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               optim_args=None,                                           â”‚ â”‚
[rank2]: â”‚ â”‚               optim_target_modules=None,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               output_dir=/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               overwrite_output_dir=True,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               past_index=-1,                                             â”‚ â”‚
[rank2]: â”‚ â”‚               per_device_eval_batch_size=8,                              â”‚ â”‚
[rank2]: â”‚ â”‚               per_device_train_batch_size=32,                            â”‚ â”‚
[rank2]: â”‚ â”‚               prediction_loss_only=False,                                â”‚ â”‚
[rank2]: â”‚ â”‚               push_to_hub=False,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               push_to_hub_model_id=None,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               push_to_hub_organization=None,                             â”‚ â”‚
[rank2]: â”‚ â”‚               push_to_hub_token=<PUSH_TO_HUB_TOKEN>,                     â”‚ â”‚
[rank2]: â”‚ â”‚               ray_scope=last,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               remove_unused_columns=False,                               â”‚ â”‚
[rank2]: â”‚ â”‚               report_to=['wandb'],                                       â”‚ â”‚
[rank2]: â”‚ â”‚               restore_callback_states_from_checkpoint=False,             â”‚ â”‚
[rank2]: â”‚ â”‚               resume_from_checkpoint=None,                               â”‚ â”‚
[rank2]: â”‚ â”‚               run_name=None,                                             â”‚ â”‚
[rank2]: â”‚ â”‚               save_on_each_node=False,                                   â”‚ â”‚
[rank2]: â”‚ â”‚               save_only_model=False,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               save_safetensors=True,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               save_steps=500,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               save_strategy=steps,                                       â”‚ â”‚
[rank2]: â”‚ â”‚               save_total_limit=1,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               seed=42,                                                   â”‚ â”‚
[rank2]: â”‚ â”‚               skip_memory_metrics=True,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               split_batches=None,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               tf32=None,                                                 â”‚ â”‚
[rank2]: â”‚ â”‚               torch_compile=False,                                       â”‚ â”‚
[rank2]: â”‚ â”‚               torch_compile_backend=None,                                â”‚ â”‚
[rank2]: â”‚ â”‚               torch_compile_mode=None,                                   â”‚ â”‚
[rank2]: â”‚ â”‚               torch_empty_cache_steps=None,                              â”‚ â”‚
[rank2]: â”‚ â”‚               torchdynamo=None,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               tpu_metrics_debug=False,                                   â”‚ â”‚
[rank2]: â”‚ â”‚               tpu_num_cores=None,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               use_cpu=False,                                             â”‚ â”‚
[rank2]: â”‚ â”‚               use_ipex=False,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               use_legacy_prediction_loop=False,                          â”‚ â”‚
[rank2]: â”‚ â”‚               use_liger_kernel=False,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               use_mps_device=False,                                      â”‚ â”‚
[rank2]: â”‚ â”‚               warmup_ratio=0.0,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               warmup_steps=100,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               weight_decay=0.0,                                          â”‚ â”‚
[rank2]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚                                                          â”‚ â”‚
[rank2]: â”‚ â”‚               output_dir='/ivi/ilps/personal/jqiao/colpali/scripts/confâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   max_length=256,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   run_eval=True,                                         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   run_train=True,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   peft_config=LoraConfig(                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   peft_type=<PeftType.LORA: 'LORA'>,                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   auto_mapping=None,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank2]: â”‚ â”‚               base_model_name_or_path='/ivi/ilps/personal/jqiao/models/â€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   revision=None,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   task_type='FEATURE_EXTRACTION',                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   inference_mode=False,                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   r=32,                                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank2]: â”‚ â”‚               target_modules='(.*(model).*(down_proj|gate_proj|up_proj|â€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   lora_alpha=32,                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   lora_dropout=0.1,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   fan_in_fan_out=False,                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   bias='none',                                       â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   use_rslora=False,                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   modules_to_save=None,                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   init_lora_weights='gaussian',                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   layers_to_transform=None,                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   layers_pattern=None,                               â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   rank_pattern={},                                   â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   alpha_pattern={},                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   megatron_config=None,                              â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   megatron_core='megatron.core',                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   loftq_config={},                                   â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   use_dora=False,                                    â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   layer_replication=None                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   ),                                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   add_suffix=False,                                      â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   processor=ColQwen2Processor:                           â”‚ â”‚
[rank2]: â”‚ â”‚               - image_processor: Qwen2VLImageProcessor {                 â”‚ â”‚
[rank2]: â”‚ â”‚                 "do_convert_rgb": true,                                  â”‚ â”‚
[rank2]: â”‚ â”‚                 "do_normalize": true,                                    â”‚ â”‚
[rank2]: â”‚ â”‚                 "do_rescale": true,                                      â”‚ â”‚
[rank2]: â”‚ â”‚                 "do_resize": true,                                       â”‚ â”‚
[rank2]: â”‚ â”‚                 "image_mean": [                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.48145466,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.4578275,                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.40821073                                             â”‚ â”‚
[rank2]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank2]: â”‚ â”‚                 "image_processor_type": "Qwen2VLImageProcessor",         â”‚ â”‚
[rank2]: â”‚ â”‚                 "image_std": [                                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.26862954,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.26130258,                                            â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   0.27577711                                             â”‚ â”‚
[rank2]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank2]: â”‚ â”‚                 "max_pixels": 12845056,                                  â”‚ â”‚
[rank2]: â”‚ â”‚                 "merge_size": 2,                                         â”‚ â”‚
[rank2]: â”‚ â”‚                 "min_pixels": 3136,                                      â”‚ â”‚
[rank2]: â”‚ â”‚                 "patch_size": 14,                                        â”‚ â”‚
[rank2]: â”‚ â”‚                 "processor_class": "ColQwen2Processor",                  â”‚ â”‚
[rank2]: â”‚ â”‚                 "resample": 3,                                           â”‚ â”‚
[rank2]: â”‚ â”‚                 "rescale_factor": 0.00392156862745098,                   â”‚ â”‚
[rank2]: â”‚ â”‚                 "size": {                                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   "max_pixels": 12845056,                                â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   "min_pixels": 3136                                     â”‚ â”‚
[rank2]: â”‚ â”‚                 },                                                       â”‚ â”‚
[rank2]: â”‚ â”‚                 "temporal_patch_size": 2                                 â”‚ â”‚
[rank2]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank2]: â”‚ â”‚                                                                          â”‚ â”‚
[rank2]: â”‚ â”‚               - tokenizer:                                               â”‚ â”‚
[rank2]: â”‚ â”‚               Qwen2TokenizerFast(name_or_path='/ivi/ilps/personal/jqiaoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               vocab_size=151643, model_max_length=32768, is_fast=True,   â”‚ â”‚
[rank2]: â”‚ â”‚               padding_side='left', truncation_side='right',              â”‚ â”‚
[rank2]: â”‚ â”‚               special_tokens={'eos_token': '<|im_end|>', 'pad_token':    â”‚ â”‚
[rank2]: â”‚ â”‚               '<|endoftext|>', 'additional_special_tokens':              â”‚ â”‚
[rank2]: â”‚ â”‚               ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>',     â”‚ â”‚
[rank2]: â”‚ â”‚               '<|object_ref_end|>', '<|box_start|>', '<|box_end|>',      â”‚ â”‚
[rank2]: â”‚ â”‚               '<|quad_start|>', '<|quad_end|>', '<|vision_start|>',      â”‚ â”‚
[rank2]: â”‚ â”‚               '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>',       â”‚ â”‚
[rank2]: â”‚ â”‚               '<|video_pad|>']}, clean_up_tokenization_spaces=False),    â”‚ â”‚
[rank2]: â”‚ â”‚               added_tokens_decoder={                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151643: AddedToken("<|endoftext|>", rstrip=False,  â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151644: AddedToken("<|im_start|>", rstrip=False,   â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151645: AddedToken("<|im_end|>", rstrip=False,     â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151646: AddedToken("<|object_ref_start|>",         â”‚ â”‚
[rank2]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank2]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151647: AddedToken("<|object_ref_end|>",           â”‚ â”‚
[rank2]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank2]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151648: AddedToken("<|box_start|>", rstrip=False,  â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151649: AddedToken("<|box_end|>", rstrip=False,    â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151650: AddedToken("<|quad_start|>", rstrip=False, â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151651: AddedToken("<|quad_end|>", rstrip=False,   â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151652: AddedToken("<|vision_start|>",             â”‚ â”‚
[rank2]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank2]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151653: AddedToken("<|vision_end|>", rstrip=False, â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151654: AddedToken("<|vision_pad|>", rstrip=False, â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151655: AddedToken("<|image_pad|>", rstrip=False,  â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   151656: AddedToken("<|video_pad|>", rstrip=False,  â”‚ â”‚
[rank2]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank2]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank2]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank2]: â”‚ â”‚                                                                          â”‚ â”‚
[rank2]: â”‚ â”‚               {                                                          â”‚ â”‚
[rank2]: â”‚ â”‚                 "processor_class": "ColQwen2Processor"                   â”‚ â”‚
[rank2]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank2]: â”‚ â”‚               ,                                                          â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   tokenizer=None,                                        â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   loss_func=ColbertPairwiseCELoss(                       â”‚ â”‚
[rank2]: â”‚ â”‚                 (ce_loss): CrossEntropyLoss()                            â”‚ â”‚
[rank2]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   dataset_loading_func=<function load_train_set at       â”‚ â”‚
[rank2]: â”‚ â”‚               0x7f3894b5feb0>,                                           â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   eval_dataset_loader={                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_energy':                           â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fc10>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_healthcare_industry':              â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3f820>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_artificial_intelligence_test':     â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3f700>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_government_reports':               â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fb20>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'infovqa_subsampled':                              â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fa90>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'docvqa_subsampled':                               â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fc70>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'arxivqa_subsampled':                              â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fcd0>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'tabfquad_subsampled':                             â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fd30>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'tatdqa':                                          â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fd90>,                                 â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   â”‚   'shift_project':                                   â”‚ â”‚
[rank2]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚               object at 0x7f3894b3fdf0>                                  â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   },                                                     â”‚ â”‚
[rank2]: â”‚ â”‚               â”‚   pretrained_peft_model_name_or_path=None                â”‚ â”‚
[rank2]: â”‚ â”‚               )                                                          â”‚ â”‚
[rank2]: â”‚ â”‚ config_file = PosixPath('/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank2]: â”‚ /colpali_engine/trainer/colmodel_training.py:224 in eval                     â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   221 â”‚   â”‚   if self.config.eval_dataset_loader is not None:                â”‚
[rank2]: â”‚   222 â”‚   â”‚   â”‚   for test_name, test_dataset_loading_func in self.config.ev â”‚
[rank2]: â”‚   223 â”‚   â”‚   â”‚   â”‚   print(f"Evaluating {test_name}")                       â”‚
[rank2]: â”‚ â± 224 â”‚   â”‚   â”‚   â”‚   test_ds = test_dataset_loading_func()                  â”‚
[rank2]: â”‚   225 â”‚   â”‚   â”‚   â”‚   metrics = self.eval_dataset(test_ds)                   â”‚
[rank2]: â”‚   226 â”‚   â”‚   â”‚   â”‚   all_metrics[test_name] = metrics                       â”‚
[rank2]: â”‚   227 â”‚   â”‚   â”‚   â”‚   print(f"Metrics for {test_name}: {metrics}")           â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚               all_metrics = {                                            â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'validation_set': {                      â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_1': 0.842,                  â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_3': 0.88859,                â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_5': 0.89281,                â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_10': 0.89876,               â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_20': 0.90318,               â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_50': 0.90714,               â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_100': 0.90876,              â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'map_at_1': 0.842,                   â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'map_at_3': 0.87767,                 â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   'map_at_5': 0.88007,                 â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   â”‚   ... +46                              â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   }                                        â”‚ â”‚
[rank2]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank2]: â”‚ â”‚                   metrics = {                                            â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_1': 0.842,                      â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_3': 0.88859,                    â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_5': 0.89281,                    â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_10': 0.89876,                   â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_20': 0.90318,                   â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_50': 0.90714,                   â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'ndcg_at_100': 0.90876,                  â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'map_at_1': 0.842,                       â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'map_at_3': 0.87767,                     â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   'map_at_5': 0.88007,                     â”‚ â”‚
[rank2]: â”‚ â”‚                             â”‚   ... +46                                  â”‚ â”‚
[rank2]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank2]: â”‚ â”‚                      self = <colpali_engine.trainer.colmodel_training.Câ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚                             object at 0x7f3894a80760>                    â”‚ â”‚
[rank2]: â”‚ â”‚ test_dataset_loading_func = <colpali_engine.utils.dataset_transformatioâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚                             object at 0x7f3894b3fc10>                    â”‚ â”‚
[rank2]: â”‚ â”‚                 test_name = 'syntheticDocQA_energy'                      â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank2]: â”‚ /colpali_engine/utils/dataset_transformation.py:184 in __call__              â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   181 â”‚   â”‚   self.dataset_path = dataset_path                               â”‚
[rank2]: â”‚   182 â”‚                                                                      â”‚
[rank2]: â”‚   183 â”‚   def __call__(self, *args, **kwargs):                               â”‚
[rank2]: â”‚ â± 184 â”‚   â”‚   dataset = load_dataset(self.dataset_path, split="test")        â”‚
[rank2]: â”‚   185 â”‚   â”‚   return dataset                                                 â”‚
[rank2]: â”‚   186                                                                        â”‚
[rank2]: â”‚   187                                                                        â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚   args = ()                                                              â”‚ â”‚
[rank2]: â”‚ â”‚ kwargs = {}                                                              â”‚ â”‚
[rank2]: â”‚ â”‚   self = <colpali_engine.utils.dataset_transformation.TestSetFactory     â”‚ â”‚
[rank2]: â”‚ â”‚          object at 0x7f3894b3fc10>                                       â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank2]: â”‚ /datasets/load.py:2132 in load_dataset                                       â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   2129 â”‚   )                                                                 â”‚
[rank2]: â”‚   2130 â”‚                                                                     â”‚
[rank2]: â”‚   2131 â”‚   # Create a dataset builder                                        â”‚
[rank2]: â”‚ â± 2132 â”‚   builder_instance = load_dataset_builder(                          â”‚
[rank2]: â”‚   2133 â”‚   â”‚   path=path,                                                    â”‚
[rank2]: â”‚   2134 â”‚   â”‚   name=name,                                                    â”‚
[rank2]: â”‚   2135 â”‚   â”‚   data_dir=data_dir,                                            â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚         cache_dir = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚     config_kwargs = {}                                                   â”‚ â”‚
[rank2]: â”‚ â”‚          data_dir = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚        data_files = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚   download_config = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚     download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:               â”‚ â”‚
[rank2]: â”‚ â”‚                     'reuse_dataset_if_exists'>                           â”‚ â”‚
[rank2]: â”‚ â”‚          features = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚    keep_in_memory = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚              name = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚          num_proc = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚              path = '/ivi/ilps/personal/jqiao/colpali/scripts/configs/qâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚          revision = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚        save_infos = False                                                â”‚ â”‚
[rank2]: â”‚ â”‚             split = 'test'                                               â”‚ â”‚
[rank2]: â”‚ â”‚   storage_options = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚         streaming = False                                                â”‚ â”‚
[rank2]: â”‚ â”‚             token = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚ trust_remote_code = None                                                 â”‚ â”‚
[rank2]: â”‚ â”‚ verification_mode = <VerificationMode.BASIC_CHECKS: 'basic_checks'>      â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank2]: â”‚ /datasets/load.py:1853 in load_dataset_builder                               â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   1850 â”‚   if storage_options is not None:                                   â”‚
[rank2]: â”‚   1851 â”‚   â”‚   download_config = download_config.copy() if download_config e â”‚
[rank2]: â”‚   1852 â”‚   â”‚   download_config.storage_options.update(storage_options)       â”‚
[rank2]: â”‚ â± 1853 â”‚   dataset_module = dataset_module_factory(                          â”‚
[rank2]: â”‚   1854 â”‚   â”‚   path,                                                         â”‚
[rank2]: â”‚   1855 â”‚   â”‚   revision=revision,                                            â”‚
[rank2]: â”‚   1856 â”‚   â”‚   download_config=download_config,                              â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank2]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                config_kwargs = {}                                        â”‚ â”‚
[rank2]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚              download_config = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank2]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank2]: â”‚ â”‚                     features = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                         name = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚              storage_options = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                        token = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank2]: â”‚ /datasets/load.py:1735 in dataset_module_factory                             â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚   1732 â”‚   â”‚   â”‚   f"Couldn't find a dataset script at {relative_to_absolute â”‚
[rank2]: â”‚   1733 â”‚   â”‚   )                                                             â”‚
[rank2]: â”‚   1734 â”‚   else:                                                             â”‚
[rank2]: â”‚ â± 1735 â”‚   â”‚   raise FileNotFoundError(f"Couldn't find any data file at {rel â”‚
[rank2]: â”‚   1736                                                                       â”‚
[rank2]: â”‚   1737                                                                       â”‚
[rank2]: â”‚   1738 def load_dataset_builder(                                             â”‚
[rank2]: â”‚                                                                              â”‚
[rank2]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank2]: â”‚ â”‚      _require_custom_configs = False                                     â”‚ â”‚
[rank2]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank2]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                combined_path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚              download_config = DownloadConfig(                           â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   cache_dir=None,                       â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   force_download=False,                 â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   resume_download=False,                â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   local_files_only=False,               â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   proxies=None,                         â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   user_agent=None,                      â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   extract_compressed_file=True,         â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   force_extract=True,                   â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   delete_extracted=False,               â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   extract_on_the_fly=False,             â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   use_etag=True,                        â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   num_proc=None,                        â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   max_retries=1,                        â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   token=None,                           â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   storage_options={},                   â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   download_desc=None,                   â”‚ â”‚
[rank2]: â”‚ â”‚                                â”‚   disable_tqdm=False                    â”‚ â”‚
[rank2]: â”‚ â”‚                                )                                         â”‚ â”‚
[rank2]: â”‚ â”‚              download_kwargs = {}                                        â”‚ â”‚
[rank2]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank2]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank2]: â”‚ â”‚         dynamic_modules_path = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚                     filename = 'syntheticDocQA_energy_test.py'           â”‚ â”‚
[rank2]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank2]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank2]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank2]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank2]: â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank2]: FileNotFoundError: Couldn't find any data file at 
[rank2]: /ivi/ilps/personal/jqiao/colpali/data_dir/syntheticDocQA_energy_test.
[rank1]: â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank1]: â”‚ /ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py:26 in main   â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   23 â”‚   â”‚   app.save(config_file=config_file)                               â”‚
[rank1]: â”‚   24 â”‚   if config.run_eval:                                                 â”‚
[rank1]: â”‚   25 â”‚   â”‚   print("Running evaluation")                                     â”‚
[rank1]: â”‚ â± 26 â”‚   â”‚   app.eval()                                                      â”‚
[rank1]: â”‚   27 â”‚   print("Done!")                                                      â”‚
[rank1]: â”‚   28                                                                         â”‚
[rank1]: â”‚   29                                                                         â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚         app = <colpali_engine.trainer.colmodel_training.ColModelTraining â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c2fb2e0>                                  â”‚ â”‚
[rank1]: â”‚ â”‚      config = ColModelTrainingConfig(                                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   model=PeftModelForFeatureExtraction(                   â”‚ â”‚
[rank1]: â”‚ â”‚                 (base_model): LoraModel(                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   (model): ColQwen2(                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     (visual): Qwen2VisionTransformerPretrainedModel(     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (patch_embed): PatchEmbed(                         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), â”‚ â”‚
[rank1]: â”‚ â”‚               stride=(2, 14, 14), bias=False)                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (rotary_pos_emb): VisionRotaryEmbedding()          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (blocks): ModuleList(                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (0-31): 32 x Qwen2VLVisionBlock(                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm1): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank1]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (norm2): LayerNorm((1280,), eps=1e-06,         â”‚ â”‚
[rank1]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (attn): VisionFlashAttention2(                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (qkv): Linear(in_features=1280,              â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=3840, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (proj): Linear(in_features=1280,             â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): VisionMlp(                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc1): Linear(in_features=1280,              â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (act): QuickGELUActivation()                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (fc2): Linear(in_features=5120,              â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1280, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (merger): PatchMerger(                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (ln_q): LayerNorm((1280,), eps=1e-06,            â”‚ â”‚
[rank1]: â”‚ â”‚               elementwise_affine=True)                                   â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (mlp): Sequential(                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (0): Linear(in_features=5120,                  â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=5120, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (1): GELU(approximate='none')                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (2): Linear(in_features=5120,                  â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     (model): Qwen2VLModel(                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (embed_tokens): Embedding(151936, 1536)            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (layers): ModuleList(                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (0-27): 28 x Qwen2VLDecoderLayer(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (self_attn): Qwen2VLFlashAttention2(           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (q_proj): lora.Linear(                       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=True)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (k_proj): lora.Linear(                       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (v_proj): lora.Linear(                       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=256, bias=True)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=256, bias=False)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (o_proj): lora.Linear(                       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (rotary_emb): Qwen2VLRotaryEmbedding()       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (mlp): Qwen2MLP(                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (gate_proj): lora.Linear(                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (up_proj): lora.Linear(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=1536,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=1536,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=8960, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (down_proj): lora.Linear(                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (base_layer): Linear(in_features=8960,     â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_dropout): ModuleDict(                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Dropout(p=0.1, inplace=False) â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_A): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=8960,      â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_B): ModuleDict(                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚     (default): Linear(in_features=32,        â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=1536, bias=False)                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   )                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_A): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   â”‚   (lora_embedding_B): ParameterDict()        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     )                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚     (act_fn): SiLU()                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   )                                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (input_layernorm): Qwen2RMSNorm((1536,),       â”‚ â”‚
[rank1]: â”‚ â”‚               eps=1e-06)                                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   â”‚   (post_attention_layernorm):                    â”‚ â”‚
[rank1]: â”‚ â”‚               Qwen2RMSNorm((1536,), eps=1e-06)                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     )                                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (norm): Qwen2RMSNorm((1536,), eps=1e-06)           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (rotary_emb): Qwen2VLRotaryEmbedding()             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     (lm_head): Linear(in_features=1536,                  â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=151936, bias=False)                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     (custom_text_proj): lora.Linear(                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (base_layer): Linear(in_features=1536,             â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=128, bias=True)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (lora_dropout): ModuleDict(                        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (default): Dropout(p=0.1, inplace=False)         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (lora_A): ModuleDict(                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=1536,              â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=32, bias=False)                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (lora_B): ModuleDict(                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚     (default): Linear(in_features=32,                â”‚ â”‚
[rank1]: â”‚ â”‚               out_features=128, bias=False)                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   )                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_A): ParameterDict()                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   (lora_embedding_B): ParameterDict()                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚     )                                                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   )                                                      â”‚ â”‚
[rank1]: â”‚ â”‚                 )                                                        â”‚ â”‚
[rank1]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   tr_args=TrainingArguments(                             â”‚ â”‚
[rank1]: â”‚ â”‚               _n_gpu=1,                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               accelerator_config={'split_batches': False,                â”‚ â”‚
[rank1]: â”‚ â”‚               'dispatch_batches': None, 'even_batches': True,            â”‚ â”‚
[rank1]: â”‚ â”‚               'use_seedable_sampler': True, 'non_blocking': False,       â”‚ â”‚
[rank1]: â”‚ â”‚               'gradient_accumulation_kwargs': None,                      â”‚ â”‚
[rank1]: â”‚ â”‚               'use_configured_state': False},                            â”‚ â”‚
[rank1]: â”‚ â”‚               adafactor=False,                                           â”‚ â”‚
[rank1]: â”‚ â”‚               adam_beta1=0.9,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               adam_beta2=0.999,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               adam_epsilon=1e-08,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               auto_find_batch_size=False,                                â”‚ â”‚
[rank1]: â”‚ â”‚               average_tokens_across_devices=False,                       â”‚ â”‚
[rank1]: â”‚ â”‚               batch_eval_metrics=False,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               bf16=True,                                                 â”‚ â”‚
[rank1]: â”‚ â”‚               bf16_full_eval=False,                                      â”‚ â”‚
[rank1]: â”‚ â”‚               data_seed=None,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               dataloader_drop_last=False,                                â”‚ â”‚
[rank1]: â”‚ â”‚               dataloader_num_workers=8,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               dataloader_persistent_workers=False,                       â”‚ â”‚
[rank1]: â”‚ â”‚               dataloader_pin_memory=True,                                â”‚ â”‚
[rank1]: â”‚ â”‚               dataloader_prefetch_factor=None,                           â”‚ â”‚
[rank1]: â”‚ â”‚               ddp_backend=None,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               ddp_broadcast_buffers=None,                                â”‚ â”‚
[rank1]: â”‚ â”‚               ddp_bucket_cap_mb=None,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               ddp_find_unused_parameters=None,                           â”‚ â”‚
[rank1]: â”‚ â”‚               ddp_timeout=1800,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               debug=[],                                                  â”‚ â”‚
[rank1]: â”‚ â”‚               deepspeed=None,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               disable_tqdm=False,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               dispatch_batches=None,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               do_eval=True,                                              â”‚ â”‚
[rank1]: â”‚ â”‚               do_predict=False,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               do_train=False,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               eval_accumulation_steps=None,                              â”‚ â”‚
[rank1]: â”‚ â”‚               eval_delay=0,                                              â”‚ â”‚
[rank1]: â”‚ â”‚               eval_do_concat_batches=True,                               â”‚ â”‚
[rank1]: â”‚ â”‚               eval_on_start=False,                                       â”‚ â”‚
[rank1]: â”‚ â”‚               eval_steps=100,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               eval_strategy=steps,                                       â”‚ â”‚
[rank1]: â”‚ â”‚               eval_use_gather_object=False,                              â”‚ â”‚
[rank1]: â”‚ â”‚               evaluation_strategy=None,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               fp16=False,                                                â”‚ â”‚
[rank1]: â”‚ â”‚               fp16_backend=auto,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               fp16_full_eval=False,                                      â”‚ â”‚
[rank1]: â”‚ â”‚               fp16_opt_level=O1,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               fsdp=[],                                                   â”‚ â”‚
[rank1]: â”‚ â”‚               fsdp_config={'min_num_params': 0, 'xla': False,            â”‚ â”‚
[rank1]: â”‚ â”‚               'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},        â”‚ â”‚
[rank1]: â”‚ â”‚               fsdp_min_num_params=0,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               fsdp_transformer_layer_cls_to_wrap=None,                   â”‚ â”‚
[rank1]: â”‚ â”‚               full_determinism=False,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               gradient_accumulation_steps=1,                             â”‚ â”‚
[rank1]: â”‚ â”‚               gradient_checkpointing=True,                               â”‚ â”‚
[rank1]: â”‚ â”‚               gradient_checkpointing_kwargs={'use_reentrant': False},    â”‚ â”‚
[rank1]: â”‚ â”‚               greater_is_better=None,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               group_by_length=False,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               half_precision_backend=auto,                               â”‚ â”‚
[rank1]: â”‚ â”‚               hub_always_push=False,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               hub_model_id=None,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               hub_private_repo=False,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               hub_strategy=every_save,                                   â”‚ â”‚
[rank1]: â”‚ â”‚               hub_token=<HUB_TOKEN>,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               ignore_data_skip=False,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               include_for_metrics=[],                                    â”‚ â”‚
[rank1]: â”‚ â”‚               include_inputs_for_metrics=False,                          â”‚ â”‚
[rank1]: â”‚ â”‚               include_num_input_tokens_seen=False,                       â”‚ â”‚
[rank1]: â”‚ â”‚               include_tokens_per_second=False,                           â”‚ â”‚
[rank1]: â”‚ â”‚               jit_mode_eval=False,                                       â”‚ â”‚
[rank1]: â”‚ â”‚               label_names=None,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               label_smoothing_factor=0.0,                                â”‚ â”‚
[rank1]: â”‚ â”‚               learning_rate=0.0005,                                      â”‚ â”‚
[rank1]: â”‚ â”‚               length_column_name=length,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               load_best_model_at_end=False,                              â”‚ â”‚
[rank1]: â”‚ â”‚               local_rank=1,                                              â”‚ â”‚
[rank1]: â”‚ â”‚               log_level=passive,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               log_level_replica=warning,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               log_on_each_node=True,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               logging_dir=None,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               logging_first_step=False,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               logging_nan_inf_filter=True,                               â”‚ â”‚
[rank1]: â”‚ â”‚               logging_steps=10,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               logging_strategy=steps,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               lr_scheduler_kwargs={},                                    â”‚ â”‚
[rank1]: â”‚ â”‚               lr_scheduler_type=linear,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               max_grad_norm=1.0,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               max_steps=-1,                                              â”‚ â”‚
[rank1]: â”‚ â”‚               metric_for_best_model=None,                                â”‚ â”‚
[rank1]: â”‚ â”‚               mp_parameters=,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               neftune_noise_alpha=None,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               no_cuda=False,                                             â”‚ â”‚
[rank1]: â”‚ â”‚               num_train_epochs=5,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               optim=adamw_torch,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               optim_args=None,                                           â”‚ â”‚
[rank1]: â”‚ â”‚               optim_target_modules=None,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               output_dir=/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               overwrite_output_dir=True,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               past_index=-1,                                             â”‚ â”‚
[rank1]: â”‚ â”‚               per_device_eval_batch_size=8,                              â”‚ â”‚
[rank1]: â”‚ â”‚               per_device_train_batch_size=32,                            â”‚ â”‚
[rank1]: â”‚ â”‚               prediction_loss_only=False,                                â”‚ â”‚
[rank1]: â”‚ â”‚               push_to_hub=False,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               push_to_hub_model_id=None,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               push_to_hub_organization=None,                             â”‚ â”‚
[rank1]: â”‚ â”‚               push_to_hub_token=<PUSH_TO_HUB_TOKEN>,                     â”‚ â”‚
[rank1]: â”‚ â”‚               ray_scope=last,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               remove_unused_columns=False,                               â”‚ â”‚
[rank1]: â”‚ â”‚               report_to=['wandb'],                                       â”‚ â”‚
[rank1]: â”‚ â”‚               restore_callback_states_from_checkpoint=False,             â”‚ â”‚
[rank1]: â”‚ â”‚               resume_from_checkpoint=None,                               â”‚ â”‚
[rank1]: â”‚ â”‚               run_name=None,                                             â”‚ â”‚
[rank1]: â”‚ â”‚               save_on_each_node=False,                                   â”‚ â”‚
[rank1]: â”‚ â”‚               save_only_model=False,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               save_safetensors=True,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               save_steps=500,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               save_strategy=steps,                                       â”‚ â”‚
[rank1]: â”‚ â”‚               save_total_limit=1,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               seed=42,                                                   â”‚ â”‚
[rank1]: â”‚ â”‚               skip_memory_metrics=True,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               split_batches=None,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               tf32=None,                                                 â”‚ â”‚
[rank1]: â”‚ â”‚               torch_compile=False,                                       â”‚ â”‚
[rank1]: â”‚ â”‚               torch_compile_backend=None,                                â”‚ â”‚
[rank1]: â”‚ â”‚               torch_compile_mode=None,                                   â”‚ â”‚
[rank1]: â”‚ â”‚               torch_empty_cache_steps=None,                              â”‚ â”‚
[rank1]: â”‚ â”‚               torchdynamo=None,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               tpu_metrics_debug=False,                                   â”‚ â”‚
[rank1]: â”‚ â”‚               tpu_num_cores=None,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               use_cpu=False,                                             â”‚ â”‚
[rank1]: â”‚ â”‚               use_ipex=False,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               use_legacy_prediction_loop=False,                          â”‚ â”‚
[rank1]: â”‚ â”‚               use_liger_kernel=False,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               use_mps_device=False,                                      â”‚ â”‚
[rank1]: â”‚ â”‚               warmup_ratio=0.0,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               warmup_steps=100,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               weight_decay=0.0,                                          â”‚ â”‚
[rank1]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚                                                          â”‚ â”‚
[rank1]: â”‚ â”‚               output_dir='/ivi/ilps/personal/jqiao/colpali/scripts/confâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   max_length=256,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   run_eval=True,                                         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   run_train=True,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   peft_config=LoraConfig(                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   peft_type=<PeftType.LORA: 'LORA'>,                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   auto_mapping=None,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank1]: â”‚ â”‚               base_model_name_or_path='/ivi/ilps/personal/jqiao/models/â€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   revision=None,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   task_type='FEATURE_EXTRACTION',                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   inference_mode=False,                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   r=32,                                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚                                                      â”‚ â”‚
[rank1]: â”‚ â”‚               target_modules='(.*(model).*(down_proj|gate_proj|up_proj|â€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   lora_alpha=32,                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   lora_dropout=0.1,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   fan_in_fan_out=False,                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   bias='none',                                       â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   use_rslora=False,                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   modules_to_save=None,                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   init_lora_weights='gaussian',                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   layers_to_transform=None,                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   layers_pattern=None,                               â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   rank_pattern={},                                   â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   alpha_pattern={},                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   megatron_config=None,                              â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   megatron_core='megatron.core',                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   loftq_config={},                                   â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   use_dora=False,                                    â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   layer_replication=None                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   ),                                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   add_suffix=False,                                      â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   processor=ColQwen2Processor:                           â”‚ â”‚
[rank1]: â”‚ â”‚               - image_processor: Qwen2VLImageProcessor {                 â”‚ â”‚
[rank1]: â”‚ â”‚                 "do_convert_rgb": true,                                  â”‚ â”‚
[rank1]: â”‚ â”‚                 "do_normalize": true,                                    â”‚ â”‚
[rank1]: â”‚ â”‚                 "do_rescale": true,                                      â”‚ â”‚
[rank1]: â”‚ â”‚                 "do_resize": true,                                       â”‚ â”‚
[rank1]: â”‚ â”‚                 "image_mean": [                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.48145466,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.4578275,                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.40821073                                             â”‚ â”‚
[rank1]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank1]: â”‚ â”‚                 "image_processor_type": "Qwen2VLImageProcessor",         â”‚ â”‚
[rank1]: â”‚ â”‚                 "image_std": [                                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.26862954,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.26130258,                                            â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   0.27577711                                             â”‚ â”‚
[rank1]: â”‚ â”‚                 ],                                                       â”‚ â”‚
[rank1]: â”‚ â”‚                 "max_pixels": 12845056,                                  â”‚ â”‚
[rank1]: â”‚ â”‚                 "merge_size": 2,                                         â”‚ â”‚
[rank1]: â”‚ â”‚                 "min_pixels": 3136,                                      â”‚ â”‚
[rank1]: â”‚ â”‚                 "patch_size": 14,                                        â”‚ â”‚
[rank1]: â”‚ â”‚                 "processor_class": "ColQwen2Processor",                  â”‚ â”‚
[rank1]: â”‚ â”‚                 "resample": 3,                                           â”‚ â”‚
[rank1]: â”‚ â”‚                 "rescale_factor": 0.00392156862745098,                   â”‚ â”‚
[rank1]: â”‚ â”‚                 "size": {                                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   "max_pixels": 12845056,                                â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   "min_pixels": 3136                                     â”‚ â”‚
[rank1]: â”‚ â”‚                 },                                                       â”‚ â”‚
[rank1]: â”‚ â”‚                 "temporal_patch_size": 2                                 â”‚ â”‚
[rank1]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank1]: â”‚ â”‚                                                                          â”‚ â”‚
[rank1]: â”‚ â”‚               - tokenizer:                                               â”‚ â”‚
[rank1]: â”‚ â”‚               Qwen2TokenizerFast(name_or_path='/ivi/ilps/personal/jqiaoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               vocab_size=151643, model_max_length=32768, is_fast=True,   â”‚ â”‚
[rank1]: â”‚ â”‚               padding_side='left', truncation_side='right',              â”‚ â”‚
[rank1]: â”‚ â”‚               special_tokens={'eos_token': '<|im_end|>', 'pad_token':    â”‚ â”‚
[rank1]: â”‚ â”‚               '<|endoftext|>', 'additional_special_tokens':              â”‚ â”‚
[rank1]: â”‚ â”‚               ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>',     â”‚ â”‚
[rank1]: â”‚ â”‚               '<|object_ref_end|>', '<|box_start|>', '<|box_end|>',      â”‚ â”‚
[rank1]: â”‚ â”‚               '<|quad_start|>', '<|quad_end|>', '<|vision_start|>',      â”‚ â”‚
[rank1]: â”‚ â”‚               '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>',       â”‚ â”‚
[rank1]: â”‚ â”‚               '<|video_pad|>']}, clean_up_tokenization_spaces=False),    â”‚ â”‚
[rank1]: â”‚ â”‚               added_tokens_decoder={                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151643: AddedToken("<|endoftext|>", rstrip=False,  â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151644: AddedToken("<|im_start|>", rstrip=False,   â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151645: AddedToken("<|im_end|>", rstrip=False,     â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151646: AddedToken("<|object_ref_start|>",         â”‚ â”‚
[rank1]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank1]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151647: AddedToken("<|object_ref_end|>",           â”‚ â”‚
[rank1]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank1]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151648: AddedToken("<|box_start|>", rstrip=False,  â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151649: AddedToken("<|box_end|>", rstrip=False,    â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151650: AddedToken("<|quad_start|>", rstrip=False, â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151651: AddedToken("<|quad_end|>", rstrip=False,   â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151652: AddedToken("<|vision_start|>",             â”‚ â”‚
[rank1]: â”‚ â”‚               rstrip=False, lstrip=False, single_word=False,             â”‚ â”‚
[rank1]: â”‚ â”‚               normalized=False, special=True),                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151653: AddedToken("<|vision_end|>", rstrip=False, â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151654: AddedToken("<|vision_pad|>", rstrip=False, â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151655: AddedToken("<|image_pad|>", rstrip=False,  â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   151656: AddedToken("<|video_pad|>", rstrip=False,  â”‚ â”‚
[rank1]: â”‚ â”‚               lstrip=False, single_word=False, normalized=False,         â”‚ â”‚
[rank1]: â”‚ â”‚               special=True),                                             â”‚ â”‚
[rank1]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank1]: â”‚ â”‚                                                                          â”‚ â”‚
[rank1]: â”‚ â”‚               {                                                          â”‚ â”‚
[rank1]: â”‚ â”‚                 "processor_class": "ColQwen2Processor"                   â”‚ â”‚
[rank1]: â”‚ â”‚               }                                                          â”‚ â”‚
[rank1]: â”‚ â”‚               ,                                                          â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   tokenizer=None,                                        â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   loss_func=ColbertPairwiseCELoss(                       â”‚ â”‚
[rank1]: â”‚ â”‚                 (ce_loss): CrossEntropyLoss()                            â”‚ â”‚
[rank1]: â”‚ â”‚               ),                                                         â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   dataset_loading_func=<function load_train_set at       â”‚ â”‚
[rank1]: â”‚ â”‚               0x7fdb85353eb0>,                                           â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   eval_dataset_loader={                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_energy':                           â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7c40>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_healthcare_industry':              â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7850>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_artificial_intelligence_test':     â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7730>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'syntheticDocQA_government_reports':               â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7b50>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'infovqa_subsampled':                              â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7ac0>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'docvqa_subsampled':                               â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7ca0>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'arxivqa_subsampled':                              â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7d00>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'tabfquad_subsampled':                             â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7d60>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'tatdqa':                                          â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7dc0>,                                 â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   â”‚   'shift_project':                                   â”‚ â”‚
[rank1]: â”‚ â”‚               <colpali_engine.utils.dataset_transformation.TestSetFactoâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚               object at 0x7fdb5c3b7e20>                                  â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   },                                                     â”‚ â”‚
[rank1]: â”‚ â”‚               â”‚   pretrained_peft_model_name_or_path=None                â”‚ â”‚
[rank1]: â”‚ â”‚               )                                                          â”‚ â”‚
[rank1]: â”‚ â”‚ config_file = PosixPath('/ivi/ilps/personal/jqiao/colpali/scripts/confiâ€¦ â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank1]: â”‚ /colpali_engine/trainer/colmodel_training.py:224 in eval                     â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   221 â”‚   â”‚   if self.config.eval_dataset_loader is not None:                â”‚
[rank1]: â”‚   222 â”‚   â”‚   â”‚   for test_name, test_dataset_loading_func in self.config.ev â”‚
[rank1]: â”‚   223 â”‚   â”‚   â”‚   â”‚   print(f"Evaluating {test_name}")                       â”‚
[rank1]: â”‚ â± 224 â”‚   â”‚   â”‚   â”‚   test_ds = test_dataset_loading_func()                  â”‚
[rank1]: â”‚   225 â”‚   â”‚   â”‚   â”‚   metrics = self.eval_dataset(test_ds)                   â”‚
[rank1]: â”‚   226 â”‚   â”‚   â”‚   â”‚   all_metrics[test_name] = metrics                       â”‚
[rank1]: â”‚   227 â”‚   â”‚   â”‚   â”‚   print(f"Metrics for {test_name}: {metrics}")           â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚               all_metrics = {                                            â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'validation_set': {                      â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_1': 0.842,                  â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_3': 0.88859,                â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_5': 0.89281,                â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_10': 0.89876,               â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_20': 0.90318,               â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_50': 0.90714,               â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'ndcg_at_100': 0.90876,              â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'map_at_1': 0.842,                   â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'map_at_3': 0.87767,                 â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   'map_at_5': 0.88007,                 â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   â”‚   ... +46                              â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   }                                        â”‚ â”‚
[rank1]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank1]: â”‚ â”‚                   metrics = {                                            â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_1': 0.842,                      â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_3': 0.88859,                    â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_5': 0.89281,                    â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_10': 0.89876,                   â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_20': 0.90318,                   â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_50': 0.90714,                   â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'ndcg_at_100': 0.90876,                  â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'map_at_1': 0.842,                       â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'map_at_3': 0.87767,                     â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   'map_at_5': 0.88007,                     â”‚ â”‚
[rank1]: â”‚ â”‚                             â”‚   ... +46                                  â”‚ â”‚
[rank1]: â”‚ â”‚                             }                                            â”‚ â”‚
[rank1]: â”‚ â”‚                      self = <colpali_engine.trainer.colmodel_training.Câ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚                             object at 0x7fdb5c2fb2e0>                    â”‚ â”‚
[rank1]: â”‚ â”‚ test_dataset_loading_func = <colpali_engine.utils.dataset_transformatioâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚                             object at 0x7fdb5c3b7c40>                    â”‚ â”‚
[rank1]: â”‚ â”‚                 test_name = 'syntheticDocQA_energy'                      â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank1]: â”‚ /colpali_engine/utils/dataset_transformation.py:184 in __call__              â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   181 â”‚   â”‚   self.dataset_path = dataset_path                               â”‚
[rank1]: â”‚   182 â”‚                                                                      â”‚
[rank1]: â”‚   183 â”‚   def __call__(self, *args, **kwargs):                               â”‚
[rank1]: â”‚ â± 184 â”‚   â”‚   dataset = load_dataset(self.dataset_path, split="test")        â”‚
[rank1]: â”‚   185 â”‚   â”‚   return dataset                                                 â”‚
[rank1]: â”‚   186                                                                        â”‚
[rank1]: â”‚   187                                                                        â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚   args = ()                                                              â”‚ â”‚
[rank1]: â”‚ â”‚ kwargs = {}                                                              â”‚ â”‚
[rank1]: â”‚ â”‚   self = <colpali_engine.utils.dataset_transformation.TestSetFactory     â”‚ â”‚
[rank1]: â”‚ â”‚          object at 0x7fdb5c3b7c40>                                       â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank1]: â”‚ /datasets/load.py:2132 in load_dataset                                       â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   2129 â”‚   )                                                                 â”‚
[rank1]: â”‚   2130 â”‚                                                                     â”‚
[rank1]: â”‚   2131 â”‚   # Create a dataset builder                                        â”‚
[rank1]: â”‚ â± 2132 â”‚   builder_instance = load_dataset_builder(                          â”‚
[rank1]: â”‚   2133 â”‚   â”‚   path=path,                                                    â”‚
[rank1]: â”‚   2134 â”‚   â”‚   name=name,                                                    â”‚
[rank1]: â”‚   2135 â”‚   â”‚   data_dir=data_dir,                                            â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚         cache_dir = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚     config_kwargs = {}                                                   â”‚ â”‚
[rank1]: â”‚ â”‚          data_dir = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚        data_files = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚   download_config = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚     download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:               â”‚ â”‚
[rank1]: â”‚ â”‚                     'reuse_dataset_if_exists'>                           â”‚ â”‚
[rank1]: â”‚ â”‚          features = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚    keep_in_memory = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚              name = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚          num_proc = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚              path = '/ivi/ilps/personal/jqiao/colpali/scripts/configs/qâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚          revision = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚        save_infos = False                                                â”‚ â”‚
[rank1]: â”‚ â”‚             split = 'test'                                               â”‚ â”‚
[rank1]: â”‚ â”‚   storage_options = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚         streaming = False                                                â”‚ â”‚
[rank1]: â”‚ â”‚             token = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚ trust_remote_code = None                                                 â”‚ â”‚
[rank1]: â”‚ â”‚ verification_mode = <VerificationMode.BASIC_CHECKS: 'basic_checks'>      â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank1]: â”‚ /datasets/load.py:1853 in load_dataset_builder                               â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   1850 â”‚   if storage_options is not None:                                   â”‚
[rank1]: â”‚   1851 â”‚   â”‚   download_config = download_config.copy() if download_config e â”‚
[rank1]: â”‚   1852 â”‚   â”‚   download_config.storage_options.update(storage_options)       â”‚
[rank1]: â”‚ â± 1853 â”‚   dataset_module = dataset_module_factory(                          â”‚
[rank1]: â”‚   1854 â”‚   â”‚   path,                                                         â”‚
[rank1]: â”‚   1855 â”‚   â”‚   revision=revision,                                            â”‚
[rank1]: â”‚   1856 â”‚   â”‚   download_config=download_config,                              â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank1]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                config_kwargs = {}                                        â”‚ â”‚
[rank1]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚              download_config = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank1]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank1]: â”‚ â”‚                     features = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                         name = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚              storage_options = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                        token = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages â”‚
[rank1]: â”‚ /datasets/load.py:1735 in dataset_module_factory                             â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚   1732 â”‚   â”‚   â”‚   f"Couldn't find a dataset script at {relative_to_absolute â”‚
[rank1]: â”‚   1733 â”‚   â”‚   )                                                             â”‚
[rank1]: â”‚   1734 â”‚   else:                                                             â”‚
[rank1]: â”‚ â± 1735 â”‚   â”‚   raise FileNotFoundError(f"Couldn't find any data file at {rel â”‚
[rank1]: â”‚   1736                                                                       â”‚
[rank1]: â”‚   1737                                                                       â”‚
[rank1]: â”‚   1738 def load_dataset_builder(                                             â”‚
[rank1]: â”‚                                                                              â”‚
[rank1]: â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
[rank1]: â”‚ â”‚      _require_custom_configs = False                                     â”‚ â”‚
[rank1]: â”‚ â”‚ _require_default_config_name = True                                      â”‚ â”‚
[rank1]: â”‚ â”‚                    cache_dir = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                combined_path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚                     data_dir = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                   data_files = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚              download_config = DownloadConfig(                           â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   cache_dir=None,                       â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   force_download=False,                 â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   resume_download=False,                â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   local_files_only=False,               â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   proxies=None,                         â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   user_agent=None,                      â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   extract_compressed_file=True,         â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   force_extract=True,                   â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   delete_extracted=False,               â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   extract_on_the_fly=False,             â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   use_etag=True,                        â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   num_proc=None,                        â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   max_retries=1,                        â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   token=None,                           â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   storage_options={},                   â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   download_desc=None,                   â”‚ â”‚
[rank1]: â”‚ â”‚                                â”‚   disable_tqdm=False                    â”‚ â”‚
[rank1]: â”‚ â”‚                                )                                         â”‚ â”‚
[rank1]: â”‚ â”‚              download_kwargs = {}                                        â”‚ â”‚
[rank1]: â”‚ â”‚                download_mode = <DownloadMode.REUSE_DATASET_IF_EXISTS:    â”‚ â”‚
[rank1]: â”‚ â”‚                                'reuse_dataset_if_exists'>                â”‚ â”‚
[rank1]: â”‚ â”‚         dynamic_modules_path = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚                     filename = 'syntheticDocQA_energy_test.py'           â”‚ â”‚
[rank1]: â”‚ â”‚                         path = '/ivi/ilps/personal/jqiao/colpali/scriptâ€¦ â”‚ â”‚
[rank1]: â”‚ â”‚                     revision = None                                      â”‚ â”‚
[rank1]: â”‚ â”‚            trust_remote_code = None                                      â”‚ â”‚
[rank1]: â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
[rank1]: â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank1]: FileNotFoundError: Couldn't find any data file at 
[rank1]: /ivi/ilps/personal/jqiao/colpali/data_dir/syntheticDocQA_energy_test.
[1;34mwandb[0m: ðŸš€ View run [33mrich-snow-13[0m at: [34mhttps://wandb.ai/jingfenqiao/huggingface/runs/dwl0ad4a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241223_000738-dwl0ad4a/logs[0m
[rank0]:[W1223 17:46:26.971893111 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
E1223 17:46:46.621000 4114917 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 4114930) of binary: /ivi/ilps/personal/jqiao/anaconda3/envs/colpali/bin/python
Traceback (most recent call last):
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ivi/ilps/personal/jqiao/anaconda3/envs/colpali/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/ivi/ilps/personal/jqiao/colpali/scripts/train/train_colbert.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-12-23_17:46:46
  host      : ilps-cn117.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 4114931)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-12-23_17:46:46
  host      : ilps-cn117.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 4114932)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-12-23_17:46:46
  host      : ilps-cn117.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4114933)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-23_17:46:46
  host      : ilps-cn117.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4114930)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
